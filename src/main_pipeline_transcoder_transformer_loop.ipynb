{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feed0634-9ddf-43f3-af9f-8f927022921d",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f588fa-ac74-40a0-a719-81f0e30e1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import mne\n",
    "# from mne import events_from_annotations, concatenate_raws\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.integrate import simps\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "# from mat4py import loadmat, savemat\n",
    "\n",
    "#from imblearn.pipeline import Pipeline\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF,WhiteKernel,Matern,RationalQuadratic,ExpSineSquared\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from processing.Processing_EEG import process_eeg_raw, process_eeg_epochs\n",
    "from processing.Processing_NIRS import process_nirs_raw, process_nirs_epochs\n",
    "\n",
    "from utilities.Read_Data import read_subject_raw_nirs, read_subject_raw_eeg\n",
    "from utilities.utilities import translate_channel_name_to_ch_id, find_sections, spatial_zscore\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.insert(1, '../../iTransformer/')\n",
    "\n",
    "from iTransformer.iTransformerTranscoding import iTransformer\n",
    "\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da322b19-dcea-4c84-9623-2949c4e78059",
   "metadata": {},
   "source": [
    "## Constants ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82377b6c-5c6c-437e-b778-20f434d63e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/scratch/mjm9724/stinger/data/'\n",
    "\n",
    "ROOT_DIRECTORY_EEG = os.path.join(BASE_PATH, 'raw/eeg/')\n",
    "ROOT_DIRECTORY_NIRS = os.path.join(BASE_PATH, 'raw/nirs/')\n",
    "MODEL_WEIGHTS = os.path.join(BASE_PATH, 'model_weights/')\n",
    "OUTPUT_DIRECTORY = os.path.join(BASE_PATH, 'output/')\n",
    "\n",
    "# Trial order\n",
    "TRIAL_TO_CHECK_NIRS = {'VP001': {\n",
    "                            'nback': ['2016-05-26_007', '2016-05-26_008', '2016-05-26_009',],\n",
    "                            'gonogo': ['2016-05-26_001', '2016-05-26_002', '2016-05-26_003',],\n",
    "                            'word': ['2016-05-26_004', '2016-05-26_005', '2016-05-26_006',]\n",
    "                        },\n",
    "                        'VP002': {\n",
    "                            'nback': ['2016-05-26_016', '2016-05-26_017', '2016-05-26_018',],\n",
    "                            'gonogo': ['2016-05-26_010', '2016-05-26_011', '2016-05-26_012',],\n",
    "                            'word': ['2016-05-26_013', '2016-05-26_014', '2016-05-26_015',]\n",
    "                        },\n",
    "                        'VP003': {\n",
    "                            'nback': ['2016-05-27_001', '2016-05-27_002', '2016-05-27_003',],\n",
    "                            'gonogo': ['2016-05-27_007', '2016-05-27_008', '2016-05-27_009',],\n",
    "                            'word': ['2016-05-27_004', '2016-05-27_005', '2016-05-27_006',]\n",
    "                        },\n",
    "                        'VP004': {\n",
    "                            'nback': ['2016-05-30_001', '2016-05-30_002', '2016-05-30_003'],\n",
    "                            'gonogo': ['2016-05-30_007', '2016-05-30_008', '2016-05-30_009'],\n",
    "                            'word': ['2016-05-30_004', '2016-05-30_005', '2016-05-30_006']\n",
    "                        },\n",
    "                        'VP005': {\n",
    "                            'nback': ['2016-05-30_010', '2016-05-30_011', '2016-05-30_012'],\n",
    "                            'gonogo': ['2016-05-30_016', '2016-05-30_017', '2016-05-30_018'],\n",
    "                            'word': ['2016-05-30_013', '2016-05-30_014', '2016-05-30_015']\n",
    "                        },\n",
    "                        'VP006': {\n",
    "                            'nback': ['2016-05-31_001', '2016-05-31_002', '2016-05-31_003'],\n",
    "                            'gonogo': ['2016-05-31_007', '2016-05-31_008', '2016-05-31_009'],\n",
    "                            'word': ['2016-05-31_004', '2016-05-31_005', '2016-05-31_006']\n",
    "                        },\n",
    "                        'VP007': {\n",
    "                            'nback': ['2016-06-01_001', '2016-06-01_002', '2016-06-01_003'],\n",
    "                            'gonogo': ['2016-06-01_007', '2016-06-01_008', '2016-06-01_009'],\n",
    "                            'word': ['2016-06-01_004', '2016-06-01_005', '2016-06-01_006']\n",
    "                        },\n",
    "                        'VP008': {\n",
    "                            'nback': ['2016-06-02_001', '2016-06-02_002', '2016-06-02_003'],\n",
    "                            'gonogo': ['2016-06-02_007', '2016-06-02_008', '2016-06-02_009'],\n",
    "                            'word': ['2016-06-02_004', '2016-06-02_005', '2016-06-02_006']\n",
    "                        },\n",
    "                        'VP009': {\n",
    "                            'nback': ['2016-06-02_010', '2016-06-02_011', '2016-06-02_012'],\n",
    "                            'gonogo': ['2016-06-02_016', '2016-06-02_017', '2016-06-02_018'],\n",
    "                            'word': ['2016-06-02_013', '2016-06-02_014', '2016-06-02_015']\n",
    "                        },\n",
    "                        'VP010': {\n",
    "                            'nback': ['2016-06-03_001', '2016-06-03_002', '2016-06-03_003'],\n",
    "                            'gonogo': ['2016-06-03_007', '2016-06-03_008', '2016-06-03_009'],\n",
    "                            'word': ['2016-06-03_004', '2016-06-03_005', '2016-06-03_006']\n",
    "                        },\n",
    "                        'VP011': {\n",
    "                            'nback': ['2016-06-03_010', '2016-06-03_011', '2016-06-03_012'],\n",
    "                            'gonogo': ['2016-06-03_016', '2016-06-03_017', '2016-06-03_018'],\n",
    "                            'word': ['2016-06-03_013', '2016-06-03_014', '2016-06-03_015']\n",
    "                        },'VP012': {\n",
    "                            'nback': ['2016-06-06_001', '2016-06-06_002', '2016-06-06_003'],\n",
    "                            'gonogo': ['2016-06-06_007', '2016-06-06_008', '2016-06-06_009'],\n",
    "                            'word': ['2016-06-06_004', '2016-06-06_005', '2016-06-06_006']\n",
    "                        },'VP013': {\n",
    "                            'nback': ['2016-06-06_010', '2016-06-06_011', '2016-06-06_012'],\n",
    "                            'gonogo': ['2016-06-06_016', '2016-06-06_017', '2016-06-06_018'],\n",
    "                            'word': ['2016-06-06_013', '2016-06-06_014', '2016-06-06_015']\n",
    "                        },'VP014': {\n",
    "                            'nback': ['2016-06-07_001', '2016-06-07_002', '2016-06-07_003'],\n",
    "                            'gonogo': ['2016-06-07_007', '2016-06-07_008', '2016-06-07_009'],\n",
    "                            'word': ['2016-06-07_004', '2016-06-07_005', '2016-06-07_006']\n",
    "                        },'VP015': {\n",
    "                            'nback': ['2016-06-07_010', '2016-06-07_011', '2016-06-07_012'],\n",
    "                            'gonogo': ['2016-06-07_016', '2016-06-07_017', '2016-06-07_018'],\n",
    "                            'word': ['2016-06-07_013', '2016-06-07_014', '2016-06-07_015']\n",
    "                        },'VP016': {\n",
    "                            'nback': ['2016-06-08_001', '2016-06-08_002', '2016-06-08_003'],\n",
    "                            'gonogo': ['2016-06-08_007', '2016-06-08_008', '2016-06-08_009'],\n",
    "                            'word': ['2016-06-08_004', '2016-06-08_005', '2016-06-08_006']\n",
    "                        },'VP017': {\n",
    "                            'nback': ['2016-06-09_001', '2016-06-09_002', '2016-06-09_003'],\n",
    "                            'gonogo': ['2016-06-09_007', '2016-06-09_008', '2016-06-09_009'],\n",
    "                            'word': ['2016-06-09_004', '2016-06-09_005', '2016-06-09_006']\n",
    "                        },'VP018': {\n",
    "                            'nback': ['2016-06-10_001', '2016-06-10_002', '2016-06-10_003'],\n",
    "                            'gonogo': ['2016-06-10_007', '2016-06-10_008', '2016-06-10_009'],\n",
    "                            'word': ['2016-06-10_004', '2016-06-10_005', '2016-06-10_006']\n",
    "                        },'VP019': {\n",
    "                            'nback': ['2016-06-13_001', '2016-06-13_002', '2016-06-13_003'],\n",
    "                            'gonogo': ['2016-06-13_007', '2016-06-13_008', '2016-06-13_009'],\n",
    "                            'word': ['2016-06-13_004', '2016-06-13_005', '2016-06-13_006']\n",
    "                        },'VP020': {\n",
    "                            'nback': ['2016-06-14_001', '2016-06-14_002', '2016-06-14_003'],\n",
    "                            'gonogo': ['2016-06-14_007', '2016-06-14_008', '2016-06-14_009'],\n",
    "                            'word': ['2016-06-14_004', '2016-06-14_005', '2016-06-14_006']\n",
    "                        },'VP021': {\n",
    "                            'nback': ['2016-06-14_010', '2016-06-14_011', '2016-06-14_012'],\n",
    "                            'gonogo': ['2016-06-14_016', '2016-06-14_017', '2016-06-14_018'],\n",
    "                            'word': ['2016-06-14_013', '2016-06-14_014', '2016-06-14_015']\n",
    "                        },'VP022': {\n",
    "                            'nback': ['2016-06-15_001', '2016-06-15_002', '2016-06-15_003'],\n",
    "                            'gonogo': ['2016-06-15_007', '2016-06-15_008', '2016-06-15_009'],\n",
    "                            'word': ['2016-06-15_004', '2016-06-15_005', '2016-06-15_006']\n",
    "                        },'VP023': {\n",
    "                            'nback': ['2016-06-16_001', '2016-06-16_002', '2016-06-16_003'],\n",
    "                            'gonogo': ['2016-06-16_007', '2016-06-16_008', '2016-06-16_009'],\n",
    "                            'word': ['2016-06-16_004', '2016-06-16_005', '2016-06-16_006']\n",
    "                        },'VP024': {\n",
    "                            'nback': ['2016-06-16_010', '2016-06-16_011', '2016-06-16_012'],\n",
    "                            'gonogo': ['2016-06-16_016', '2016-06-16_017', '2016-06-16_018'],\n",
    "                            'word': ['2016-06-16_013', '2016-06-16_014', '2016-06-16_015']\n",
    "                        },\n",
    "                        'VP025': {\n",
    "                            'nback': ['2016-06-17_010', '2016-06-17_011', '2016-06-17_012',],\n",
    "                            'gonogo': ['2016-06-17_016', '2016-06-17_017', '2016-06-17_018',],\n",
    "                            'word': ['2016-06-17_013', '2016-06-17_014', '2016-06-17_015',]\n",
    "                        },\n",
    "                        'VP026': {\n",
    "                            'nback': ['2016-07-11_001', '2016-07-11_002', '2016-07-11_003',],\n",
    "                            'gonogo': ['2016-07-11_007', '2016-07-11_008', '2016-07-11_009',],\n",
    "                            'word': ['2016-07-11_004', '2016-07-11_005', '2016-07-11_006',]\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "# Task translation dictionaries\n",
    "EEG_EVENT_TRANSLATIONS = {\n",
    "            'nback': {\n",
    "                'Stimulus/S 16': '0-back target',\n",
    "                'Stimulus/S 48': '2-back target',\n",
    "                'Stimulus/S 64': '2-back non-target',\n",
    "                'Stimulus/S 80': '3-back target',\n",
    "                'Stimulus/S 96': '3-back non-target',\n",
    "                'Stimulus/S112': '0-back session',\n",
    "                'Stimulus/S128': '2-back session',\n",
    "                'Stimulus/S144': '3-back session'},\n",
    "            'gonogo': {\n",
    "                'Stimulus/S 16': 'go',\n",
    "                'Stimulus/S 32': 'nogo',\n",
    "                'Stimulus/S 48': 'gonogo session'},\n",
    "            'word': {\n",
    "                'Stimulus/S 16': 'verbal_fluency',\n",
    "                'Stimulus/S 32': 'baseline'}\n",
    "}\n",
    "NIRS_EVENT_TRANSLATIONS = {\n",
    "    'nback': {\n",
    "        '7.0': '0-back session',\n",
    "        '8.0': '2-back session',\n",
    "        '9.0': '3-back session'},\n",
    "    'gonogo': {\n",
    "        '3.0': 'gonogo session'},\n",
    "    'word': {\n",
    "        '1.0': 'verbal_fluency',\n",
    "        '2.0': 'baseline'}\n",
    "}\n",
    "\n",
    "# Sub tasks to crop times to for same length\n",
    "TASK_STIMULOUS_TO_CROP = {'nback': ['0-back session', '2-back session', '3-back session'],\n",
    "                            'gonogo': ['gonogo session'],\n",
    "                            'word': ['verbal_fluency', 'baseline']\n",
    "                            }\n",
    "\n",
    "# EEG Coordinates\n",
    "EEG_COORDS = {'FP1':(-0.3090,0.9511,0.0001), #Fp1\n",
    "                'AFF5':(-0.5417,0.7777,0.3163), #AFF5h\n",
    "                'AFz':(0.0000,0.9230,0.3824),\n",
    "                'F1':(-0.2888,0.6979,0.6542),\n",
    "                'FC5':(-0.8709,0.3373,0.3549),\n",
    "                'FC1':(-0.3581,0.3770,0.8532),\n",
    "                'T7':(-1.0000,0.0000,0.0000),\n",
    "                'C3':(-0.7066,0.0001,0.7066),\n",
    "                'Cz':(0.0000,0.0002,1.0000),\n",
    "                'CP5':(-0.8712,-0.3372,0.3552),\n",
    "                'CP1':(-0.3580,-0.3767,0.8534),\n",
    "                'P7':(-0.8090,-0.5878,-0.0001),\n",
    "                'P3':(-0.5401,-0.6724,0.5045),\n",
    "                'Pz':(0.0000,-0.7063,0.7065),\n",
    "                'POz':(0.0000,-0.9230,0.3824),\n",
    "                'O1':(-0.3090,-0.9511,0.0000),\n",
    "                'FP2':(0.3091,0.9511,0.0000), #Fp2\n",
    "                'AFF6':(0.5417,0.7777,0.3163), #AFF6h\n",
    "                'F2':(0.2888,0.6979,0.6542),\n",
    "                'FC2':(0.3581,0.3770,0.8532),\n",
    "                'FC6':(0.8709,0.3373,0.3549),\n",
    "                'C4':(0.7066,0.0001,0.7066),\n",
    "                'T8':(1.0000,0.0000,0.0000),\n",
    "                'CP2':(0.3580,-0.3767,0.8534),\n",
    "                'CP6':(0.8712,-0.3372,0.3552),\n",
    "                'P4':(0.5401,-0.6724,0.5045),\n",
    "                'P8':(0.8090,-0.5878,-0.0001),\n",
    "                'O2':(0.3090,-0.9511,0.0000),\n",
    "                'TP9':(-0.8777,-0.2852,-0.3826),\n",
    "                'TP10':(0.8777,-0.2853,-0.3826),\n",
    "                \n",
    "                'Fp1':(-0.3090,0.9511,0.0001),\n",
    "                'AFF5h':(-0.5417,0.7777,0.3163),\n",
    "                'Fp2':(0.3091,0.9511,0.0000),\n",
    "                'AFF6h':(0.5417,0.7777,0.3163),}\n",
    "\n",
    "# NIRS Ccoordinates\n",
    "NIRS_COORDS = {\n",
    "    'AF7':(-0.5878,0.809,0),\n",
    "    'AFF5':(-0.6149,0.7564,0.2206),\n",
    "    'AFp7':(-0.454,0.891,0),\n",
    "    'AF5h':(-0.4284,0.875,0.2213),\n",
    "    'AFp3':(-0.2508,0.9565,0.1438),\n",
    "    'AFF3h':(-0.352,0.8111,0.4658),\n",
    "    'AF1':(-0.1857,0.915,0.3558),\n",
    "    'AFFz':(0,0.8312,0.5554),\n",
    "    'AFpz':(0,0.9799,0.1949),\n",
    "    'AF2':(0.1857,0.915,0.3558),\n",
    "    'AFp4':(0.2508,0.9565,0.1437),\n",
    "    'FCC3':(-0.6957,0.1838,0.6933),\n",
    "    'C3h':(-0.555,0.0002,0.8306),\n",
    "    'C5h':(-0.8311,0.0001,0.5552),\n",
    "    'CCP3':(-0.6959,-0.1836,0.6936),\n",
    "    'CPP3':(-0.6109,-0.5259,0.5904),\n",
    "    'P3h':(-0.4217,-0.6869,0.5912),\n",
    "    'P5h':(-0.6411,-0.6546,0.3985),\n",
    "    'PPO3':(-0.4537,-0.796,0.3995),\n",
    "    'AFF4h':(0.352,0.8111,0.4658),\n",
    "    'AF6h':(0.4284,0.875,0.2212),\n",
    "    'AFF6':(0.6149,0.7564,0.2206),\n",
    "    'AFp8':(0.454,0.891,0),\n",
    "    'AF8':(0.5878,0.809,0),\n",
    "    'FCC4':(0.6957,0.1838,0.6933),\n",
    "    'C6h':(0.8311,0.0001,0.5552),\n",
    "    'C4h':(0.555,0.0002,0.8306),\n",
    "    'CCP4':(0.6959,-0.1836,0.6936),\n",
    "    'CPP4':(0.6109,-0.5258,0.5904),\n",
    "    'P6h':(0.6411,-0.6546,0.3985),\n",
    "    'P4h':(0.4216,-0.687,0.5912),\n",
    "    'PPO4':(0.4537,-0.796,0.3995),\n",
    "    'PPOz':(0,-0.8306,0.5551),\n",
    "    'PO1':(-0.1858,-0.9151,0.3559),\n",
    "    'PO2':(0.1859,-0.9151,0.3559),\n",
    "    'POOz':(0,-0.9797,0.1949)}\n",
    "\n",
    "# EEG Channels names\n",
    "EEG_CHANNEL_NAMES = ['FP1', \n",
    "                    'AFF5h', \n",
    "                    'AFz', \n",
    "                    'F1', \n",
    "                    'FC5', \n",
    "                    'FC1', \n",
    "                    'T7', \n",
    "                    'C3', \n",
    "                    'Cz', \n",
    "                    'CP5', \n",
    "                    'CP1', \n",
    "                    'P7', \n",
    "                    'P3', \n",
    "                    'Pz', \n",
    "                    'POz', \n",
    "                    'O1',  \n",
    "                    'FP2', \n",
    "                    'AFF6h',\n",
    "                    'F2', \n",
    "                    'FC2', \n",
    "                    'FC6', \n",
    "                    'C4', \n",
    "                    'T8', \n",
    "                    'CP2', \n",
    "                    'CP6', \n",
    "                    'P4', \n",
    "                    'P8', \n",
    "                    'O2',\n",
    "                    'HEOG',\n",
    "                    'VEOG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a6026",
   "metadata": {},
   "source": [
    "## Parameters - Raw ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2db1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subject/Trial Parameters ##\n",
    "subject_ids = np.arange(1,2) # 1-27\n",
    "subjects = []\n",
    "for i in subject_ids:\n",
    "    subjects.append(f'VP{i:03d}')\n",
    "\n",
    "tasks = ['nback','gonogo','word']\n",
    "\n",
    "# NIRS Sampling rate\n",
    "fnirs_sample_rate = 10\n",
    "# EEG Downsampling rate\n",
    "eeg_sample_rate = 10\n",
    "\n",
    "# Do processing or not\n",
    "do_processing = True\n",
    "\n",
    "# Redo preprocessing pickle files, TAKES A LONG TIME \n",
    "redo_preprocessing = False\n",
    "\n",
    "# Redo data formating pickle files, TAKES A LONG TIME\n",
    "redo_data_formatting = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514427a",
   "metadata": {},
   "source": [
    "## Signal Prediction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09cead",
   "metadata": {},
   "source": [
    "### Parameters - Signal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606a092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window (seconds)\n",
    "eeg_t_min = 0\n",
    "eeg_t_max = 1\n",
    "nirs_t_min = -10\n",
    "nirs_t_max = 10\n",
    "\n",
    "offset_t = 0\n",
    "\n",
    "# Train/Test Size\n",
    "train_size = 4000\n",
    "test_size = 500\n",
    "\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "\n",
    "do_load = False\n",
    "do_train = True\n",
    "\n",
    "fnirs_lookback = 4000\n",
    "eeg_lookback = 200\n",
    "\n",
    "use_hbr = False\n",
    "\n",
    "subjects = np.arange(1,27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7f5c6",
   "metadata": {},
   "source": [
    "### Extract Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ca1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_window(center_point, \n",
    "                      nirs_data, \n",
    "                      eeg_data, \n",
    "                      eeg_i_min, \n",
    "                      eeg_i_max, \n",
    "                      nirs_i_min, \n",
    "                      nirs_i_max):\n",
    "    eeg_low_index = center_point + eeg_i_min\n",
    "    eeg_high_index = center_point + eeg_i_max\n",
    "    single_eeg_window = eeg_data[:,eeg_low_index:eeg_high_index]\n",
    "\n",
    "    nirs_low_index = center_point + nirs_i_min\n",
    "    nirs_high_index = center_point + nirs_i_max\n",
    "    single_nirs_window = nirs_data[:,nirs_low_index:nirs_high_index]\n",
    "    \n",
    "    return single_eeg_window, single_nirs_window\n",
    "\n",
    "def grab_ordered_windows(nirs_data,\n",
    "                        eeg_data,\n",
    "                        sampling_rate,\n",
    "                        nirs_t_min, \n",
    "                        nirs_t_max,\n",
    "                        eeg_t_min, \n",
    "                        eeg_t_max):\n",
    "    nirs_i_min = int(nirs_t_min*sampling_rate)\n",
    "    nirs_i_max = int(nirs_t_max*sampling_rate)\n",
    "    eeg_i_min = int(eeg_t_min*sampling_rate)\n",
    "    eeg_i_max = int(eeg_t_max*sampling_rate)\n",
    "\n",
    "    eeg_window_size = eeg_i_max - eeg_i_min\n",
    "\n",
    "    max_center_eeg = eeg_data.shape[1] - eeg_i_max\n",
    "    max_center_nirs = nirs_data.shape[1] - nirs_i_max\n",
    "    max_center = np.min([max_center_eeg, max_center_nirs])\n",
    "\n",
    "    min_center_eeg = np.abs(eeg_i_min)\n",
    "    min_center_nirs = np.abs(nirs_i_min)\n",
    "    min_center = np.max([min_center_eeg, min_center_nirs])\n",
    "\n",
    "    nirs_full_windows = []\n",
    "    eeg_full_windows = []\n",
    "    meta_data = []\n",
    "\n",
    "    for i in range(min_center, max_center, eeg_window_size):\n",
    "        center_point = i\n",
    "        meta_data.append(center_point)\n",
    "        single_eeg_window, single_nirs_window = get_single_window(center_point, \n",
    "                                                                  nirs_data, \n",
    "                                                                  eeg_data, \n",
    "                                                                  eeg_i_min, \n",
    "                                                                  eeg_i_max, \n",
    "                                                                  nirs_i_min, \n",
    "                                                                  nirs_i_max)\n",
    "        \n",
    "        eeg_full_windows.append(single_eeg_window)\n",
    "        nirs_full_windows.append(single_nirs_window)\n",
    "\n",
    "    nirs_full_windows = np.array(nirs_full_windows)\n",
    "    eeg_full_windows = np.array(eeg_full_windows)\n",
    "\n",
    "    return eeg_full_windows, nirs_full_windows, meta_data\n",
    "    \n",
    "def grab_random_windows(nirs_data, \n",
    "                        eeg_data,\n",
    "                        sampling_rate,\n",
    "                        nirs_t_min, \n",
    "                        nirs_t_max,\n",
    "                        eeg_t_min, \n",
    "                        eeg_t_max,\n",
    "                        number_of_windows=1000):\n",
    "    '''make number_of_windows of size t_min to t_max for each offset 0 to offset_t for eeg and nirs'''\n",
    "\n",
    "    nirs_i_min = int(nirs_t_min*sampling_rate)\n",
    "    nirs_i_max = int(nirs_t_max*sampling_rate)\n",
    "    eeg_i_min = int(eeg_t_min*sampling_rate)\n",
    "    eeg_i_max = int(eeg_t_max*sampling_rate)\n",
    "\n",
    "    max_center_eeg = eeg_data.shape[1] - eeg_i_max\n",
    "    max_center_nirs = nirs_data.shape[1] - nirs_i_max\n",
    "    max_center = np.min([max_center_eeg, max_center_nirs])\n",
    "\n",
    "    min_center_eeg = np.abs(eeg_i_min)\n",
    "    min_center_nirs = np.abs(nirs_i_min)\n",
    "    min_center = np.max([min_center_eeg, min_center_nirs])\n",
    "\n",
    "    nirs_full_windows = []\n",
    "    eeg_full_windows = []\n",
    "    meta_data = []\n",
    "    for i in range(number_of_windows):\n",
    "        center_point = np.random.randint(min_center, max_center)\n",
    "        meta_data.append(center_point)\n",
    "        single_eeg_window, single_nirs_window = get_single_window(center_point, \n",
    "                                                                  nirs_data, \n",
    "                                                                  eeg_data, \n",
    "                                                                  eeg_i_min, \n",
    "                                                                  eeg_i_max, \n",
    "                                                                  nirs_i_min, \n",
    "                                                                  nirs_i_max)\n",
    "        \n",
    "        eeg_full_windows.append(single_eeg_window)\n",
    "        nirs_full_windows.append(single_nirs_window)\n",
    "    \n",
    "    nirs_full_windows = np.array(nirs_full_windows)\n",
    "    eeg_full_windows = np.array(eeg_full_windows)\n",
    "\n",
    "    return eeg_full_windows, nirs_full_windows, meta_data\n",
    "\n",
    "class EEGfNIRSData(Dataset):\n",
    "    def __init__(self, fnirs_data, eeg_data):\n",
    "        self.fnirs_data = fnirs_data\n",
    "        self.eeg_data = eeg_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.eeg_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.fnirs_data[idx], self.eeg_data[idx]\n",
    "\n",
    "def plot_series(target, output, epoch):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(target, label='Target')\n",
    "    plt.plot(output, label='Output', linestyle='--')\n",
    "    plt.title(f'Epoch {epoch + 1}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def find_indices(x, y):\n",
    "    indices = []\n",
    "    for item in y:\n",
    "        if item in x:\n",
    "            indices.append(x.index(item))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71ea0f-4185-41e8-a2ee-53c86288f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model name exists, skipping transformer_01_FP1\n",
      "Model name exists, skipping transformer_01_AFF5h\n",
      "Starting transformer_01_AFz\n",
      "A100 GPU detected, using flash attention if input tensor is on cuda\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.3798\n",
      "Epoch: 2, Average Loss: 1.2834\n",
      "Epoch: 3, Average Loss: 1.2481\n",
      "Epoch: 4, Average Loss: 1.2284\n",
      "Epoch: 5, Average Loss: 1.2159\n",
      "Epoch: 6, Average Loss: 1.2068\n",
      "Epoch: 7, Average Loss: 1.2004\n",
      "Epoch: 8, Average Loss: 1.1955\n",
      "Epoch: 9, Average Loss: 1.1912\n",
      "Epoch: 10, Average Loss: 1.1881\n",
      "Epoch: 11, Average Loss: 1.1849\n",
      "Epoch: 12, Average Loss: 1.1823\n",
      "Epoch: 13, Average Loss: 1.1803\n",
      "Epoch: 14, Average Loss: 1.1780\n",
      "Epoch: 15, Average Loss: 1.1762\n",
      "Epoch: 16, Average Loss: 1.1739\n",
      "Epoch: 17, Average Loss: 1.1723\n",
      "Epoch: 18, Average Loss: 1.1704\n",
      "Epoch: 19, Average Loss: 1.1685\n",
      "Epoch: 20, Average Loss: 1.1669\n",
      "Epoch: 21, Average Loss: 1.1648\n",
      "Epoch: 22, Average Loss: 1.1632\n",
      "Epoch: 23, Average Loss: 1.1611\n",
      "Epoch: 24, Average Loss: 1.1592\n",
      "Epoch: 25, Average Loss: 1.1574\n",
      "Epoch: 26, Average Loss: 1.1552\n",
      "Epoch: 27, Average Loss: 1.1529\n",
      "Epoch: 28, Average Loss: 1.1511\n",
      "Epoch: 29, Average Loss: 1.1487\n",
      "Epoch: 30, Average Loss: 1.1465\n",
      "Epoch: 31, Average Loss: 1.1443\n",
      "Epoch: 32, Average Loss: 1.1414\n",
      "Epoch: 33, Average Loss: 1.1388\n",
      "Epoch: 34, Average Loss: 1.1366\n",
      "Epoch: 35, Average Loss: 1.1333\n",
      "Epoch: 36, Average Loss: 1.1303\n",
      "Epoch: 37, Average Loss: 1.1273\n",
      "Epoch: 38, Average Loss: 1.1241\n",
      "Epoch: 39, Average Loss: 1.1208\n",
      "Epoch: 40, Average Loss: 1.1181\n",
      "Epoch: 41, Average Loss: 1.1145\n",
      "Epoch: 42, Average Loss: 1.1111\n",
      "Epoch: 43, Average Loss: 1.1079\n",
      "Epoch: 44, Average Loss: 1.1030\n",
      "Epoch: 45, Average Loss: 1.0985\n",
      "Epoch: 46, Average Loss: 1.0951\n",
      "Epoch: 47, Average Loss: 1.0907\n",
      "Epoch: 48, Average Loss: 1.0870\n",
      "Epoch: 49, Average Loss: 1.0810\n",
      "Epoch: 50, Average Loss: 1.0767\n",
      "Epoch: 51, Average Loss: 1.0731\n",
      "Epoch: 52, Average Loss: 1.0669\n",
      "Epoch: 53, Average Loss: 1.0630\n",
      "Epoch: 54, Average Loss: 1.0596\n",
      "Epoch: 55, Average Loss: 1.0543\n",
      "Epoch: 56, Average Loss: 1.0492\n",
      "Epoch: 57, Average Loss: 1.0426\n",
      "Epoch: 58, Average Loss: 1.0401\n",
      "Epoch: 59, Average Loss: 1.0359\n",
      "Epoch: 60, Average Loss: 1.0278\n",
      "Epoch: 61, Average Loss: 1.0265\n",
      "Epoch: 62, Average Loss: 1.0212\n",
      "Epoch: 63, Average Loss: 1.0160\n",
      "Epoch: 64, Average Loss: 1.0116\n",
      "Epoch: 65, Average Loss: 1.0060\n",
      "Epoch: 66, Average Loss: 1.0014\n",
      "Epoch: 67, Average Loss: 0.9982\n",
      "Epoch: 68, Average Loss: 0.9934\n",
      "Epoch: 69, Average Loss: 0.9879\n",
      "Epoch: 70, Average Loss: 0.9842\n",
      "Epoch: 71, Average Loss: 0.9803\n",
      "Epoch: 72, Average Loss: 0.9759\n",
      "Epoch: 73, Average Loss: 0.9702\n",
      "Epoch: 74, Average Loss: 0.9691\n",
      "Epoch: 75, Average Loss: 0.9677\n",
      "Epoch: 76, Average Loss: 0.9590\n",
      "Epoch: 77, Average Loss: 0.9558\n",
      "Epoch: 78, Average Loss: 0.9536\n",
      "Epoch: 79, Average Loss: 0.9488\n",
      "Epoch: 80, Average Loss: 0.9454\n",
      "Epoch: 81, Average Loss: 0.9404\n",
      "Epoch: 82, Average Loss: 0.9380\n",
      "Epoch: 83, Average Loss: 0.9350\n",
      "Epoch: 84, Average Loss: 0.9300\n",
      "Epoch: 85, Average Loss: 0.9282\n",
      "Epoch: 86, Average Loss: 0.9251\n",
      "Epoch: 87, Average Loss: 0.9184\n",
      "Epoch: 88, Average Loss: 0.9158\n",
      "Epoch: 89, Average Loss: 0.9160\n",
      "Epoch: 90, Average Loss: 0.9122\n",
      "Epoch: 91, Average Loss: 0.9060\n",
      "Epoch: 92, Average Loss: 0.9024\n",
      "Epoch: 93, Average Loss: 0.9009\n",
      "Epoch: 94, Average Loss: 0.9003\n",
      "Epoch: 95, Average Loss: 0.8950\n",
      "Epoch: 96, Average Loss: 0.8898\n",
      "Epoch: 97, Average Loss: 0.8893\n",
      "Epoch: 98, Average Loss: 0.8847\n",
      "Epoch: 99, Average Loss: 0.8847\n",
      "Epoch: 100, Average Loss: 0.8818\n",
      "Epoch-50: -0.04430540387418813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.14171630277094116\n",
      "Starting transformer_01_F1\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.3152\n",
      "Epoch: 2, Average Loss: 1.2160\n",
      "Epoch: 3, Average Loss: 1.1792\n",
      "Epoch: 4, Average Loss: 1.1583\n",
      "Epoch: 5, Average Loss: 1.1447\n",
      "Epoch: 6, Average Loss: 1.1350\n",
      "Epoch: 7, Average Loss: 1.1281\n",
      "Epoch: 8, Average Loss: 1.1223\n",
      "Epoch: 9, Average Loss: 1.1178\n",
      "Epoch: 10, Average Loss: 1.1137\n",
      "Epoch: 11, Average Loss: 1.1104\n",
      "Epoch: 12, Average Loss: 1.1072\n",
      "Epoch: 13, Average Loss: 1.1043\n",
      "Epoch: 14, Average Loss: 1.1012\n",
      "Epoch: 15, Average Loss: 1.0987\n",
      "Epoch: 16, Average Loss: 1.0963\n",
      "Epoch: 17, Average Loss: 1.0934\n",
      "Epoch: 18, Average Loss: 1.0905\n",
      "Epoch: 19, Average Loss: 1.0879\n",
      "Epoch: 20, Average Loss: 1.0850\n",
      "Epoch: 21, Average Loss: 1.0823\n",
      "Epoch: 22, Average Loss: 1.0791\n",
      "Epoch: 23, Average Loss: 1.0765\n",
      "Epoch: 24, Average Loss: 1.0732\n",
      "Epoch: 25, Average Loss: 1.0702\n",
      "Epoch: 26, Average Loss: 1.0667\n",
      "Epoch: 27, Average Loss: 1.0632\n",
      "Epoch: 28, Average Loss: 1.0600\n",
      "Epoch: 29, Average Loss: 1.0558\n",
      "Epoch: 30, Average Loss: 1.0524\n",
      "Epoch: 31, Average Loss: 1.0480\n",
      "Epoch: 32, Average Loss: 1.0440\n",
      "Epoch: 33, Average Loss: 1.0395\n",
      "Epoch: 34, Average Loss: 1.0353\n",
      "Epoch: 35, Average Loss: 1.0317\n",
      "Epoch: 36, Average Loss: 1.0273\n",
      "Epoch: 37, Average Loss: 1.0226\n",
      "Epoch: 38, Average Loss: 1.0159\n",
      "Epoch: 39, Average Loss: 1.0122\n",
      "Epoch: 40, Average Loss: 1.0061\n",
      "Epoch: 41, Average Loss: 1.0010\n",
      "Epoch: 42, Average Loss: 0.9977\n",
      "Epoch: 43, Average Loss: 0.9908\n",
      "Epoch: 44, Average Loss: 0.9852\n",
      "Epoch: 45, Average Loss: 0.9793\n",
      "Epoch: 46, Average Loss: 0.9741\n",
      "Epoch: 47, Average Loss: 0.9704\n",
      "Epoch: 48, Average Loss: 0.9680\n",
      "Epoch: 49, Average Loss: 0.9577\n",
      "Epoch: 50, Average Loss: 0.9536\n",
      "Epoch: 51, Average Loss: 0.9480\n",
      "Epoch: 52, Average Loss: 0.9452\n",
      "Epoch: 53, Average Loss: 0.9387\n",
      "Epoch: 54, Average Loss: 0.9320\n",
      "Epoch: 55, Average Loss: 0.9257\n",
      "Epoch: 56, Average Loss: 0.9223\n",
      "Epoch: 57, Average Loss: 0.9151\n",
      "Epoch: 58, Average Loss: 0.9101\n",
      "Epoch: 59, Average Loss: 0.9073\n",
      "Epoch: 60, Average Loss: 0.9017\n",
      "Epoch: 61, Average Loss: 0.8967\n",
      "Epoch: 62, Average Loss: 0.8929\n",
      "Epoch: 63, Average Loss: 0.8869\n",
      "Epoch: 64, Average Loss: 0.8821\n",
      "Epoch: 65, Average Loss: 0.8773\n",
      "Epoch: 66, Average Loss: 0.8718\n",
      "Epoch: 67, Average Loss: 0.8708\n",
      "Epoch: 68, Average Loss: 0.8644\n",
      "Epoch: 69, Average Loss: 0.8612\n",
      "Epoch: 70, Average Loss: 0.8572\n",
      "Epoch: 71, Average Loss: 0.8522\n",
      "Epoch: 72, Average Loss: 0.8486\n",
      "Epoch: 73, Average Loss: 0.8451\n",
      "Epoch: 74, Average Loss: 0.8433\n",
      "Epoch: 75, Average Loss: 0.8361\n",
      "Epoch: 76, Average Loss: 0.8332\n",
      "Epoch: 77, Average Loss: 0.8282\n",
      "Epoch: 78, Average Loss: 0.8242\n",
      "Epoch: 79, Average Loss: 0.8226\n",
      "Epoch: 80, Average Loss: 0.8176\n",
      "Epoch: 81, Average Loss: 0.8141\n",
      "Epoch: 82, Average Loss: 0.8116\n",
      "Epoch: 83, Average Loss: 0.8092\n",
      "Epoch: 84, Average Loss: 0.8062\n",
      "Epoch: 85, Average Loss: 0.8004\n",
      "Epoch: 86, Average Loss: 0.7995\n",
      "Epoch: 87, Average Loss: 0.7957\n",
      "Epoch: 88, Average Loss: 0.7929\n",
      "Epoch: 89, Average Loss: 0.7889\n",
      "Epoch: 90, Average Loss: 0.7875\n",
      "Epoch: 91, Average Loss: 0.7879\n",
      "Epoch: 92, Average Loss: 0.7819\n",
      "Epoch: 93, Average Loss: 0.7779\n",
      "Epoch: 94, Average Loss: 0.7748\n",
      "Epoch: 95, Average Loss: 0.7703\n",
      "Epoch: 96, Average Loss: 0.7693\n",
      "Epoch: 97, Average Loss: 0.7725\n",
      "Epoch: 98, Average Loss: 0.7634\n",
      "Epoch: 99, Average Loss: 0.7599\n",
      "Epoch: 100, Average Loss: 0.7575\n",
      "Epoch-50: -0.07955619803502967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.19995375261421144\n",
      "Starting transformer_01_FC5\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1987\n",
      "Epoch: 2, Average Loss: 1.1041\n",
      "Epoch: 3, Average Loss: 1.0693\n",
      "Epoch: 4, Average Loss: 1.0497\n",
      "Epoch: 5, Average Loss: 1.0365\n",
      "Epoch: 6, Average Loss: 1.0275\n",
      "Epoch: 7, Average Loss: 1.0210\n",
      "Epoch: 8, Average Loss: 1.0158\n",
      "Epoch: 9, Average Loss: 1.0116\n",
      "Epoch: 10, Average Loss: 1.0083\n",
      "Epoch: 11, Average Loss: 1.0052\n",
      "Epoch: 12, Average Loss: 1.0024\n",
      "Epoch: 13, Average Loss: 0.9998\n",
      "Epoch: 14, Average Loss: 0.9977\n",
      "Epoch: 15, Average Loss: 0.9951\n",
      "Epoch: 16, Average Loss: 0.9929\n",
      "Epoch: 17, Average Loss: 0.9907\n",
      "Epoch: 18, Average Loss: 0.9884\n",
      "Epoch: 19, Average Loss: 0.9864\n",
      "Epoch: 20, Average Loss: 0.9839\n",
      "Epoch: 21, Average Loss: 0.9814\n",
      "Epoch: 22, Average Loss: 0.9791\n",
      "Epoch: 23, Average Loss: 0.9770\n",
      "Epoch: 24, Average Loss: 0.9744\n",
      "Epoch: 25, Average Loss: 0.9715\n",
      "Epoch: 26, Average Loss: 0.9692\n",
      "Epoch: 27, Average Loss: 0.9662\n",
      "Epoch: 28, Average Loss: 0.9629\n",
      "Epoch: 29, Average Loss: 0.9600\n",
      "Epoch: 30, Average Loss: 0.9572\n",
      "Epoch: 31, Average Loss: 0.9532\n",
      "Epoch: 32, Average Loss: 0.9494\n",
      "Epoch: 33, Average Loss: 0.9461\n",
      "Epoch: 34, Average Loss: 0.9416\n",
      "Epoch: 35, Average Loss: 0.9374\n",
      "Epoch: 36, Average Loss: 0.9337\n",
      "Epoch: 37, Average Loss: 0.9297\n",
      "Epoch: 38, Average Loss: 0.9252\n",
      "Epoch: 39, Average Loss: 0.9201\n",
      "Epoch: 40, Average Loss: 0.9144\n",
      "Epoch: 41, Average Loss: 0.9107\n",
      "Epoch: 42, Average Loss: 0.9056\n",
      "Epoch: 43, Average Loss: 0.9002\n",
      "Epoch: 44, Average Loss: 0.8967\n",
      "Epoch: 45, Average Loss: 0.8897\n",
      "Epoch: 46, Average Loss: 0.8836\n",
      "Epoch: 47, Average Loss: 0.8811\n",
      "Epoch: 48, Average Loss: 0.8761\n",
      "Epoch: 49, Average Loss: 0.8696\n",
      "Epoch: 50, Average Loss: 0.8640\n",
      "Epoch: 51, Average Loss: 0.8592\n",
      "Epoch: 52, Average Loss: 0.8537\n",
      "Epoch: 53, Average Loss: 0.8474\n",
      "Epoch: 54, Average Loss: 0.8432\n",
      "Epoch: 55, Average Loss: 0.8398\n",
      "Epoch: 56, Average Loss: 0.8324\n",
      "Epoch: 57, Average Loss: 0.8282\n",
      "Epoch: 58, Average Loss: 0.8240\n",
      "Epoch: 59, Average Loss: 0.8177\n",
      "Epoch: 60, Average Loss: 0.8188\n",
      "Epoch: 61, Average Loss: 0.8109\n",
      "Epoch: 62, Average Loss: 0.8066\n",
      "Epoch: 63, Average Loss: 0.7998\n",
      "Epoch: 64, Average Loss: 0.7965\n",
      "Epoch: 65, Average Loss: 0.7906\n",
      "Epoch: 66, Average Loss: 0.7865\n",
      "Epoch: 67, Average Loss: 0.7849\n",
      "Epoch: 68, Average Loss: 0.7792\n",
      "Epoch: 69, Average Loss: 0.7725\n",
      "Epoch: 70, Average Loss: 0.7701\n",
      "Epoch: 71, Average Loss: 0.7681\n",
      "Epoch: 72, Average Loss: 0.7618\n",
      "Epoch: 73, Average Loss: 0.7563\n",
      "Epoch: 74, Average Loss: 0.7527\n",
      "Epoch: 75, Average Loss: 0.7509\n",
      "Epoch: 76, Average Loss: 0.7508\n",
      "Epoch: 77, Average Loss: 0.7495\n",
      "Epoch: 78, Average Loss: 0.7451\n",
      "Epoch: 79, Average Loss: 0.7368\n",
      "Epoch: 80, Average Loss: 0.7347\n",
      "Epoch: 81, Average Loss: 0.7313\n",
      "Epoch: 82, Average Loss: 0.7265\n",
      "Epoch: 83, Average Loss: 0.7231\n",
      "Epoch: 84, Average Loss: 0.7193\n",
      "Epoch: 85, Average Loss: 0.7175\n",
      "Epoch: 86, Average Loss: 0.7149\n",
      "Epoch: 87, Average Loss: 0.7088\n",
      "Epoch: 88, Average Loss: 0.7095\n",
      "Epoch: 89, Average Loss: 0.7063\n",
      "Epoch: 90, Average Loss: 0.7045\n",
      "Epoch: 91, Average Loss: 0.6982\n",
      "Epoch: 92, Average Loss: 0.6992\n",
      "Epoch: 93, Average Loss: 0.6965\n",
      "Epoch: 94, Average Loss: 0.6933\n",
      "Epoch: 95, Average Loss: 0.6893\n",
      "Epoch: 96, Average Loss: 0.6848\n",
      "Epoch: 97, Average Loss: 0.6832\n",
      "Epoch: 98, Average Loss: 0.6811\n",
      "Epoch: 99, Average Loss: 0.6771\n",
      "Epoch: 100, Average Loss: 0.6793\n",
      "Epoch-50: -0.10623093885090773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.1774066230175182\n",
      "Starting transformer_01_FC1\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1657\n",
      "Epoch: 2, Average Loss: 1.0656\n",
      "Epoch: 3, Average Loss: 1.0291\n",
      "Epoch: 4, Average Loss: 1.0090\n",
      "Epoch: 5, Average Loss: 0.9958\n",
      "Epoch: 6, Average Loss: 0.9864\n",
      "Epoch: 7, Average Loss: 0.9797\n",
      "Epoch: 8, Average Loss: 0.9739\n",
      "Epoch: 9, Average Loss: 0.9693\n",
      "Epoch: 10, Average Loss: 0.9650\n",
      "Epoch: 11, Average Loss: 0.9613\n",
      "Epoch: 12, Average Loss: 0.9581\n",
      "Epoch: 13, Average Loss: 0.9553\n",
      "Epoch: 14, Average Loss: 0.9524\n",
      "Epoch: 15, Average Loss: 0.9492\n",
      "Epoch: 16, Average Loss: 0.9467\n",
      "Epoch: 17, Average Loss: 0.9442\n",
      "Epoch: 18, Average Loss: 0.9413\n",
      "Epoch: 19, Average Loss: 0.9385\n",
      "Epoch: 20, Average Loss: 0.9367\n",
      "Epoch: 21, Average Loss: 0.9336\n",
      "Epoch: 22, Average Loss: 0.9306\n",
      "Epoch: 23, Average Loss: 0.9277\n",
      "Epoch: 24, Average Loss: 0.9255\n",
      "Epoch: 25, Average Loss: 0.9212\n",
      "Epoch: 26, Average Loss: 0.9190\n",
      "Epoch: 27, Average Loss: 0.9154\n",
      "Epoch: 28, Average Loss: 0.9117\n",
      "Epoch: 29, Average Loss: 0.9082\n",
      "Epoch: 30, Average Loss: 0.9037\n",
      "Epoch: 31, Average Loss: 0.9005\n",
      "Epoch: 32, Average Loss: 0.8966\n",
      "Epoch: 33, Average Loss: 0.8912\n",
      "Epoch: 34, Average Loss: 0.8875\n",
      "Epoch: 35, Average Loss: 0.8829\n",
      "Epoch: 36, Average Loss: 0.8779\n",
      "Epoch: 37, Average Loss: 0.8730\n",
      "Epoch: 38, Average Loss: 0.8667\n",
      "Epoch: 39, Average Loss: 0.8612\n",
      "Epoch: 40, Average Loss: 0.8567\n",
      "Epoch: 41, Average Loss: 0.8507\n",
      "Epoch: 42, Average Loss: 0.8455\n",
      "Epoch: 43, Average Loss: 0.8380\n",
      "Epoch: 44, Average Loss: 0.8336\n",
      "Epoch: 45, Average Loss: 0.8281\n",
      "Epoch: 46, Average Loss: 0.8208\n",
      "Epoch: 47, Average Loss: 0.8146\n",
      "Epoch: 48, Average Loss: 0.8092\n",
      "Epoch: 49, Average Loss: 0.8031\n",
      "Epoch: 50, Average Loss: 0.7997\n",
      "Epoch: 51, Average Loss: 0.7938\n",
      "Epoch: 52, Average Loss: 0.7897\n",
      "Epoch: 53, Average Loss: 0.7813\n",
      "Epoch: 54, Average Loss: 0.7767\n",
      "Epoch: 55, Average Loss: 0.7713\n",
      "Epoch: 56, Average Loss: 0.7650\n",
      "Epoch: 57, Average Loss: 0.7623\n",
      "Epoch: 58, Average Loss: 0.7552\n",
      "Epoch: 59, Average Loss: 0.7485\n",
      "Epoch: 60, Average Loss: 0.7468\n",
      "Epoch: 61, Average Loss: 0.7425\n",
      "Epoch: 62, Average Loss: 0.7362\n",
      "Epoch: 63, Average Loss: 0.7361\n",
      "Epoch: 64, Average Loss: 0.7267\n",
      "Epoch: 65, Average Loss: 0.7237\n",
      "Epoch: 66, Average Loss: 0.7181\n",
      "Epoch: 67, Average Loss: 0.7134\n",
      "Epoch: 68, Average Loss: 0.7114\n",
      "Epoch: 69, Average Loss: 0.7075\n",
      "Epoch: 70, Average Loss: 0.7023\n",
      "Epoch: 71, Average Loss: 0.6991\n",
      "Epoch: 72, Average Loss: 0.6931\n",
      "Epoch: 73, Average Loss: 0.6889\n",
      "Epoch: 74, Average Loss: 0.6877\n",
      "Epoch: 75, Average Loss: 0.6830\n",
      "Epoch: 76, Average Loss: 0.6797\n",
      "Epoch: 77, Average Loss: 0.6766\n",
      "Epoch: 78, Average Loss: 0.6742\n",
      "Epoch: 79, Average Loss: 0.6694\n",
      "Epoch: 80, Average Loss: 0.6656\n",
      "Epoch: 81, Average Loss: 0.6616\n",
      "Epoch: 82, Average Loss: 0.6589\n",
      "Epoch: 83, Average Loss: 0.6559\n",
      "Epoch: 84, Average Loss: 0.6538\n",
      "Epoch: 85, Average Loss: 0.6503\n",
      "Epoch: 86, Average Loss: 0.6467\n",
      "Epoch: 87, Average Loss: 0.6459\n",
      "Epoch: 88, Average Loss: 0.6410\n",
      "Epoch: 89, Average Loss: 0.6359\n",
      "Epoch: 90, Average Loss: 0.6339\n",
      "Epoch: 91, Average Loss: 0.6334\n",
      "Epoch: 92, Average Loss: 0.6291\n",
      "Epoch: 93, Average Loss: 0.6262\n",
      "Epoch: 94, Average Loss: 0.6255\n",
      "Epoch: 95, Average Loss: 0.6212\n",
      "Epoch: 96, Average Loss: 0.6181\n",
      "Epoch: 97, Average Loss: 0.6154\n",
      "Epoch: 98, Average Loss: 0.6139\n",
      "Epoch: 99, Average Loss: 0.6120\n",
      "Epoch: 100, Average Loss: 0.6077\n",
      "Epoch-50: -0.04533853476617278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.17577198853456166\n",
      "Starting transformer_01_T7\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1183\n",
      "Epoch: 2, Average Loss: 1.0218\n",
      "Epoch: 3, Average Loss: 0.9861\n",
      "Epoch: 4, Average Loss: 0.9659\n",
      "Epoch: 5, Average Loss: 0.9527\n",
      "Epoch: 6, Average Loss: 0.9436\n",
      "Epoch: 7, Average Loss: 0.9369\n",
      "Epoch: 8, Average Loss: 0.9311\n",
      "Epoch: 9, Average Loss: 0.9268\n",
      "Epoch: 10, Average Loss: 0.9227\n",
      "Epoch: 11, Average Loss: 0.9189\n",
      "Epoch: 12, Average Loss: 0.9159\n",
      "Epoch: 13, Average Loss: 0.9127\n",
      "Epoch: 14, Average Loss: 0.9099\n",
      "Epoch: 15, Average Loss: 0.9067\n",
      "Epoch: 16, Average Loss: 0.9030\n",
      "Epoch: 17, Average Loss: 0.9002\n",
      "Epoch: 18, Average Loss: 0.8968\n",
      "Epoch: 19, Average Loss: 0.8938\n",
      "Epoch: 20, Average Loss: 0.8905\n",
      "Epoch: 21, Average Loss: 0.8872\n",
      "Epoch: 22, Average Loss: 0.8841\n",
      "Epoch: 23, Average Loss: 0.8795\n",
      "Epoch: 24, Average Loss: 0.8744\n",
      "Epoch: 25, Average Loss: 0.8708\n",
      "Epoch: 26, Average Loss: 0.8660\n",
      "Epoch: 27, Average Loss: 0.8615\n",
      "Epoch: 28, Average Loss: 0.8574\n",
      "Epoch: 29, Average Loss: 0.8514\n",
      "Epoch: 30, Average Loss: 0.8463\n",
      "Epoch: 31, Average Loss: 0.8414\n",
      "Epoch: 32, Average Loss: 0.8352\n",
      "Epoch: 33, Average Loss: 0.8292\n",
      "Epoch: 34, Average Loss: 0.8222\n",
      "Epoch: 35, Average Loss: 0.8172\n",
      "Epoch: 36, Average Loss: 0.8097\n",
      "Epoch: 37, Average Loss: 0.8028\n",
      "Epoch: 38, Average Loss: 0.7961\n",
      "Epoch: 39, Average Loss: 0.7896\n",
      "Epoch: 40, Average Loss: 0.7837\n",
      "Epoch: 41, Average Loss: 0.7769\n",
      "Epoch: 42, Average Loss: 0.7701\n",
      "Epoch: 43, Average Loss: 0.7618\n",
      "Epoch: 44, Average Loss: 0.7548\n",
      "Epoch: 45, Average Loss: 0.7504\n",
      "Epoch: 46, Average Loss: 0.7412\n",
      "Epoch: 47, Average Loss: 0.7346\n",
      "Epoch: 48, Average Loss: 0.7272\n",
      "Epoch: 49, Average Loss: 0.7212\n",
      "Epoch: 50, Average Loss: 0.7128\n",
      "Epoch: 51, Average Loss: 0.7057\n",
      "Epoch: 52, Average Loss: 0.7003\n",
      "Epoch: 53, Average Loss: 0.6961\n",
      "Epoch: 54, Average Loss: 0.6870\n",
      "Epoch: 55, Average Loss: 0.6843\n",
      "Epoch: 56, Average Loss: 0.6736\n",
      "Epoch: 57, Average Loss: 0.6697\n",
      "Epoch: 58, Average Loss: 0.6660\n",
      "Epoch: 59, Average Loss: 0.6571\n",
      "Epoch: 60, Average Loss: 0.6551\n",
      "Epoch: 61, Average Loss: 0.6474\n",
      "Epoch: 62, Average Loss: 0.6431\n",
      "Epoch: 63, Average Loss: 0.6375\n",
      "Epoch: 64, Average Loss: 0.6326\n",
      "Epoch: 65, Average Loss: 0.6267\n",
      "Epoch: 66, Average Loss: 0.6205\n",
      "Epoch: 67, Average Loss: 0.6189\n",
      "Epoch: 68, Average Loss: 0.6163\n",
      "Epoch: 69, Average Loss: 0.6077\n",
      "Epoch: 70, Average Loss: 0.6058\n",
      "Epoch: 71, Average Loss: 0.6030\n",
      "Epoch: 72, Average Loss: 0.5980\n",
      "Epoch: 73, Average Loss: 0.5936\n",
      "Epoch: 74, Average Loss: 0.5905\n",
      "Epoch: 75, Average Loss: 0.5849\n",
      "Epoch: 76, Average Loss: 0.5795\n",
      "Epoch: 77, Average Loss: 0.5781\n",
      "Epoch: 78, Average Loss: 0.5753\n",
      "Epoch: 79, Average Loss: 0.5702\n",
      "Epoch: 80, Average Loss: 0.5698\n",
      "Epoch: 81, Average Loss: 0.5637\n",
      "Epoch: 82, Average Loss: 0.5602\n",
      "Epoch: 83, Average Loss: 0.5582\n",
      "Epoch: 84, Average Loss: 0.5547\n",
      "Epoch: 85, Average Loss: 0.5522\n",
      "Epoch: 86, Average Loss: 0.5467\n",
      "Epoch: 87, Average Loss: 0.5459\n",
      "Epoch: 88, Average Loss: 0.5414\n",
      "Epoch: 89, Average Loss: 0.5393\n",
      "Epoch: 90, Average Loss: 0.5383\n",
      "Epoch: 91, Average Loss: 0.5341\n",
      "Epoch: 92, Average Loss: 0.5329\n",
      "Epoch: 93, Average Loss: 0.5296\n",
      "Epoch: 94, Average Loss: 0.5273\n",
      "Epoch: 95, Average Loss: 0.5239\n",
      "Epoch: 96, Average Loss: 0.5233\n",
      "Epoch: 97, Average Loss: 0.5194\n",
      "Epoch: 98, Average Loss: 0.5172\n",
      "Epoch: 99, Average Loss: 0.5177\n",
      "Epoch: 100, Average Loss: 0.5119\n",
      "Epoch-50: -0.12319083595406366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.25005585437655453\n",
      "Starting transformer_01_C3\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.2188\n",
      "Epoch: 2, Average Loss: 1.1181\n",
      "Epoch: 3, Average Loss: 1.0812\n",
      "Epoch: 4, Average Loss: 1.0610\n",
      "Epoch: 5, Average Loss: 1.0474\n",
      "Epoch: 6, Average Loss: 1.0377\n",
      "Epoch: 7, Average Loss: 1.0308\n",
      "Epoch: 8, Average Loss: 1.0249\n",
      "Epoch: 9, Average Loss: 1.0199\n",
      "Epoch: 10, Average Loss: 1.0160\n",
      "Epoch: 11, Average Loss: 1.0122\n",
      "Epoch: 12, Average Loss: 1.0087\n",
      "Epoch: 13, Average Loss: 1.0053\n",
      "Epoch: 14, Average Loss: 1.0024\n",
      "Epoch: 15, Average Loss: 0.9993\n",
      "Epoch: 16, Average Loss: 0.9961\n",
      "Epoch: 17, Average Loss: 0.9934\n",
      "Epoch: 18, Average Loss: 0.9898\n",
      "Epoch: 19, Average Loss: 0.9867\n",
      "Epoch: 20, Average Loss: 0.9835\n",
      "Epoch: 21, Average Loss: 0.9800\n",
      "Epoch: 22, Average Loss: 0.9764\n",
      "Epoch: 23, Average Loss: 0.9725\n",
      "Epoch: 24, Average Loss: 0.9685\n",
      "Epoch: 25, Average Loss: 0.9646\n",
      "Epoch: 26, Average Loss: 0.9603\n",
      "Epoch: 27, Average Loss: 0.9566\n",
      "Epoch: 28, Average Loss: 0.9519\n",
      "Epoch: 29, Average Loss: 0.9469\n",
      "Epoch: 30, Average Loss: 0.9419\n",
      "Epoch: 31, Average Loss: 0.9362\n",
      "Epoch: 32, Average Loss: 0.9307\n",
      "Epoch: 33, Average Loss: 0.9244\n",
      "Epoch: 34, Average Loss: 0.9191\n",
      "Epoch: 35, Average Loss: 0.9138\n",
      "Epoch: 36, Average Loss: 0.9063\n",
      "Epoch: 37, Average Loss: 0.9014\n",
      "Epoch: 38, Average Loss: 0.8937\n",
      "Epoch: 39, Average Loss: 0.8890\n",
      "Epoch: 40, Average Loss: 0.8811\n",
      "Epoch: 41, Average Loss: 0.8729\n",
      "Epoch: 42, Average Loss: 0.8664\n",
      "Epoch: 43, Average Loss: 0.8597\n",
      "Epoch: 44, Average Loss: 0.8514\n",
      "Epoch: 45, Average Loss: 0.8463\n",
      "Epoch: 46, Average Loss: 0.8380\n",
      "Epoch: 47, Average Loss: 0.8327\n",
      "Epoch: 48, Average Loss: 0.8252\n",
      "Epoch: 49, Average Loss: 0.8190\n",
      "Epoch: 50, Average Loss: 0.8112\n",
      "Epoch: 51, Average Loss: 0.8072\n",
      "Epoch: 52, Average Loss: 0.7992\n",
      "Epoch: 53, Average Loss: 0.7932\n",
      "Epoch: 54, Average Loss: 0.7870\n",
      "Epoch: 55, Average Loss: 0.7806\n",
      "Epoch: 56, Average Loss: 0.7774\n",
      "Epoch: 57, Average Loss: 0.7728\n",
      "Epoch: 58, Average Loss: 0.7635\n",
      "Epoch: 59, Average Loss: 0.7622\n",
      "Epoch: 60, Average Loss: 0.7554\n",
      "Epoch: 61, Average Loss: 0.7478\n",
      "Epoch: 62, Average Loss: 0.7422\n",
      "Epoch: 63, Average Loss: 0.7392\n",
      "Epoch: 64, Average Loss: 0.7327\n",
      "Epoch: 65, Average Loss: 0.7277\n",
      "Epoch: 66, Average Loss: 0.7245\n",
      "Epoch: 67, Average Loss: 0.7198\n",
      "Epoch: 68, Average Loss: 0.7134\n",
      "Epoch: 69, Average Loss: 0.7121\n",
      "Epoch: 70, Average Loss: 0.7033\n",
      "Epoch: 71, Average Loss: 0.7011\n",
      "Epoch: 72, Average Loss: 0.6961\n",
      "Epoch: 73, Average Loss: 0.6944\n",
      "Epoch: 74, Average Loss: 0.6883\n",
      "Epoch: 75, Average Loss: 0.6862\n",
      "Epoch: 76, Average Loss: 0.6808\n",
      "Epoch: 77, Average Loss: 0.6755\n",
      "Epoch: 78, Average Loss: 0.6714\n",
      "Epoch: 79, Average Loss: 0.6715\n",
      "Epoch: 80, Average Loss: 0.6675\n",
      "Epoch: 81, Average Loss: 0.6625\n",
      "Epoch: 82, Average Loss: 0.6595\n",
      "Epoch: 83, Average Loss: 0.6570\n",
      "Epoch: 84, Average Loss: 0.6513\n",
      "Epoch: 85, Average Loss: 0.6486\n",
      "Epoch: 86, Average Loss: 0.6467\n",
      "Epoch: 87, Average Loss: 0.6446\n",
      "Epoch: 88, Average Loss: 0.6404\n",
      "Epoch: 89, Average Loss: 0.6382\n",
      "Epoch: 90, Average Loss: 0.6355\n",
      "Epoch: 91, Average Loss: 0.6327\n",
      "Epoch: 92, Average Loss: 0.6297\n",
      "Epoch: 93, Average Loss: 0.6267\n",
      "Epoch: 94, Average Loss: 0.6246\n",
      "Epoch: 95, Average Loss: 0.6203\n",
      "Epoch: 96, Average Loss: 0.6180\n",
      "Epoch: 97, Average Loss: 0.6164\n",
      "Epoch: 98, Average Loss: 0.6124\n",
      "Epoch: 99, Average Loss: 0.6103\n",
      "Epoch: 100, Average Loss: 0.6086\n",
      "Epoch-50: -0.18265552791403383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.3032633549848134\n",
      "Starting transformer_01_Cz\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 0.9757\n",
      "Epoch: 2, Average Loss: 0.8744\n",
      "Epoch: 3, Average Loss: 0.8381\n",
      "Epoch: 4, Average Loss: 0.8182\n",
      "Epoch: 5, Average Loss: 0.8054\n",
      "Epoch: 6, Average Loss: 0.7965\n",
      "Epoch: 7, Average Loss: 0.7902\n",
      "Epoch: 8, Average Loss: 0.7850\n",
      "Epoch: 9, Average Loss: 0.7808\n",
      "Epoch: 10, Average Loss: 0.7768\n",
      "Epoch: 11, Average Loss: 0.7739\n",
      "Epoch: 12, Average Loss: 0.7706\n",
      "Epoch: 13, Average Loss: 0.7676\n",
      "Epoch: 14, Average Loss: 0.7649\n",
      "Epoch: 15, Average Loss: 0.7623\n",
      "Epoch: 16, Average Loss: 0.7597\n",
      "Epoch: 17, Average Loss: 0.7573\n",
      "Epoch: 18, Average Loss: 0.7543\n",
      "Epoch: 19, Average Loss: 0.7516\n",
      "Epoch: 20, Average Loss: 0.7491\n",
      "Epoch: 21, Average Loss: 0.7459\n",
      "Epoch: 22, Average Loss: 0.7434\n",
      "Epoch: 23, Average Loss: 0.7403\n",
      "Epoch: 24, Average Loss: 0.7366\n",
      "Epoch: 25, Average Loss: 0.7339\n",
      "Epoch: 26, Average Loss: 0.7309\n",
      "Epoch: 27, Average Loss: 0.7275\n",
      "Epoch: 28, Average Loss: 0.7236\n",
      "Epoch: 29, Average Loss: 0.7204\n",
      "Epoch: 30, Average Loss: 0.7165\n",
      "Epoch: 31, Average Loss: 0.7123\n",
      "Epoch: 32, Average Loss: 0.7079\n",
      "Epoch: 33, Average Loss: 0.7041\n",
      "Epoch: 34, Average Loss: 0.7011\n",
      "Epoch: 35, Average Loss: 0.6961\n",
      "Epoch: 36, Average Loss: 0.6912\n",
      "Epoch: 37, Average Loss: 0.6880\n",
      "Epoch: 38, Average Loss: 0.6820\n",
      "Epoch: 39, Average Loss: 0.6801\n",
      "Epoch: 40, Average Loss: 0.6734\n",
      "Epoch: 41, Average Loss: 0.6691\n",
      "Epoch: 42, Average Loss: 0.6647\n",
      "Epoch: 43, Average Loss: 0.6582\n",
      "Epoch: 44, Average Loss: 0.6537\n",
      "Epoch: 45, Average Loss: 0.6485\n",
      "Epoch: 46, Average Loss: 0.6421\n",
      "Epoch: 47, Average Loss: 0.6384\n",
      "Epoch: 48, Average Loss: 0.6341\n",
      "Epoch: 49, Average Loss: 0.6299\n",
      "Epoch: 50, Average Loss: 0.6238\n",
      "Epoch: 51, Average Loss: 0.6186\n",
      "Epoch: 52, Average Loss: 0.6136\n",
      "Epoch: 53, Average Loss: 0.6092\n",
      "Epoch: 54, Average Loss: 0.6046\n",
      "Epoch: 55, Average Loss: 0.5996\n",
      "Epoch: 56, Average Loss: 0.5953\n",
      "Epoch: 57, Average Loss: 0.5903\n",
      "Epoch: 58, Average Loss: 0.5843\n",
      "Epoch: 59, Average Loss: 0.5813\n",
      "Epoch: 60, Average Loss: 0.5754\n",
      "Epoch: 61, Average Loss: 0.5708\n",
      "Epoch: 62, Average Loss: 0.5683\n",
      "Epoch: 63, Average Loss: 0.5643\n",
      "Epoch: 64, Average Loss: 0.5617\n",
      "Epoch: 65, Average Loss: 0.5534\n",
      "Epoch: 66, Average Loss: 0.5497\n",
      "Epoch: 67, Average Loss: 0.5472\n",
      "Epoch: 68, Average Loss: 0.5459\n",
      "Epoch: 69, Average Loss: 0.5410\n",
      "Epoch: 70, Average Loss: 0.5360\n",
      "Epoch: 71, Average Loss: 0.5310\n",
      "Epoch: 72, Average Loss: 0.5308\n",
      "Epoch: 73, Average Loss: 0.5250\n",
      "Epoch: 74, Average Loss: 0.5211\n",
      "Epoch: 75, Average Loss: 0.5190\n",
      "Epoch: 76, Average Loss: 0.5177\n",
      "Epoch: 77, Average Loss: 0.5119\n",
      "Epoch: 78, Average Loss: 0.5093\n",
      "Epoch: 79, Average Loss: 0.5052\n",
      "Epoch: 80, Average Loss: 0.5022\n",
      "Epoch: 81, Average Loss: 0.4998\n",
      "Epoch: 82, Average Loss: 0.4971\n",
      "Epoch: 83, Average Loss: 0.4947\n",
      "Epoch: 84, Average Loss: 0.4919\n",
      "Epoch: 85, Average Loss: 0.4924\n",
      "Epoch: 86, Average Loss: 0.4879\n",
      "Epoch: 87, Average Loss: 0.4847\n",
      "Epoch: 88, Average Loss: 0.4820\n",
      "Epoch: 89, Average Loss: 0.4794\n",
      "Epoch: 90, Average Loss: 0.4767\n",
      "Epoch: 91, Average Loss: 0.4772\n",
      "Epoch: 92, Average Loss: 0.4736\n",
      "Epoch: 93, Average Loss: 0.4703\n",
      "Epoch: 94, Average Loss: 0.4694\n",
      "Epoch: 95, Average Loss: 0.4657\n",
      "Epoch: 96, Average Loss: 0.4654\n",
      "Epoch: 97, Average Loss: 0.4617\n",
      "Epoch: 98, Average Loss: 0.4602\n",
      "Epoch: 99, Average Loss: 0.4581\n",
      "Epoch: 100, Average Loss: 0.4560\n",
      "Epoch-50: -0.12288345410004897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.28649734407444005\n",
      "Starting transformer_01_CP5\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1686\n",
      "Epoch: 2, Average Loss: 1.0694\n",
      "Epoch: 3, Average Loss: 1.0331\n",
      "Epoch: 4, Average Loss: 1.0126\n",
      "Epoch: 5, Average Loss: 0.9997\n",
      "Epoch: 6, Average Loss: 0.9899\n",
      "Epoch: 7, Average Loss: 0.9831\n",
      "Epoch: 8, Average Loss: 0.9772\n",
      "Epoch: 9, Average Loss: 0.9723\n",
      "Epoch: 10, Average Loss: 0.9681\n",
      "Epoch: 11, Average Loss: 0.9643\n",
      "Epoch: 12, Average Loss: 0.9614\n",
      "Epoch: 13, Average Loss: 0.9574\n",
      "Epoch: 14, Average Loss: 0.9535\n",
      "Epoch: 15, Average Loss: 0.9501\n",
      "Epoch: 16, Average Loss: 0.9469\n",
      "Epoch: 17, Average Loss: 0.9439\n",
      "Epoch: 18, Average Loss: 0.9403\n",
      "Epoch: 19, Average Loss: 0.9368\n",
      "Epoch: 20, Average Loss: 0.9329\n",
      "Epoch: 21, Average Loss: 0.9294\n",
      "Epoch: 22, Average Loss: 0.9266\n",
      "Epoch: 23, Average Loss: 0.9221\n",
      "Epoch: 24, Average Loss: 0.9181\n",
      "Epoch: 25, Average Loss: 0.9138\n",
      "Epoch: 26, Average Loss: 0.9098\n",
      "Epoch: 27, Average Loss: 0.9053\n",
      "Epoch: 28, Average Loss: 0.9011\n",
      "Epoch: 29, Average Loss: 0.8957\n",
      "Epoch: 30, Average Loss: 0.8907\n",
      "Epoch: 31, Average Loss: 0.8865\n",
      "Epoch: 32, Average Loss: 0.8809\n",
      "Epoch: 33, Average Loss: 0.8766\n",
      "Epoch: 34, Average Loss: 0.8709\n",
      "Epoch: 35, Average Loss: 0.8639\n",
      "Epoch: 36, Average Loss: 0.8583\n",
      "Epoch: 37, Average Loss: 0.8516\n",
      "Epoch: 38, Average Loss: 0.8462\n",
      "Epoch: 39, Average Loss: 0.8385\n",
      "Epoch: 40, Average Loss: 0.8328\n",
      "Epoch: 41, Average Loss: 0.8247\n",
      "Epoch: 42, Average Loss: 0.8187\n",
      "Epoch: 43, Average Loss: 0.8133\n",
      "Epoch: 44, Average Loss: 0.8079\n",
      "Epoch: 45, Average Loss: 0.7984\n",
      "Epoch: 46, Average Loss: 0.7919\n",
      "Epoch: 47, Average Loss: 0.7854\n",
      "Epoch: 48, Average Loss: 0.7795\n",
      "Epoch: 49, Average Loss: 0.7715\n",
      "Epoch: 50, Average Loss: 0.7659\n",
      "Epoch: 51, Average Loss: 0.7578\n",
      "Epoch: 52, Average Loss: 0.7502\n",
      "Epoch: 53, Average Loss: 0.7448\n",
      "Epoch: 54, Average Loss: 0.7408\n",
      "Epoch: 55, Average Loss: 0.7306\n",
      "Epoch: 56, Average Loss: 0.7271\n",
      "Epoch: 57, Average Loss: 0.7206\n",
      "Epoch: 58, Average Loss: 0.7150\n",
      "Epoch: 59, Average Loss: 0.7067\n",
      "Epoch: 60, Average Loss: 0.7035\n",
      "Epoch: 61, Average Loss: 0.6965\n",
      "Epoch: 62, Average Loss: 0.6902\n",
      "Epoch: 63, Average Loss: 0.6894\n",
      "Epoch: 64, Average Loss: 0.6769\n",
      "Epoch: 65, Average Loss: 0.6747\n",
      "Epoch: 66, Average Loss: 0.6671\n",
      "Epoch: 67, Average Loss: 0.6659\n",
      "Epoch: 68, Average Loss: 0.6588\n",
      "Epoch: 69, Average Loss: 0.6580\n",
      "Epoch: 70, Average Loss: 0.6493\n",
      "Epoch: 71, Average Loss: 0.6479\n",
      "Epoch: 72, Average Loss: 0.6392\n",
      "Epoch: 73, Average Loss: 0.6351\n",
      "Epoch: 74, Average Loss: 0.6336\n",
      "Epoch: 75, Average Loss: 0.6299\n",
      "Epoch: 76, Average Loss: 0.6239\n",
      "Epoch: 77, Average Loss: 0.6190\n",
      "Epoch: 78, Average Loss: 0.6145\n",
      "Epoch: 79, Average Loss: 0.6115\n",
      "Epoch: 80, Average Loss: 0.6080\n",
      "Epoch: 81, Average Loss: 0.6044\n",
      "Epoch: 82, Average Loss: 0.5994\n",
      "Epoch: 83, Average Loss: 0.5969\n",
      "Epoch: 84, Average Loss: 0.5967\n",
      "Epoch: 85, Average Loss: 0.5891\n",
      "Epoch: 86, Average Loss: 0.5847\n",
      "Epoch: 87, Average Loss: 0.5843\n",
      "Epoch: 88, Average Loss: 0.5821\n",
      "Epoch: 89, Average Loss: 0.5773\n",
      "Epoch: 90, Average Loss: 0.5729\n",
      "Epoch: 91, Average Loss: 0.5714\n",
      "Epoch: 92, Average Loss: 0.5676\n",
      "Epoch: 93, Average Loss: 0.5662\n",
      "Epoch: 94, Average Loss: 0.5607\n",
      "Epoch: 95, Average Loss: 0.5597\n",
      "Epoch: 96, Average Loss: 0.5566\n",
      "Epoch: 97, Average Loss: 0.5540\n",
      "Epoch: 98, Average Loss: 0.5520\n",
      "Epoch: 99, Average Loss: 0.5489\n",
      "Epoch: 100, Average Loss: 0.5453\n",
      "Epoch-50: -0.07164311774678023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.25701135910683104\n",
      "Starting transformer_01_CP1\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1618\n",
      "Epoch: 2, Average Loss: 1.0554\n",
      "Epoch: 3, Average Loss: 1.0169\n",
      "Epoch: 4, Average Loss: 0.9954\n",
      "Epoch: 5, Average Loss: 0.9812\n",
      "Epoch: 6, Average Loss: 0.9714\n",
      "Epoch: 7, Average Loss: 0.9636\n",
      "Epoch: 8, Average Loss: 0.9572\n",
      "Epoch: 9, Average Loss: 0.9519\n",
      "Epoch: 10, Average Loss: 0.9470\n",
      "Epoch: 11, Average Loss: 0.9428\n",
      "Epoch: 12, Average Loss: 0.9380\n",
      "Epoch: 13, Average Loss: 0.9340\n",
      "Epoch: 14, Average Loss: 0.9304\n",
      "Epoch: 15, Average Loss: 0.9265\n",
      "Epoch: 16, Average Loss: 0.9230\n",
      "Epoch: 17, Average Loss: 0.9194\n",
      "Epoch: 18, Average Loss: 0.9158\n",
      "Epoch: 19, Average Loss: 0.9121\n",
      "Epoch: 20, Average Loss: 0.9082\n",
      "Epoch: 21, Average Loss: 0.9042\n",
      "Epoch: 22, Average Loss: 0.9006\n",
      "Epoch: 23, Average Loss: 0.8962\n",
      "Epoch: 24, Average Loss: 0.8925\n",
      "Epoch: 25, Average Loss: 0.8874\n",
      "Epoch: 26, Average Loss: 0.8834\n",
      "Epoch: 27, Average Loss: 0.8788\n",
      "Epoch: 28, Average Loss: 0.8748\n",
      "Epoch: 29, Average Loss: 0.8686\n",
      "Epoch: 30, Average Loss: 0.8630\n",
      "Epoch: 31, Average Loss: 0.8575\n",
      "Epoch: 32, Average Loss: 0.8528\n",
      "Epoch: 33, Average Loss: 0.8465\n",
      "Epoch: 34, Average Loss: 0.8408\n",
      "Epoch: 35, Average Loss: 0.8345\n",
      "Epoch: 36, Average Loss: 0.8282\n",
      "Epoch: 37, Average Loss: 0.8222\n",
      "Epoch: 38, Average Loss: 0.8154\n",
      "Epoch: 39, Average Loss: 0.8086\n",
      "Epoch: 40, Average Loss: 0.8010\n",
      "Epoch: 41, Average Loss: 0.7973\n",
      "Epoch: 42, Average Loss: 0.7895\n",
      "Epoch: 43, Average Loss: 0.7833\n",
      "Epoch: 44, Average Loss: 0.7774\n",
      "Epoch: 45, Average Loss: 0.7708\n",
      "Epoch: 46, Average Loss: 0.7615\n",
      "Epoch: 47, Average Loss: 0.7560\n",
      "Epoch: 48, Average Loss: 0.7485\n",
      "Epoch: 49, Average Loss: 0.7418\n",
      "Epoch: 50, Average Loss: 0.7352\n",
      "Epoch: 51, Average Loss: 0.7291\n",
      "Epoch: 52, Average Loss: 0.7236\n",
      "Epoch: 53, Average Loss: 0.7172\n",
      "Epoch: 54, Average Loss: 0.7095\n",
      "Epoch: 55, Average Loss: 0.7035\n",
      "Epoch: 56, Average Loss: 0.6975\n",
      "Epoch: 57, Average Loss: 0.6916\n",
      "Epoch: 58, Average Loss: 0.6856\n",
      "Epoch: 59, Average Loss: 0.6793\n",
      "Epoch: 60, Average Loss: 0.6763\n",
      "Epoch: 61, Average Loss: 0.6706\n",
      "Epoch: 62, Average Loss: 0.6628\n",
      "Epoch: 63, Average Loss: 0.6577\n",
      "Epoch: 64, Average Loss: 0.6537\n",
      "Epoch: 65, Average Loss: 0.6523\n",
      "Epoch: 66, Average Loss: 0.6435\n",
      "Epoch: 67, Average Loss: 0.6419\n",
      "Epoch: 68, Average Loss: 0.6353\n",
      "Epoch: 69, Average Loss: 0.6292\n",
      "Epoch: 70, Average Loss: 0.6252\n",
      "Epoch: 71, Average Loss: 0.6218\n",
      "Epoch: 72, Average Loss: 0.6180\n",
      "Epoch: 73, Average Loss: 0.6106\n",
      "Epoch: 74, Average Loss: 0.6082\n",
      "Epoch: 75, Average Loss: 0.6071\n",
      "Epoch: 76, Average Loss: 0.5999\n",
      "Epoch: 77, Average Loss: 0.5974\n",
      "Epoch: 78, Average Loss: 0.5937\n",
      "Epoch: 79, Average Loss: 0.5883\n",
      "Epoch: 80, Average Loss: 0.5856\n",
      "Epoch: 81, Average Loss: 0.5826\n",
      "Epoch: 82, Average Loss: 0.5789\n",
      "Epoch: 83, Average Loss: 0.5756\n",
      "Epoch: 84, Average Loss: 0.5738\n",
      "Epoch: 85, Average Loss: 0.5695\n",
      "Epoch: 86, Average Loss: 0.5648\n",
      "Epoch: 87, Average Loss: 0.5641\n",
      "Epoch: 88, Average Loss: 0.5625\n",
      "Epoch: 89, Average Loss: 0.5561\n",
      "Epoch: 90, Average Loss: 0.5558\n",
      "Epoch: 91, Average Loss: 0.5517\n",
      "Epoch: 92, Average Loss: 0.5514\n",
      "Epoch: 93, Average Loss: 0.5476\n",
      "Epoch: 94, Average Loss: 0.5452\n",
      "Epoch: 95, Average Loss: 0.5432\n",
      "Epoch: 96, Average Loss: 0.5391\n",
      "Epoch: 97, Average Loss: 0.5364\n",
      "Epoch: 98, Average Loss: 0.5349\n",
      "Epoch: 99, Average Loss: 0.5314\n",
      "Epoch: 100, Average Loss: 0.5321\n",
      "Epoch-50: -0.1390128862893396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.25697586671699546\n",
      "Starting transformer_01_P7\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1266\n",
      "Epoch: 2, Average Loss: 1.0264\n",
      "Epoch: 3, Average Loss: 0.9896\n",
      "Epoch: 4, Average Loss: 0.9687\n",
      "Epoch: 5, Average Loss: 0.9547\n",
      "Epoch: 6, Average Loss: 0.9448\n",
      "Epoch: 7, Average Loss: 0.9366\n",
      "Epoch: 8, Average Loss: 0.9307\n",
      "Epoch: 9, Average Loss: 0.9254\n",
      "Epoch: 10, Average Loss: 0.9206\n",
      "Epoch: 11, Average Loss: 0.9164\n",
      "Epoch: 12, Average Loss: 0.9116\n",
      "Epoch: 13, Average Loss: 0.9081\n",
      "Epoch: 14, Average Loss: 0.9041\n",
      "Epoch: 15, Average Loss: 0.9005\n",
      "Epoch: 16, Average Loss: 0.8967\n",
      "Epoch: 17, Average Loss: 0.8926\n",
      "Epoch: 18, Average Loss: 0.8884\n",
      "Epoch: 19, Average Loss: 0.8848\n",
      "Epoch: 20, Average Loss: 0.8804\n",
      "Epoch: 21, Average Loss: 0.8760\n",
      "Epoch: 22, Average Loss: 0.8711\n",
      "Epoch: 23, Average Loss: 0.8671\n",
      "Epoch: 24, Average Loss: 0.8621\n",
      "Epoch: 25, Average Loss: 0.8588\n",
      "Epoch: 26, Average Loss: 0.8523\n",
      "Epoch: 27, Average Loss: 0.8451\n",
      "Epoch: 28, Average Loss: 0.8400\n",
      "Epoch: 29, Average Loss: 0.8333\n",
      "Epoch: 30, Average Loss: 0.8277\n",
      "Epoch: 31, Average Loss: 0.8226\n",
      "Epoch: 32, Average Loss: 0.8149\n",
      "Epoch: 33, Average Loss: 0.8094\n",
      "Epoch: 34, Average Loss: 0.8020\n",
      "Epoch: 35, Average Loss: 0.7964\n",
      "Epoch: 36, Average Loss: 0.7882\n",
      "Epoch: 37, Average Loss: 0.7811\n",
      "Epoch: 38, Average Loss: 0.7738\n",
      "Epoch: 39, Average Loss: 0.7662\n",
      "Epoch: 40, Average Loss: 0.7598\n",
      "Epoch: 41, Average Loss: 0.7512\n",
      "Epoch: 42, Average Loss: 0.7433\n",
      "Epoch: 43, Average Loss: 0.7388\n",
      "Epoch: 44, Average Loss: 0.7305\n",
      "Epoch: 45, Average Loss: 0.7247\n",
      "Epoch: 46, Average Loss: 0.7177\n",
      "Epoch: 47, Average Loss: 0.7108\n",
      "Epoch: 48, Average Loss: 0.7024\n",
      "Epoch: 49, Average Loss: 0.6977\n",
      "Epoch: 50, Average Loss: 0.6897\n",
      "Epoch: 51, Average Loss: 0.6840\n",
      "Epoch: 52, Average Loss: 0.6785\n",
      "Epoch: 53, Average Loss: 0.6723\n",
      "Epoch: 54, Average Loss: 0.6660\n",
      "Epoch: 55, Average Loss: 0.6590\n",
      "Epoch: 56, Average Loss: 0.6542\n",
      "Epoch: 57, Average Loss: 0.6471\n",
      "Epoch: 58, Average Loss: 0.6438\n",
      "Epoch: 59, Average Loss: 0.6368\n",
      "Epoch: 60, Average Loss: 0.6326\n",
      "Epoch: 61, Average Loss: 0.6301\n",
      "Epoch: 62, Average Loss: 0.6217\n",
      "Epoch: 63, Average Loss: 0.6125\n",
      "Epoch: 64, Average Loss: 0.6098\n",
      "Epoch: 65, Average Loss: 0.6074\n",
      "Epoch: 66, Average Loss: 0.6012\n",
      "Epoch: 67, Average Loss: 0.5972\n",
      "Epoch: 68, Average Loss: 0.5910\n",
      "Epoch: 69, Average Loss: 0.5863\n",
      "Epoch: 70, Average Loss: 0.5839\n",
      "Epoch: 71, Average Loss: 0.5850\n",
      "Epoch: 72, Average Loss: 0.5741\n",
      "Epoch: 73, Average Loss: 0.5722\n",
      "Epoch: 74, Average Loss: 0.5685\n",
      "Epoch: 75, Average Loss: 0.5653\n",
      "Epoch: 76, Average Loss: 0.5617\n",
      "Epoch: 77, Average Loss: 0.5536\n",
      "Epoch: 78, Average Loss: 0.5537\n",
      "Epoch: 79, Average Loss: 0.5491\n",
      "Epoch: 80, Average Loss: 0.5444\n",
      "Epoch: 81, Average Loss: 0.5438\n",
      "Epoch: 82, Average Loss: 0.5392\n",
      "Epoch: 83, Average Loss: 0.5357\n",
      "Epoch: 84, Average Loss: 0.5303\n",
      "Epoch: 85, Average Loss: 0.5303\n",
      "Epoch: 86, Average Loss: 0.5269\n",
      "Epoch: 87, Average Loss: 0.5248\n",
      "Epoch: 88, Average Loss: 0.5224\n",
      "Epoch: 89, Average Loss: 0.5186\n",
      "Epoch: 90, Average Loss: 0.5153\n",
      "Epoch: 91, Average Loss: 0.5127\n",
      "Epoch: 92, Average Loss: 0.5090\n",
      "Epoch: 93, Average Loss: 0.5065\n",
      "Epoch: 94, Average Loss: 0.5061\n",
      "Epoch: 95, Average Loss: 0.5044\n",
      "Epoch: 96, Average Loss: 0.4998\n",
      "Epoch: 97, Average Loss: 0.4976\n",
      "Epoch: 98, Average Loss: 0.4952\n",
      "Epoch: 99, Average Loss: 0.4916\n",
      "Epoch: 100, Average Loss: 0.4892\n",
      "Epoch-50: -0.12066515230284525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.24241549907865068\n",
      "Starting transformer_01_P3\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1502\n",
      "Epoch: 2, Average Loss: 1.0445\n",
      "Epoch: 3, Average Loss: 1.0047\n",
      "Epoch: 4, Average Loss: 0.9819\n",
      "Epoch: 5, Average Loss: 0.9674\n",
      "Epoch: 6, Average Loss: 0.9570\n",
      "Epoch: 7, Average Loss: 0.9491\n",
      "Epoch: 8, Average Loss: 0.9424\n",
      "Epoch: 9, Average Loss: 0.9367\n",
      "Epoch: 10, Average Loss: 0.9319\n",
      "Epoch: 11, Average Loss: 0.9273\n",
      "Epoch: 12, Average Loss: 0.9230\n",
      "Epoch: 13, Average Loss: 0.9189\n",
      "Epoch: 14, Average Loss: 0.9147\n",
      "Epoch: 15, Average Loss: 0.9110\n",
      "Epoch: 16, Average Loss: 0.9074\n",
      "Epoch: 17, Average Loss: 0.9034\n",
      "Epoch: 18, Average Loss: 0.8997\n",
      "Epoch: 19, Average Loss: 0.8958\n",
      "Epoch: 20, Average Loss: 0.8922\n",
      "Epoch: 21, Average Loss: 0.8882\n",
      "Epoch: 22, Average Loss: 0.8845\n",
      "Epoch: 23, Average Loss: 0.8799\n",
      "Epoch: 24, Average Loss: 0.8751\n",
      "Epoch: 25, Average Loss: 0.8716\n",
      "Epoch: 26, Average Loss: 0.8662\n",
      "Epoch: 27, Average Loss: 0.8619\n",
      "Epoch: 28, Average Loss: 0.8560\n",
      "Epoch: 29, Average Loss: 0.8505\n",
      "Epoch: 30, Average Loss: 0.8457\n",
      "Epoch: 31, Average Loss: 0.8389\n",
      "Epoch: 32, Average Loss: 0.8329\n",
      "Epoch: 33, Average Loss: 0.8278\n",
      "Epoch: 34, Average Loss: 0.8193\n",
      "Epoch: 35, Average Loss: 0.8129\n",
      "Epoch: 36, Average Loss: 0.8055\n",
      "Epoch: 37, Average Loss: 0.7991\n",
      "Epoch: 38, Average Loss: 0.7930\n",
      "Epoch: 39, Average Loss: 0.7843\n",
      "Epoch: 40, Average Loss: 0.7769\n",
      "Epoch: 41, Average Loss: 0.7685\n",
      "Epoch: 42, Average Loss: 0.7640\n",
      "Epoch: 43, Average Loss: 0.7571\n",
      "Epoch: 44, Average Loss: 0.7479\n",
      "Epoch: 45, Average Loss: 0.7424\n",
      "Epoch: 46, Average Loss: 0.7329\n",
      "Epoch: 47, Average Loss: 0.7284\n",
      "Epoch: 48, Average Loss: 0.7187\n",
      "Epoch: 49, Average Loss: 0.7144\n",
      "Epoch: 50, Average Loss: 0.7057\n",
      "Epoch: 51, Average Loss: 0.7002\n",
      "Epoch: 52, Average Loss: 0.6940\n",
      "Epoch: 53, Average Loss: 0.6870\n",
      "Epoch: 54, Average Loss: 0.6809\n",
      "Epoch: 55, Average Loss: 0.6738\n",
      "Epoch: 56, Average Loss: 0.6699\n",
      "Epoch: 57, Average Loss: 0.6651\n",
      "Epoch: 58, Average Loss: 0.6597\n",
      "Epoch: 59, Average Loss: 0.6521\n",
      "Epoch: 60, Average Loss: 0.6505\n",
      "Epoch: 61, Average Loss: 0.6440\n",
      "Epoch: 62, Average Loss: 0.6373\n",
      "Epoch: 63, Average Loss: 0.6330\n",
      "Epoch: 64, Average Loss: 0.6259\n",
      "Epoch: 65, Average Loss: 0.6245\n",
      "Epoch: 66, Average Loss: 0.6174\n",
      "Epoch: 67, Average Loss: 0.6113\n",
      "Epoch: 68, Average Loss: 0.6058\n",
      "Epoch: 69, Average Loss: 0.6044\n",
      "Epoch: 70, Average Loss: 0.5972\n",
      "Epoch: 71, Average Loss: 0.5979\n",
      "Epoch: 72, Average Loss: 0.5904\n",
      "Epoch: 73, Average Loss: 0.5873\n",
      "Epoch: 74, Average Loss: 0.5843\n",
      "Epoch: 75, Average Loss: 0.5803\n",
      "Epoch: 76, Average Loss: 0.5749\n",
      "Epoch: 77, Average Loss: 0.5737\n",
      "Epoch: 78, Average Loss: 0.5683\n",
      "Epoch: 79, Average Loss: 0.5655\n",
      "Epoch: 80, Average Loss: 0.5634\n",
      "Epoch: 81, Average Loss: 0.5587\n",
      "Epoch: 82, Average Loss: 0.5554\n",
      "Epoch: 83, Average Loss: 0.5540\n",
      "Epoch: 84, Average Loss: 0.5495\n",
      "Epoch: 85, Average Loss: 0.5438\n",
      "Epoch: 86, Average Loss: 0.5415\n",
      "Epoch: 87, Average Loss: 0.5395\n",
      "Epoch: 88, Average Loss: 0.5382\n",
      "Epoch: 89, Average Loss: 0.5340\n",
      "Epoch: 90, Average Loss: 0.5318\n",
      "Epoch: 91, Average Loss: 0.5274\n",
      "Epoch: 92, Average Loss: 0.5253\n",
      "Epoch: 93, Average Loss: 0.5246\n",
      "Epoch: 94, Average Loss: 0.5209\n",
      "Epoch: 95, Average Loss: 0.5200\n",
      "Epoch: 96, Average Loss: 0.5179\n",
      "Epoch: 97, Average Loss: 0.5136\n",
      "Epoch: 98, Average Loss: 0.5121\n",
      "Epoch: 99, Average Loss: 0.5084\n",
      "Epoch: 100, Average Loss: 0.5068\n",
      "Epoch-50: -0.11494861978043902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-47654705/ipykernel_942967/3130641603.py:264: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 4))\n",
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.20458324059294064\n",
      "Starting transformer_01_Pz\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1021\n",
      "Epoch: 2, Average Loss: 0.9970\n",
      "Epoch: 3, Average Loss: 0.9583\n",
      "Epoch: 4, Average Loss: 0.9366\n",
      "Epoch: 5, Average Loss: 0.9226\n",
      "Epoch: 6, Average Loss: 0.9126\n",
      "Epoch: 7, Average Loss: 0.9048\n",
      "Epoch: 8, Average Loss: 0.8986\n",
      "Epoch: 9, Average Loss: 0.8930\n",
      "Epoch: 10, Average Loss: 0.8879\n",
      "Epoch: 11, Average Loss: 0.8836\n",
      "Epoch: 12, Average Loss: 0.8790\n",
      "Epoch: 13, Average Loss: 0.8751\n",
      "Epoch: 14, Average Loss: 0.8709\n",
      "Epoch: 15, Average Loss: 0.8670\n",
      "Epoch: 16, Average Loss: 0.8631\n",
      "Epoch: 17, Average Loss: 0.8590\n",
      "Epoch: 18, Average Loss: 0.8554\n",
      "Epoch: 19, Average Loss: 0.8510\n",
      "Epoch: 20, Average Loss: 0.8463\n",
      "Epoch: 21, Average Loss: 0.8426\n",
      "Epoch: 22, Average Loss: 0.8386\n",
      "Epoch: 23, Average Loss: 0.8340\n",
      "Epoch: 24, Average Loss: 0.8296\n",
      "Epoch: 25, Average Loss: 0.8250\n",
      "Epoch: 26, Average Loss: 0.8206\n",
      "Epoch: 27, Average Loss: 0.8150\n",
      "Epoch: 28, Average Loss: 0.8110\n",
      "Epoch: 29, Average Loss: 0.8056\n",
      "Epoch: 30, Average Loss: 0.8003\n",
      "Epoch: 31, Average Loss: 0.7946\n",
      "Epoch: 32, Average Loss: 0.7899\n",
      "Epoch: 33, Average Loss: 0.7832\n",
      "Epoch: 34, Average Loss: 0.7772\n",
      "Epoch: 35, Average Loss: 0.7714\n",
      "Epoch: 36, Average Loss: 0.7647\n",
      "Epoch: 37, Average Loss: 0.7588\n",
      "Epoch: 38, Average Loss: 0.7520\n",
      "Epoch: 39, Average Loss: 0.7457\n",
      "Epoch: 40, Average Loss: 0.7399\n",
      "Epoch: 41, Average Loss: 0.7344\n",
      "Epoch: 42, Average Loss: 0.7264\n",
      "Epoch: 43, Average Loss: 0.7202\n",
      "Epoch: 44, Average Loss: 0.7133\n",
      "Epoch: 45, Average Loss: 0.7083\n",
      "Epoch: 46, Average Loss: 0.7012\n",
      "Epoch: 47, Average Loss: 0.6951\n",
      "Epoch: 48, Average Loss: 0.6875\n",
      "Epoch: 49, Average Loss: 0.6824\n",
      "Epoch: 50, Average Loss: 0.6773\n",
      "Epoch: 51, Average Loss: 0.6706\n",
      "Epoch: 52, Average Loss: 0.6661\n",
      "Epoch: 53, Average Loss: 0.6631\n",
      "Epoch: 54, Average Loss: 0.6530\n",
      "Epoch: 55, Average Loss: 0.6471\n",
      "Epoch: 56, Average Loss: 0.6433\n",
      "Epoch: 57, Average Loss: 0.6388\n",
      "Epoch: 58, Average Loss: 0.6340\n",
      "Epoch: 59, Average Loss: 0.6294\n",
      "Epoch: 60, Average Loss: 0.6225\n",
      "Epoch: 61, Average Loss: 0.6178\n",
      "Epoch: 62, Average Loss: 0.6115\n",
      "Epoch: 63, Average Loss: 0.6093\n",
      "Epoch: 64, Average Loss: 0.6034\n",
      "Epoch: 65, Average Loss: 0.5971\n",
      "Epoch: 66, Average Loss: 0.5969\n",
      "Epoch: 67, Average Loss: 0.5893\n",
      "Epoch: 68, Average Loss: 0.5850\n",
      "Epoch: 69, Average Loss: 0.5793\n",
      "Epoch: 70, Average Loss: 0.5766\n",
      "Epoch: 71, Average Loss: 0.5727\n",
      "Epoch: 72, Average Loss: 0.5731\n",
      "Epoch: 73, Average Loss: 0.5643\n",
      "Epoch: 74, Average Loss: 0.5601\n",
      "Epoch: 75, Average Loss: 0.5576\n",
      "Epoch: 76, Average Loss: 0.5529\n",
      "Epoch: 77, Average Loss: 0.5516\n",
      "Epoch: 78, Average Loss: 0.5483\n",
      "Epoch: 79, Average Loss: 0.5436\n",
      "Epoch: 80, Average Loss: 0.5414\n",
      "Epoch: 81, Average Loss: 0.5394\n",
      "Epoch: 82, Average Loss: 0.5353\n",
      "Epoch: 83, Average Loss: 0.5305\n",
      "Epoch: 84, Average Loss: 0.5283\n",
      "Epoch: 85, Average Loss: 0.5255\n",
      "Epoch: 86, Average Loss: 0.5227\n",
      "Epoch: 87, Average Loss: 0.5201\n",
      "Epoch: 88, Average Loss: 0.5183\n",
      "Epoch: 89, Average Loss: 0.5140\n",
      "Epoch: 90, Average Loss: 0.5118\n",
      "Epoch: 91, Average Loss: 0.5119\n",
      "Epoch: 92, Average Loss: 0.5104\n",
      "Epoch: 93, Average Loss: 0.5044\n",
      "Epoch: 94, Average Loss: 0.5014\n",
      "Epoch: 95, Average Loss: 0.5006\n",
      "Epoch: 96, Average Loss: 0.4981\n",
      "Epoch: 97, Average Loss: 0.4947\n",
      "Epoch: 98, Average Loss: 0.4928\n",
      "Epoch: 99, Average Loss: 0.4900\n",
      "Epoch: 100, Average Loss: 0.4881\n",
      "Epoch-50: -0.1631081481366543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.3149634995853865\n",
      "Starting transformer_01_POz\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.0791\n",
      "Epoch: 2, Average Loss: 0.9815\n",
      "Epoch: 3, Average Loss: 0.9457\n",
      "Epoch: 4, Average Loss: 0.9255\n",
      "Epoch: 5, Average Loss: 0.9120\n",
      "Epoch: 6, Average Loss: 0.9032\n",
      "Epoch: 7, Average Loss: 0.8958\n",
      "Epoch: 8, Average Loss: 0.8902\n",
      "Epoch: 9, Average Loss: 0.8857\n",
      "Epoch: 10, Average Loss: 0.8818\n",
      "Epoch: 11, Average Loss: 0.8779\n",
      "Epoch: 12, Average Loss: 0.8750\n",
      "Epoch: 13, Average Loss: 0.8712\n",
      "Epoch: 14, Average Loss: 0.8679\n",
      "Epoch: 15, Average Loss: 0.8651\n",
      "Epoch: 16, Average Loss: 0.8614\n",
      "Epoch: 17, Average Loss: 0.8581\n",
      "Epoch: 18, Average Loss: 0.8547\n",
      "Epoch: 19, Average Loss: 0.8514\n",
      "Epoch: 20, Average Loss: 0.8477\n",
      "Epoch: 21, Average Loss: 0.8440\n",
      "Epoch: 22, Average Loss: 0.8400\n",
      "Epoch: 23, Average Loss: 0.8369\n",
      "Epoch: 24, Average Loss: 0.8320\n",
      "Epoch: 25, Average Loss: 0.8276\n",
      "Epoch: 26, Average Loss: 0.8229\n",
      "Epoch: 27, Average Loss: 0.8179\n",
      "Epoch: 28, Average Loss: 0.8138\n",
      "Epoch: 29, Average Loss: 0.8076\n",
      "Epoch: 30, Average Loss: 0.8028\n",
      "Epoch: 31, Average Loss: 0.7970\n",
      "Epoch: 32, Average Loss: 0.7920\n",
      "Epoch: 33, Average Loss: 0.7856\n",
      "Epoch: 34, Average Loss: 0.7812\n",
      "Epoch: 35, Average Loss: 0.7738\n",
      "Epoch: 36, Average Loss: 0.7694\n",
      "Epoch: 37, Average Loss: 0.7627\n",
      "Epoch: 38, Average Loss: 0.7560\n",
      "Epoch: 39, Average Loss: 0.7489\n",
      "Epoch: 40, Average Loss: 0.7461\n",
      "Epoch: 41, Average Loss: 0.7384\n",
      "Epoch: 42, Average Loss: 0.7319\n",
      "Epoch: 43, Average Loss: 0.7305\n",
      "Epoch: 44, Average Loss: 0.7205\n",
      "Epoch: 45, Average Loss: 0.7143\n",
      "Epoch: 46, Average Loss: 0.7076\n",
      "Epoch: 47, Average Loss: 0.7018\n",
      "Epoch: 48, Average Loss: 0.6950\n",
      "Epoch: 49, Average Loss: 0.6897\n",
      "Epoch: 50, Average Loss: 0.6859\n",
      "Epoch: 51, Average Loss: 0.6803\n",
      "Epoch: 52, Average Loss: 0.6752\n",
      "Epoch: 53, Average Loss: 0.6675\n",
      "Epoch: 54, Average Loss: 0.6619\n",
      "Epoch: 55, Average Loss: 0.6587\n",
      "Epoch: 56, Average Loss: 0.6521\n",
      "Epoch: 57, Average Loss: 0.6477\n",
      "Epoch: 58, Average Loss: 0.6420\n",
      "Epoch: 59, Average Loss: 0.6375\n",
      "Epoch: 60, Average Loss: 0.6349\n",
      "Epoch: 61, Average Loss: 0.6270\n",
      "Epoch: 62, Average Loss: 0.6232\n",
      "Epoch: 63, Average Loss: 0.6184\n",
      "Epoch: 64, Average Loss: 0.6139\n",
      "Epoch: 65, Average Loss: 0.6104\n",
      "Epoch: 66, Average Loss: 0.6038\n",
      "Epoch: 67, Average Loss: 0.6006\n",
      "Epoch: 68, Average Loss: 0.5979\n",
      "Epoch: 69, Average Loss: 0.5955\n",
      "Epoch: 70, Average Loss: 0.5899\n",
      "Epoch: 71, Average Loss: 0.5851\n",
      "Epoch: 72, Average Loss: 0.5807\n",
      "Epoch: 73, Average Loss: 0.5761\n",
      "Epoch: 74, Average Loss: 0.5718\n",
      "Epoch: 75, Average Loss: 0.5686\n",
      "Epoch: 76, Average Loss: 0.5651\n",
      "Epoch: 77, Average Loss: 0.5620\n",
      "Epoch: 78, Average Loss: 0.5594\n",
      "Epoch: 79, Average Loss: 0.5542\n",
      "Epoch: 80, Average Loss: 0.5519\n",
      "Epoch: 81, Average Loss: 0.5518\n",
      "Epoch: 82, Average Loss: 0.5440\n",
      "Epoch: 83, Average Loss: 0.5403\n",
      "Epoch: 84, Average Loss: 0.5381\n",
      "Epoch: 85, Average Loss: 0.5366\n",
      "Epoch: 86, Average Loss: 0.5330\n",
      "Epoch: 87, Average Loss: 0.5298\n",
      "Epoch: 88, Average Loss: 0.5279\n",
      "Epoch: 89, Average Loss: 0.5251\n",
      "Epoch: 90, Average Loss: 0.5215\n",
      "Epoch: 91, Average Loss: 0.5231\n",
      "Epoch: 92, Average Loss: 0.5169\n",
      "Epoch: 93, Average Loss: 0.5156\n",
      "Epoch: 94, Average Loss: 0.5132\n",
      "Epoch: 95, Average Loss: 0.5101\n",
      "Epoch: 96, Average Loss: 0.5058\n",
      "Epoch: 97, Average Loss: 0.5056\n",
      "Epoch: 98, Average Loss: 0.5015\n",
      "Epoch: 99, Average Loss: 0.4999\n",
      "Epoch: 100, Average Loss: 0.4985\n",
      "Epoch-50: -0.1022896058066205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.25071853981386916\n",
      "Starting transformer_01_O1\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.1097\n",
      "Epoch: 2, Average Loss: 1.0104\n",
      "Epoch: 3, Average Loss: 0.9739\n",
      "Epoch: 4, Average Loss: 0.9533\n",
      "Epoch: 5, Average Loss: 0.9399\n",
      "Epoch: 6, Average Loss: 0.9302\n",
      "Epoch: 7, Average Loss: 0.9230\n",
      "Epoch: 8, Average Loss: 0.9172\n",
      "Epoch: 9, Average Loss: 0.9125\n",
      "Epoch: 10, Average Loss: 0.9080\n",
      "Epoch: 11, Average Loss: 0.9041\n",
      "Epoch: 12, Average Loss: 0.9004\n",
      "Epoch: 13, Average Loss: 0.8969\n",
      "Epoch: 14, Average Loss: 0.8935\n",
      "Epoch: 15, Average Loss: 0.8903\n",
      "Epoch: 16, Average Loss: 0.8869\n",
      "Epoch: 17, Average Loss: 0.8834\n",
      "Epoch: 18, Average Loss: 0.8798\n",
      "Epoch: 19, Average Loss: 0.8762\n",
      "Epoch: 20, Average Loss: 0.8724\n",
      "Epoch: 21, Average Loss: 0.8681\n",
      "Epoch: 22, Average Loss: 0.8639\n",
      "Epoch: 23, Average Loss: 0.8597\n",
      "Epoch: 24, Average Loss: 0.8549\n",
      "Epoch: 25, Average Loss: 0.8504\n",
      "Epoch: 26, Average Loss: 0.8452\n",
      "Epoch: 27, Average Loss: 0.8398\n",
      "Epoch: 28, Average Loss: 0.8346\n",
      "Epoch: 29, Average Loss: 0.8289\n",
      "Epoch: 30, Average Loss: 0.8218\n",
      "Epoch: 31, Average Loss: 0.8162\n",
      "Epoch: 32, Average Loss: 0.8089\n",
      "Epoch: 33, Average Loss: 0.8033\n",
      "Epoch: 34, Average Loss: 0.7956\n",
      "Epoch: 35, Average Loss: 0.7885\n",
      "Epoch: 36, Average Loss: 0.7832\n",
      "Epoch: 37, Average Loss: 0.7754\n",
      "Epoch: 38, Average Loss: 0.7675\n",
      "Epoch: 39, Average Loss: 0.7606\n",
      "Epoch: 40, Average Loss: 0.7538\n",
      "Epoch: 41, Average Loss: 0.7480\n",
      "Epoch: 42, Average Loss: 0.7425\n",
      "Epoch: 43, Average Loss: 0.7367\n",
      "Epoch: 44, Average Loss: 0.7291\n",
      "Epoch: 45, Average Loss: 0.7232\n",
      "Epoch: 46, Average Loss: 0.7157\n",
      "Epoch: 47, Average Loss: 0.7106\n",
      "Epoch: 48, Average Loss: 0.7039\n",
      "Epoch: 49, Average Loss: 0.7002\n",
      "Epoch: 50, Average Loss: 0.6924\n",
      "Epoch: 51, Average Loss: 0.6861\n",
      "Epoch: 52, Average Loss: 0.6800\n",
      "Epoch: 53, Average Loss: 0.6721\n",
      "Epoch: 54, Average Loss: 0.6669\n",
      "Epoch: 55, Average Loss: 0.6611\n",
      "Epoch: 56, Average Loss: 0.6580\n",
      "Epoch: 57, Average Loss: 0.6519\n",
      "Epoch: 58, Average Loss: 0.6478\n",
      "Epoch: 59, Average Loss: 0.6427\n",
      "Epoch: 60, Average Loss: 0.6322\n",
      "Epoch: 61, Average Loss: 0.6292\n",
      "Epoch: 62, Average Loss: 0.6265\n",
      "Epoch: 63, Average Loss: 0.6211\n",
      "Epoch: 64, Average Loss: 0.6164\n",
      "Epoch: 65, Average Loss: 0.6128\n",
      "Epoch: 66, Average Loss: 0.6057\n",
      "Epoch: 67, Average Loss: 0.6015\n",
      "Epoch: 68, Average Loss: 0.5971\n",
      "Epoch: 69, Average Loss: 0.5912\n",
      "Epoch: 70, Average Loss: 0.5876\n",
      "Epoch: 71, Average Loss: 0.5812\n",
      "Epoch: 72, Average Loss: 0.5834\n",
      "Epoch: 73, Average Loss: 0.5757\n",
      "Epoch: 74, Average Loss: 0.5699\n",
      "Epoch: 75, Average Loss: 0.5710\n",
      "Epoch: 76, Average Loss: 0.5632\n",
      "Epoch: 77, Average Loss: 0.5588\n",
      "Epoch: 78, Average Loss: 0.5580\n",
      "Epoch: 79, Average Loss: 0.5529\n",
      "Epoch: 80, Average Loss: 0.5472\n",
      "Epoch: 81, Average Loss: 0.5442\n",
      "Epoch: 82, Average Loss: 0.5436\n",
      "Epoch: 83, Average Loss: 0.5372\n",
      "Epoch: 84, Average Loss: 0.5347\n",
      "Epoch: 85, Average Loss: 0.5304\n",
      "Epoch: 86, Average Loss: 0.5313\n",
      "Epoch: 87, Average Loss: 0.5241\n",
      "Epoch: 88, Average Loss: 0.5229\n",
      "Epoch: 89, Average Loss: 0.5217\n",
      "Epoch: 90, Average Loss: 0.5185\n",
      "Epoch: 91, Average Loss: 0.5129\n",
      "Epoch: 92, Average Loss: 0.5115\n",
      "Epoch: 93, Average Loss: 0.5087\n",
      "Epoch: 94, Average Loss: 0.5048\n",
      "Epoch: 95, Average Loss: 0.5048\n",
      "Epoch: 96, Average Loss: 0.5032\n",
      "Epoch: 97, Average Loss: 0.5008\n",
      "Epoch: 98, Average Loss: 0.4970\n",
      "Epoch: 99, Average Loss: 0.4946\n",
      "Epoch: 100, Average Loss: 0.4920\n",
      "Epoch-50: -0.1604295829109077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.29505702573490944\n",
      "Starting transformer_01_FP2\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.3842\n",
      "Epoch: 2, Average Loss: 1.2884\n",
      "Epoch: 3, Average Loss: 1.2537\n",
      "Epoch: 4, Average Loss: 1.2340\n",
      "Epoch: 5, Average Loss: 1.2216\n",
      "Epoch: 6, Average Loss: 1.2129\n",
      "Epoch: 7, Average Loss: 1.2066\n",
      "Epoch: 8, Average Loss: 1.2019\n",
      "Epoch: 9, Average Loss: 1.1982\n",
      "Epoch: 10, Average Loss: 1.1950\n",
      "Epoch: 11, Average Loss: 1.1927\n",
      "Epoch: 12, Average Loss: 1.1904\n",
      "Epoch: 13, Average Loss: 1.1885\n",
      "Epoch: 14, Average Loss: 1.1865\n",
      "Epoch: 15, Average Loss: 1.1847\n",
      "Epoch: 16, Average Loss: 1.1833\n",
      "Epoch: 17, Average Loss: 1.1817\n",
      "Epoch: 18, Average Loss: 1.1801\n",
      "Epoch: 19, Average Loss: 1.1787\n",
      "Epoch: 20, Average Loss: 1.1773\n",
      "Epoch: 21, Average Loss: 1.1758\n",
      "Epoch: 22, Average Loss: 1.1743\n",
      "Epoch: 23, Average Loss: 1.1726\n",
      "Epoch: 24, Average Loss: 1.1708\n",
      "Epoch: 25, Average Loss: 1.1695\n",
      "Epoch: 26, Average Loss: 1.1673\n",
      "Epoch: 27, Average Loss: 1.1656\n",
      "Epoch: 28, Average Loss: 1.1638\n",
      "Epoch: 29, Average Loss: 1.1616\n",
      "Epoch: 30, Average Loss: 1.1591\n",
      "Epoch: 31, Average Loss: 1.1575\n",
      "Epoch: 32, Average Loss: 1.1552\n",
      "Epoch: 33, Average Loss: 1.1528\n",
      "Epoch: 34, Average Loss: 1.1501\n",
      "Epoch: 35, Average Loss: 1.1477\n",
      "Epoch: 36, Average Loss: 1.1454\n",
      "Epoch: 37, Average Loss: 1.1427\n",
      "Epoch: 38, Average Loss: 1.1394\n",
      "Epoch: 39, Average Loss: 1.1379\n",
      "Epoch: 40, Average Loss: 1.1335\n",
      "Epoch: 41, Average Loss: 1.1316\n",
      "Epoch: 42, Average Loss: 1.1276\n",
      "Epoch: 43, Average Loss: 1.1239\n",
      "Epoch: 44, Average Loss: 1.1203\n",
      "Epoch: 45, Average Loss: 1.1166\n",
      "Epoch: 46, Average Loss: 1.1133\n",
      "Epoch: 47, Average Loss: 1.1091\n",
      "Epoch: 48, Average Loss: 1.1052\n",
      "Epoch: 49, Average Loss: 1.1010\n",
      "Epoch: 50, Average Loss: 1.0955\n",
      "Epoch: 51, Average Loss: 1.0920\n",
      "Epoch: 52, Average Loss: 1.0876\n",
      "Epoch: 53, Average Loss: 1.0855\n",
      "Epoch: 54, Average Loss: 1.0790\n",
      "Epoch: 55, Average Loss: 1.0733\n",
      "Epoch: 56, Average Loss: 1.0678\n",
      "Epoch: 57, Average Loss: 1.0637\n",
      "Epoch: 58, Average Loss: 1.0596\n",
      "Epoch: 59, Average Loss: 1.0544\n",
      "Epoch: 60, Average Loss: 1.0504\n",
      "Epoch: 61, Average Loss: 1.0477\n",
      "Epoch: 62, Average Loss: 1.0405\n",
      "Epoch: 63, Average Loss: 1.0355\n",
      "Epoch: 64, Average Loss: 1.0307\n",
      "Epoch: 65, Average Loss: 1.0266\n",
      "Epoch: 66, Average Loss: 1.0243\n",
      "Epoch: 67, Average Loss: 1.0191\n",
      "Epoch: 68, Average Loss: 1.0177\n",
      "Epoch: 69, Average Loss: 1.0118\n",
      "Epoch: 70, Average Loss: 1.0063\n",
      "Epoch: 71, Average Loss: 1.0001\n",
      "Epoch: 72, Average Loss: 0.9962\n",
      "Epoch: 73, Average Loss: 0.9943\n",
      "Epoch: 74, Average Loss: 0.9899\n",
      "Epoch: 75, Average Loss: 0.9859\n",
      "Epoch: 76, Average Loss: 0.9832\n",
      "Epoch: 77, Average Loss: 0.9799\n",
      "Epoch: 78, Average Loss: 0.9746\n",
      "Epoch: 79, Average Loss: 0.9716\n",
      "Epoch: 80, Average Loss: 0.9674\n",
      "Epoch: 81, Average Loss: 0.9653\n",
      "Epoch: 82, Average Loss: 0.9611\n",
      "Epoch: 83, Average Loss: 0.9570\n",
      "Epoch: 84, Average Loss: 0.9534\n",
      "Epoch: 85, Average Loss: 0.9514\n",
      "Epoch: 86, Average Loss: 0.9476\n",
      "Epoch: 87, Average Loss: 0.9442\n",
      "Epoch: 88, Average Loss: 0.9405\n",
      "Epoch: 89, Average Loss: 0.9386\n",
      "Epoch: 90, Average Loss: 0.9334\n",
      "Epoch: 91, Average Loss: 0.9307\n",
      "Epoch: 92, Average Loss: 0.9275\n",
      "Epoch: 93, Average Loss: 0.9241\n",
      "Epoch: 94, Average Loss: 0.9232\n",
      "Epoch: 95, Average Loss: 0.9192\n",
      "Epoch: 96, Average Loss: 0.9158\n",
      "Epoch: 97, Average Loss: 0.9145\n",
      "Epoch: 98, Average Loss: 0.9120\n",
      "Epoch: 99, Average Loss: 0.9100\n",
      "Epoch: 100, Average Loss: 0.9054\n",
      "Epoch-50: -0.03147484975392878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.10078590852558822\n",
      "Starting transformer_01_AFF6h\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.3010\n",
      "Epoch: 2, Average Loss: 1.2064\n",
      "Epoch: 3, Average Loss: 1.1715\n",
      "Epoch: 4, Average Loss: 1.1515\n",
      "Epoch: 5, Average Loss: 1.1389\n",
      "Epoch: 6, Average Loss: 1.1302\n",
      "Epoch: 7, Average Loss: 1.1240\n",
      "Epoch: 8, Average Loss: 1.1193\n",
      "Epoch: 9, Average Loss: 1.1152\n",
      "Epoch: 10, Average Loss: 1.1120\n",
      "Epoch: 11, Average Loss: 1.1092\n",
      "Epoch: 12, Average Loss: 1.1068\n",
      "Epoch: 13, Average Loss: 1.1047\n",
      "Epoch: 14, Average Loss: 1.1027\n",
      "Epoch: 15, Average Loss: 1.1006\n",
      "Epoch: 16, Average Loss: 1.0991\n",
      "Epoch: 17, Average Loss: 1.0969\n",
      "Epoch: 18, Average Loss: 1.0953\n",
      "Epoch: 19, Average Loss: 1.0934\n",
      "Epoch: 20, Average Loss: 1.0913\n",
      "Epoch: 21, Average Loss: 1.0899\n",
      "Epoch: 22, Average Loss: 1.0883\n",
      "Epoch: 23, Average Loss: 1.0861\n",
      "Epoch: 24, Average Loss: 1.0841\n",
      "Epoch: 25, Average Loss: 1.0820\n",
      "Epoch: 26, Average Loss: 1.0798\n",
      "Epoch: 27, Average Loss: 1.0776\n",
      "Epoch: 28, Average Loss: 1.0756\n",
      "Epoch: 29, Average Loss: 1.0734\n",
      "Epoch: 30, Average Loss: 1.0714\n",
      "Epoch: 31, Average Loss: 1.0683\n",
      "Epoch: 32, Average Loss: 1.0659\n",
      "Epoch: 33, Average Loss: 1.0634\n",
      "Epoch: 34, Average Loss: 1.0609\n",
      "Epoch: 35, Average Loss: 1.0584\n",
      "Epoch: 36, Average Loss: 1.0551\n",
      "Epoch: 37, Average Loss: 1.0518\n",
      "Epoch: 38, Average Loss: 1.0482\n",
      "Epoch: 39, Average Loss: 1.0443\n",
      "Epoch: 40, Average Loss: 1.0413\n",
      "Epoch: 41, Average Loss: 1.0379\n",
      "Epoch: 42, Average Loss: 1.0337\n",
      "Epoch: 43, Average Loss: 1.0297\n",
      "Epoch: 44, Average Loss: 1.0267\n",
      "Epoch: 45, Average Loss: 1.0217\n",
      "Epoch: 46, Average Loss: 1.0176\n",
      "Epoch: 47, Average Loss: 1.0142\n",
      "Epoch: 48, Average Loss: 1.0087\n",
      "Epoch: 49, Average Loss: 1.0051\n",
      "Epoch: 50, Average Loss: 0.9997\n",
      "Epoch: 51, Average Loss: 0.9943\n",
      "Epoch: 52, Average Loss: 0.9898\n",
      "Epoch: 53, Average Loss: 0.9863\n",
      "Epoch: 54, Average Loss: 0.9805\n",
      "Epoch: 55, Average Loss: 0.9751\n",
      "Epoch: 56, Average Loss: 0.9713\n",
      "Epoch: 57, Average Loss: 0.9666\n",
      "Epoch: 58, Average Loss: 0.9615\n",
      "Epoch: 59, Average Loss: 0.9567\n",
      "Epoch: 60, Average Loss: 0.9541\n",
      "Epoch: 61, Average Loss: 0.9475\n",
      "Epoch: 62, Average Loss: 0.9442\n",
      "Epoch: 63, Average Loss: 0.9400\n",
      "Epoch: 64, Average Loss: 0.9352\n",
      "Epoch: 65, Average Loss: 0.9308\n",
      "Epoch: 66, Average Loss: 0.9270\n",
      "Epoch: 67, Average Loss: 0.9230\n",
      "Epoch: 68, Average Loss: 0.9218\n",
      "Epoch: 69, Average Loss: 0.9148\n",
      "Epoch: 70, Average Loss: 0.9118\n",
      "Epoch: 71, Average Loss: 0.9073\n",
      "Epoch: 72, Average Loss: 0.9041\n",
      "Epoch: 73, Average Loss: 0.8993\n",
      "Epoch: 74, Average Loss: 0.8952\n",
      "Epoch: 75, Average Loss: 0.8924\n",
      "Epoch: 76, Average Loss: 0.8881\n",
      "Epoch: 77, Average Loss: 0.8860\n",
      "Epoch: 78, Average Loss: 0.8809\n",
      "Epoch: 79, Average Loss: 0.8786\n",
      "Epoch: 80, Average Loss: 0.8766\n",
      "Epoch: 81, Average Loss: 0.8731\n",
      "Epoch: 82, Average Loss: 0.8693\n",
      "Epoch: 83, Average Loss: 0.8665\n",
      "Epoch: 84, Average Loss: 0.8602\n",
      "Epoch: 85, Average Loss: 0.8594\n",
      "Epoch: 86, Average Loss: 0.8559\n",
      "Epoch: 87, Average Loss: 0.8543\n",
      "Epoch: 88, Average Loss: 0.8525\n",
      "Epoch: 89, Average Loss: 0.8483\n",
      "Epoch: 90, Average Loss: 0.8440\n",
      "Epoch: 91, Average Loss: 0.8455\n",
      "Epoch: 92, Average Loss: 0.8396\n",
      "Epoch: 93, Average Loss: 0.8367\n",
      "Epoch: 94, Average Loss: 0.8325\n",
      "Epoch: 95, Average Loss: 0.8318\n",
      "Epoch: 96, Average Loss: 0.8278\n",
      "Epoch: 97, Average Loss: 0.8264\n",
      "Epoch: 98, Average Loss: 0.8240\n",
      "Epoch: 99, Average Loss: 0.8222\n",
      "Epoch: 100, Average Loss: 0.8187\n",
      "Epoch-50: -0.04819228216720539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.14368634774012978\n",
      "Starting transformer_01_F2\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.3103\n",
      "Epoch: 2, Average Loss: 1.2120\n",
      "Epoch: 3, Average Loss: 1.1760\n",
      "Epoch: 4, Average Loss: 1.1559\n",
      "Epoch: 5, Average Loss: 1.1432\n",
      "Epoch: 6, Average Loss: 1.1340\n",
      "Epoch: 7, Average Loss: 1.1275\n",
      "Epoch: 8, Average Loss: 1.1223\n",
      "Epoch: 9, Average Loss: 1.1181\n",
      "Epoch: 10, Average Loss: 1.1143\n",
      "Epoch: 11, Average Loss: 1.1114\n",
      "Epoch: 12, Average Loss: 1.1085\n",
      "Epoch: 13, Average Loss: 1.1060\n",
      "Epoch: 14, Average Loss: 1.1032\n",
      "Epoch: 15, Average Loss: 1.1010\n",
      "Epoch: 16, Average Loss: 1.0988\n",
      "Epoch: 17, Average Loss: 1.0965\n",
      "Epoch: 18, Average Loss: 1.0944\n",
      "Epoch: 19, Average Loss: 1.0922\n",
      "Epoch: 20, Average Loss: 1.0900\n",
      "Epoch: 21, Average Loss: 1.0880\n",
      "Epoch: 22, Average Loss: 1.0852\n",
      "Epoch: 23, Average Loss: 1.0826\n",
      "Epoch: 24, Average Loss: 1.0807\n",
      "Epoch: 25, Average Loss: 1.0777\n",
      "Epoch: 26, Average Loss: 1.0754\n",
      "Epoch: 27, Average Loss: 1.0730\n",
      "Epoch: 28, Average Loss: 1.0703\n",
      "Epoch: 29, Average Loss: 1.0681\n",
      "Epoch: 30, Average Loss: 1.0649\n",
      "Epoch: 31, Average Loss: 1.0619\n",
      "Epoch: 32, Average Loss: 1.0590\n",
      "Epoch: 33, Average Loss: 1.0551\n",
      "Epoch: 34, Average Loss: 1.0518\n",
      "Epoch: 35, Average Loss: 1.0488\n",
      "Epoch: 36, Average Loss: 1.0452\n",
      "Epoch: 37, Average Loss: 1.0415\n",
      "Epoch: 38, Average Loss: 1.0375\n",
      "Epoch: 39, Average Loss: 1.0335\n",
      "Epoch: 40, Average Loss: 1.0287\n",
      "Epoch: 41, Average Loss: 1.0252\n",
      "Epoch: 42, Average Loss: 1.0196\n",
      "Epoch: 43, Average Loss: 1.0161\n",
      "Epoch: 44, Average Loss: 1.0112\n",
      "Epoch: 45, Average Loss: 1.0065\n",
      "Epoch: 46, Average Loss: 1.0022\n",
      "Epoch: 47, Average Loss: 0.9972\n",
      "Epoch: 48, Average Loss: 0.9918\n",
      "Epoch: 49, Average Loss: 0.9905\n",
      "Epoch: 50, Average Loss: 0.9818\n",
      "Epoch: 51, Average Loss: 0.9767\n",
      "Epoch: 52, Average Loss: 0.9718\n",
      "Epoch: 53, Average Loss: 0.9678\n",
      "Epoch: 54, Average Loss: 0.9621\n",
      "Epoch: 55, Average Loss: 0.9564\n",
      "Epoch: 56, Average Loss: 0.9511\n",
      "Epoch: 57, Average Loss: 0.9454\n",
      "Epoch: 58, Average Loss: 0.9406\n",
      "Epoch: 59, Average Loss: 0.9358\n",
      "Epoch: 60, Average Loss: 0.9348\n",
      "Epoch: 61, Average Loss: 0.9270\n",
      "Epoch: 62, Average Loss: 0.9207\n",
      "Epoch: 63, Average Loss: 0.9180\n",
      "Epoch: 64, Average Loss: 0.9120\n",
      "Epoch: 65, Average Loss: 0.9072\n",
      "Epoch: 66, Average Loss: 0.9039\n",
      "Epoch: 67, Average Loss: 0.8997\n",
      "Epoch: 68, Average Loss: 0.8930\n",
      "Epoch: 69, Average Loss: 0.8876\n",
      "Epoch: 70, Average Loss: 0.8838\n",
      "Epoch: 71, Average Loss: 0.8806\n",
      "Epoch: 72, Average Loss: 0.8797\n",
      "Epoch: 73, Average Loss: 0.8715\n",
      "Epoch: 74, Average Loss: 0.8694\n",
      "Epoch: 75, Average Loss: 0.8623\n",
      "Epoch: 76, Average Loss: 0.8600\n",
      "Epoch: 77, Average Loss: 0.8573\n",
      "Epoch: 78, Average Loss: 0.8532\n",
      "Epoch: 79, Average Loss: 0.8485\n",
      "Epoch: 80, Average Loss: 0.8454\n",
      "Epoch: 81, Average Loss: 0.8432\n",
      "Epoch: 82, Average Loss: 0.8372\n",
      "Epoch: 83, Average Loss: 0.8321\n",
      "Epoch: 84, Average Loss: 0.8319\n",
      "Epoch: 85, Average Loss: 0.8250\n",
      "Epoch: 86, Average Loss: 0.8234\n",
      "Epoch: 87, Average Loss: 0.8206\n",
      "Epoch: 88, Average Loss: 0.8158\n",
      "Epoch: 89, Average Loss: 0.8137\n",
      "Epoch: 90, Average Loss: 0.8097\n",
      "Epoch: 91, Average Loss: 0.8071\n",
      "Epoch: 92, Average Loss: 0.8066\n",
      "Epoch: 93, Average Loss: 0.8020\n",
      "Epoch: 94, Average Loss: 0.7957\n",
      "Epoch: 95, Average Loss: 0.7917\n",
      "Epoch: 96, Average Loss: 0.7915\n",
      "Epoch: 97, Average Loss: 0.7890\n",
      "Epoch: 98, Average Loss: 0.7867\n",
      "Epoch: 99, Average Loss: 0.7842\n",
      "Epoch: 100, Average Loss: 0.7811\n",
      "Epoch-50: -0.07373323933413856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.12860007473334978\n",
      "Starting transformer_01_FC2\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.2237\n",
      "Epoch: 2, Average Loss: 1.1261\n",
      "Epoch: 3, Average Loss: 1.0889\n",
      "Epoch: 4, Average Loss: 1.0677\n",
      "Epoch: 5, Average Loss: 1.0541\n",
      "Epoch: 6, Average Loss: 1.0441\n",
      "Epoch: 7, Average Loss: 1.0371\n",
      "Epoch: 8, Average Loss: 1.0310\n",
      "Epoch: 9, Average Loss: 1.0260\n",
      "Epoch: 10, Average Loss: 1.0220\n",
      "Epoch: 11, Average Loss: 1.0179\n",
      "Epoch: 12, Average Loss: 1.0142\n",
      "Epoch: 13, Average Loss: 1.0109\n",
      "Epoch: 14, Average Loss: 1.0076\n",
      "Epoch: 15, Average Loss: 1.0045\n",
      "Epoch: 16, Average Loss: 1.0016\n",
      "Epoch: 17, Average Loss: 0.9987\n",
      "Epoch: 18, Average Loss: 0.9958\n",
      "Epoch: 19, Average Loss: 0.9924\n",
      "Epoch: 20, Average Loss: 0.9892\n",
      "Epoch: 21, Average Loss: 0.9867\n",
      "Epoch: 22, Average Loss: 0.9844\n",
      "Epoch: 23, Average Loss: 0.9800\n",
      "Epoch: 24, Average Loss: 0.9764\n",
      "Epoch: 25, Average Loss: 0.9733\n",
      "Epoch: 26, Average Loss: 0.9698\n",
      "Epoch: 27, Average Loss: 0.9667\n",
      "Epoch: 28, Average Loss: 0.9619\n",
      "Epoch: 29, Average Loss: 0.9576\n",
      "Epoch: 30, Average Loss: 0.9539\n",
      "Epoch: 31, Average Loss: 0.9497\n",
      "Epoch: 32, Average Loss: 0.9453\n",
      "Epoch: 33, Average Loss: 0.9406\n",
      "Epoch: 34, Average Loss: 0.9371\n",
      "Epoch: 35, Average Loss: 0.9310\n",
      "Epoch: 36, Average Loss: 0.9266\n",
      "Epoch: 37, Average Loss: 0.9225\n",
      "Epoch: 38, Average Loss: 0.9159\n",
      "Epoch: 39, Average Loss: 0.9110\n",
      "Epoch: 40, Average Loss: 0.9064\n",
      "Epoch: 41, Average Loss: 0.9025\n",
      "Epoch: 42, Average Loss: 0.8955\n",
      "Epoch: 43, Average Loss: 0.8897\n",
      "Epoch: 44, Average Loss: 0.8861\n",
      "Epoch: 45, Average Loss: 0.8785\n",
      "Epoch: 46, Average Loss: 0.8722\n",
      "Epoch: 47, Average Loss: 0.8662\n",
      "Epoch: 48, Average Loss: 0.8616\n",
      "Epoch: 49, Average Loss: 0.8560\n",
      "Epoch: 50, Average Loss: 0.8485\n",
      "Epoch: 51, Average Loss: 0.8436\n",
      "Epoch: 52, Average Loss: 0.8382\n",
      "Epoch: 53, Average Loss: 0.8339\n",
      "Epoch: 54, Average Loss: 0.8283\n",
      "Epoch: 55, Average Loss: 0.8221\n",
      "Epoch: 56, Average Loss: 0.8226\n",
      "Epoch: 57, Average Loss: 0.8179\n",
      "Epoch: 58, Average Loss: 0.8070\n",
      "Epoch: 59, Average Loss: 0.8025\n",
      "Epoch: 60, Average Loss: 0.7967\n",
      "Epoch: 61, Average Loss: 0.7935\n",
      "Epoch: 62, Average Loss: 0.7875\n",
      "Epoch: 63, Average Loss: 0.7856\n",
      "Epoch: 64, Average Loss: 0.7806\n",
      "Epoch: 65, Average Loss: 0.7752\n",
      "Epoch: 66, Average Loss: 0.7709\n",
      "Epoch: 67, Average Loss: 0.7634\n",
      "Epoch: 68, Average Loss: 0.7599\n",
      "Epoch: 69, Average Loss: 0.7586\n",
      "Epoch: 70, Average Loss: 0.7537\n",
      "Epoch: 71, Average Loss: 0.7489\n",
      "Epoch: 72, Average Loss: 0.7450\n",
      "Epoch: 73, Average Loss: 0.7405\n",
      "Epoch: 74, Average Loss: 0.7382\n",
      "Epoch: 75, Average Loss: 0.7323\n",
      "Epoch: 76, Average Loss: 0.7312\n",
      "Epoch: 77, Average Loss: 0.7270\n",
      "Epoch: 78, Average Loss: 0.7237\n",
      "Epoch: 79, Average Loss: 0.7208\n",
      "Epoch: 80, Average Loss: 0.7136\n",
      "Epoch: 81, Average Loss: 0.7128\n",
      "Epoch: 82, Average Loss: 0.7103\n",
      "Epoch: 83, Average Loss: 0.7080\n",
      "Epoch: 84, Average Loss: 0.7031\n",
      "Epoch: 85, Average Loss: 0.7010\n",
      "Epoch: 86, Average Loss: 0.6993\n",
      "Epoch: 87, Average Loss: 0.6940\n",
      "Epoch: 88, Average Loss: 0.6913\n",
      "Epoch: 89, Average Loss: 0.6879\n",
      "Epoch: 90, Average Loss: 0.6850\n",
      "Epoch: 91, Average Loss: 0.6812\n",
      "Epoch: 92, Average Loss: 0.6792\n",
      "Epoch: 93, Average Loss: 0.6767\n",
      "Epoch: 94, Average Loss: 0.6731\n",
      "Epoch: 95, Average Loss: 0.6733\n",
      "Epoch: 96, Average Loss: 0.6685\n",
      "Epoch: 97, Average Loss: 0.6667\n",
      "Epoch: 98, Average Loss: 0.6645\n",
      "Epoch: 99, Average Loss: 0.6608\n",
      "Epoch: 100, Average Loss: 0.6589\n",
      "Epoch-50: -0.04836988821113941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-100: -0.17092877526117212\n",
      "Starting transformer_01_FC6\n",
      "Skipping 01\n",
      "EEG Shape: (25000, 200, 1)\n",
      "NIRS Shape: (25000, 4000, 16)\n",
      "torch.Size([25000, 4000, 16])\n",
      "torch.Size([25000, 200, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mjm9724/conda_envs/LatestText2Dungeon/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 1.2028\n"
     ]
    }
   ],
   "source": [
    "# Define channels to use\n",
    "nirs_channels_to_use_base = list(NIRS_COORDS.keys())[:16]\n",
    "nirs_channel_index = find_indices(list(NIRS_COORDS.keys()),nirs_channels_to_use_base)\n",
    "\n",
    "eeg_channels_to_use_full = EEG_CHANNEL_NAMES\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "for test_subject_id in subjects:\n",
    "    test = [test_subject_id]\n",
    "    for eeg_channel_name in eeg_channels_to_use_full:\n",
    "        eeg_channels_to_use = [eeg_channel_name]\n",
    "        eeg_channel_index = find_indices(EEG_CHANNEL_NAMES,eeg_channels_to_use)\n",
    "\n",
    "        model_name = f'transformer_{test_subject_id:02d}_{eeg_channel_name}'\n",
    "        final_model_path = os.path.join(MODEL_WEIGHTS, f'{model_name}_{num_epochs}.pth')\n",
    "        if os.path.exists(final_model_path):\n",
    "            print(f'Model name exists, skipping {model_name}')\n",
    "        else:\n",
    "            print(f'Starting {model_name}')\n",
    "        \n",
    "            \n",
    "            model = iTransformer(\n",
    "                num_variates = len(nirs_channels_to_use_base),\n",
    "                lookback_len = fnirs_lookback,      # or the lookback length in the paper\n",
    "                target_num_variates=len(eeg_channels_to_use),\n",
    "                target_lookback_len=eeg_lookback,\n",
    "                dim = 256,                          # model dimensions\n",
    "                depth = 6,                          # depth\n",
    "                heads = 8,                          # attention heads\n",
    "                dim_head = 64,                      # head dimension\n",
    "                attn_dropout=0.1,\n",
    "                ff_mult=4,\n",
    "                ff_dropout=0.1,\n",
    "                num_mem_tokens=10,\n",
    "                num_tokens_per_variate = 1,         # experimental setting that projects each variate to more than one token. the idea is that the network can learn to divide up into time tokens for more granular attention across time. thanks to flash attention, you should be able to accommodate long sequence lengths just fine\n",
    "                use_reversible_instance_norm = True # use reversible instance normalization, proposed here https://openreview.net/forum?id=cGDAkQo1C0p . may be redundant given the layernorms within iTransformer (and whatever else attention learns emergently on the first layer, prior to the first layernorm). if i come across some time, i'll gather up all the statistics across variates, project them, and condition the transformer a bit further. that makes more sense\n",
    "            )\n",
    "            \n",
    "            # Pre-allocate memory for training and testing data\n",
    "            eeg_windowed_train = np.empty((0, eeg_lookback, len(eeg_channels_to_use)))\n",
    "            nirs_windowed_train = np.empty((0, fnirs_lookback, len(nirs_channels_to_use_base)))\n",
    "            eeg_windowed_test = np.empty((0, eeg_lookback, len(eeg_channels_to_use)))\n",
    "            nirs_windowed_test = np.empty((0, fnirs_lookback, len(nirs_channels_to_use_base)))\n",
    "            \n",
    "            for i in subjects:\n",
    "                subject_id = f'{i:02d}'\n",
    "            \n",
    "                subject_data = loadmat(os.path.join(BASE_PATH, 'matfiles', f'data_vp0{subject_id}.mat'))['subject_data_struct'][0]\n",
    "                # # eeg subject_data[1][0]\n",
    "                # eeg_data = []\n",
    "                # for session_eeg_data in subject_data[1][0]:\n",
    "                #     eeg_data.append(session_eeg_data.T)\n",
    "                # eeg_data = np.hstack(eeg_data)\n",
    "                # # fnirs subject_data[3][0]\n",
    "                # nirs_data = []\n",
    "                # for session_nirs_data in subject_data[3][0]:\n",
    "                #     nirs_data.append(session_nirs_data.T)\n",
    "                # nirs_data = np.hstack(nirs_data)\n",
    "                # # mrk subject_data[5][0]\n",
    "            \n",
    "                eeg_data = subject_data[1][0][0].T\n",
    "                nirs_data = subject_data[3][0][0].T\n",
    "            \n",
    "                assert eeg_data.shape[1] == nirs_data.shape[1]\n",
    "                \n",
    "                if i not in test and do_train:\n",
    "                    single_eeg_windowed_train, single_nirs_windowed_train, meta_data = grab_random_windows(\n",
    "                                 nirs_data=nirs_data, \n",
    "                                 eeg_data=eeg_data,\n",
    "                                 sampling_rate=200,\n",
    "                                 nirs_t_min=nirs_t_min, \n",
    "                                 nirs_t_max=nirs_t_max,\n",
    "                                 eeg_t_min=0, \n",
    "                                 eeg_t_max=1,\n",
    "                                 number_of_windows=1000)\n",
    "            \n",
    "                    # Append to the preallocated arrays\n",
    "                    single_eeg_transposed = single_eeg_windowed_train.transpose(0,2,1)\n",
    "                    single_nirs_transposed = single_nirs_windowed_train.transpose(0,2,1)\n",
    "            \n",
    "                    single_eeg_transposed = single_eeg_transposed[:,:eeg_lookback, eeg_channel_index]\n",
    "                    single_nirs_transposed = single_nirs_transposed[:,:fnirs_lookback, nirs_channel_index]\n",
    "            \n",
    "                    # Stack new data into the existing array, avoiding list append\n",
    "                    eeg_windowed_train = np.vstack((eeg_windowed_train, single_eeg_transposed))\n",
    "                    nirs_windowed_train = np.vstack((nirs_windowed_train, single_nirs_transposed))\n",
    "                elif i not in test and not do_train:\n",
    "                    single_eeg_windowed_train, single_nirs_windowed_train, meta_data = grab_ordered_windows(\n",
    "                         nirs_data=nirs_data, \n",
    "                         eeg_data=eeg_data,\n",
    "                         sampling_rate=200,\n",
    "                         nirs_t_min=nirs_t_min, \n",
    "                         nirs_t_max=nirs_t_max,\n",
    "                         eeg_t_min=0, \n",
    "                         eeg_t_max=1)\n",
    "                    \n",
    "                    single_eeg_windowed_train = single_eeg_windowed_train.transpose(0,2,1)\n",
    "                    single_nirs_windowed_train = single_nirs_windowed_train.transpose(0,2,1)\n",
    "                \n",
    "                    single_eeg_windowed_train = single_eeg_windowed_train[:,:eeg_lookback, eeg_channel_index]\n",
    "                    single_nirs_windowed_train = single_nirs_windowed_train[:,:fnirs_lookback, nirs_channel_index]\n",
    "                    \n",
    "                    # For test data, direct stacking since no windowing\n",
    "                    eeg_windowed_train = np.vstack((eeg_windowed_train, single_eeg_windowed_train))\n",
    "                    nirs_windowed_train = np.vstack((nirs_windowed_train, single_nirs_windowed_train))\n",
    "                else:\n",
    "                    single_eeg_windowed_test, single_nirs_windowed_test, meta_data = grab_ordered_windows(\n",
    "                         nirs_data=nirs_data, \n",
    "                         eeg_data=eeg_data,\n",
    "                         sampling_rate=200,\n",
    "                         nirs_t_min=nirs_t_min, \n",
    "                         nirs_t_max=nirs_t_max,\n",
    "                         eeg_t_min=0, \n",
    "                         eeg_t_max=1)\n",
    "            \n",
    "                    single_eeg_windowed_test = single_eeg_windowed_test.transpose(0,2,1)\n",
    "                    single_nirs_windowed_test = single_nirs_windowed_test.transpose(0,2,1)\n",
    "                    \n",
    "                    single_eeg_windowed_test = single_eeg_windowed_test[:,:eeg_lookback, eeg_channel_index]\n",
    "                    single_nirs_windowed_test = single_nirs_windowed_test[:,:fnirs_lookback, nirs_channel_index]\n",
    "                    \n",
    "                    # For test data, direct stacking since no windowing\n",
    "                    eeg_windowed_test = np.vstack((eeg_windowed_test, single_eeg_windowed_test))\n",
    "                    nirs_windowed_test = np.vstack((nirs_windowed_test, single_nirs_windowed_test))\n",
    "                    \n",
    "                    print(f'Skipping {subject_id}')\n",
    "\n",
    "            print(f'EEG Shape: {eeg_windowed_train.shape}')\n",
    "            print(f'NIRS Shape: {nirs_windowed_train.shape}')\n",
    "            \n",
    "            if do_train:\n",
    "                nirs_train_tensor = torch.from_numpy(nirs_windowed_train).float()\n",
    "                eeg_train_tensor = torch.from_numpy(eeg_windowed_train).float()\n",
    "                meta_data_tensor = torch.from_numpy(np.array(meta_data)).float()\n",
    "                \n",
    "                print(nirs_train_tensor.shape)\n",
    "                print(eeg_train_tensor.shape)\n",
    "                \n",
    "                sequence_length = eeg_train_tensor.shape[1]\n",
    "                eeg_number_of_features = eeg_train_tensor.shape[2]\n",
    "                nirs_number_of_features = nirs_train_tensor.shape[2]\n",
    "                \n",
    "                dataset = EEGfNIRSData(nirs_train_tensor, eeg_train_tensor)\n",
    "                dataloader = DataLoader(dataset, batch_size=500, shuffle=True)\n",
    "            \n",
    "                latest_epoch = 0\n",
    "                loss_list = []\n",
    "                if do_load:\n",
    "                    model_path = f'{model_name}_epoch_1.pth'\n",
    "            \n",
    "                    # find the latest model\n",
    "                    for file in os.listdir(MODEL_WEIGHTS):\n",
    "                        if file.startswith(f'{model_name}_epoch_'):\n",
    "                            epoch = int(file.split('_')[-1].split('.')[0])\n",
    "                            if epoch > latest_epoch:\n",
    "                                latest_epoch = epoch\n",
    "                                model_path = file\n",
    "                    print(f'Using Model Weights: {model_path}')\n",
    "                    model.load_state_dict(torch.load(os.path.join(MODEL_WEIGHTS, model_path)))\n",
    "                    \n",
    "                    # load loss list\n",
    "                    with open(os.path.join(MODEL_WEIGHTS, f'loss_{model_name}_{latest_epoch}.csv'), 'r') as file_ptr:\n",
    "                        reader = csv.reader(file_ptr)\n",
    "                        loss_list = list(reader)[0]\n",
    "                    print(f'Last loss: {float(loss_list[-1])/len(dataloader):.4f}')\n",
    "            \n",
    "                # Set correct device\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model.to(device)\n",
    "            \n",
    "                # Optimizer and loss function\n",
    "                optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "                loss_function = torch.nn.MSELoss()\n",
    "                for epoch in range(latest_epoch, num_epochs):\n",
    "                    model.train()\n",
    "                    total_loss = 0\n",
    "            \n",
    "                    for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "                        X_batch = X_batch.to(device).float()\n",
    "                        y_batch = y_batch.to(device).float()\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        predictions = model(X_batch)\n",
    "            \n",
    "                        # Loss calculation\n",
    "                        loss = loss_function(predictions, y_batch)\n",
    "            \n",
    "                        # Backpropagation\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "                        total_loss += loss.item()\n",
    "                        # if (batch_idx+1) % 20 == 0 or batch_idx == 0:\n",
    "                        #     print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {loss.item():.4f}')\n",
    "                    \n",
    "                    loss_list.append(total_loss)\n",
    "            \n",
    "                    if (epoch+1) % 50 == 0:\n",
    "                        # Save model weights\n",
    "                        torch.save(model.state_dict(), os.path.join(MODEL_WEIGHTS, f'{model_name}_{epoch+1}.pth'))\n",
    "                        with open(os.path.join(MODEL_WEIGHTS,f'loss_{model_name}_{epoch+1}.csv'), 'w', newline='') as file_ptr:\n",
    "                            wr = csv.writer(file_ptr, quoting=csv.QUOTE_ALL)\n",
    "                            wr.writerow(loss_list)\n",
    "                        \n",
    "                    # Plotting target vs. output for the first example in the last batch\n",
    "                    # single_actual = y_batch[0, :, 0].detach().cpu().numpy()\n",
    "                    # single_predicted = predictions[0,:,0].detach().cpu().numpy()\n",
    "                    # r2 = r2_score(single_actual, single_predicted)\n",
    "                    # print(f'R-squared: {r2}')\n",
    "                    # if (epoch+1) % 10 == 0:\n",
    "                    #     plot_series(single_actual, single_predicted, epoch)\n",
    "            \n",
    "                    print(f'Epoch: {epoch+1}, Average Loss: {total_loss / len(dataloader):.4f}')\n",
    "            \n",
    "            # Perform inference on test\n",
    "            \n",
    "            nirs_test_tensor = torch.from_numpy(nirs_windowed_test).float()\n",
    "            eeg_test_tensor = torch.from_numpy(eeg_windowed_test).float()\n",
    "            \n",
    "            # Assuming fnirs_test and eeg_test are your test datasets\n",
    "            test_dataset = EEGfNIRSData(nirs_test_tensor, eeg_test_tensor)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "            \n",
    "            # Get weights for specific epoch\n",
    "            weight_epochs = [50,100]\n",
    "            for weight_epoch in weight_epochs:\n",
    "                model_path = f'{model_name}_{weight_epoch}.pth'\n",
    "                model.load_state_dict(torch.load(os.path.join(MODEL_WEIGHTS, model_path)))\n",
    "                model.to(device)\n",
    "                # Set model to evaluation mode\n",
    "                model.eval()\n",
    "                \n",
    "                # Perform inference on test data\n",
    "                predictions = []\n",
    "                targets = []\n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(test_loader):\n",
    "                    X_batch = X_batch.to(device).float()\n",
    "                    y_batch = y_batch.to(device).float()\n",
    "                    predictions.append(model(X_batch).detach().cpu().numpy())\n",
    "                    targets.append(y_batch.detach().cpu().numpy())\n",
    "                \n",
    "                predictions = np.array(predictions)\n",
    "                targets = np.array(targets)\n",
    "                \n",
    "                # concatenate and plot\n",
    "                predictions = predictions.reshape(-1, len(eeg_channels_to_use))\n",
    "                targets = targets.reshape(-1, len(eeg_channels_to_use))\n",
    "            \n",
    "                scipy.io.savemat(os.path.join(OUTPUT_DIRECTORY, f'test_{model_name}_{weight_epoch}.mat'), {'X': targets, \n",
    "                                                                     'XPred':predictions,\n",
    "                                                                    'bins':10,\n",
    "                                                                    'scale':10,\n",
    "                                                                    'srate':200})\n",
    "                \n",
    "                # R2 score\n",
    "                r2 = r2_score(targets, predictions)\n",
    "                print(f'Epoch-{weight_epoch}: {r2}')\n",
    "                \n",
    "                # Plotting target vs. output on concatenated data\n",
    "                for i in range(len(eeg_channels_to_use)):\n",
    "                    plt.figure(figsize=(10, 4))\n",
    "                    plt.plot(targets[:,i], label='Target')\n",
    "                    plt.plot(predictions[:,i], label='Output', linestyle='--')\n",
    "                    plt.title(f'Epoch-{weight_epoch} Channel {eeg_channels_to_use[i]} : {r2}')\n",
    "                    plt.legend()\n",
    "                    plt.grid(True)\n",
    "                    plt.savefig(os.path.join(OUTPUT_DIRECTORY, f'test_{model_name}_{weight_epoch}.jpeg'))\n",
    "\n",
    "            gc.collect()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048f118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatestText2Dungeon",
   "language": "python",
   "name": "latesttext2dungeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
