{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2879c6ca-df1b-4ecc-8c24-7edcdf24e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting reformer-pytorch==1.4.4\n",
      "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting axial-positional-embedding>=0.1.0 (from reformer-pytorch==1.4.4)\n",
      "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /home/ak10514/.local/lib/python3.12/site-packages (from reformer-pytorch==1.4.4) (0.7.0)\n",
      "Collecting local-attention (from reformer-pytorch==1.4.4)\n",
      "  Downloading local_attention-1.9.0-py3-none-any.whl.metadata (682 bytes)\n",
      "Collecting product-key-memory (from reformer-pytorch==1.4.4)\n",
      "  Downloading product_key_memory-0.2.10-py3-none-any.whl.metadata (717 bytes)\n",
      "Requirement already satisfied: torch in /ext3/miniconda3/lib/python3.12/site-packages (from reformer-pytorch==1.4.4) (2.2.1)\n",
      "Collecting colt5-attention>=0.10.14 (from product-key-memory->reformer-pytorch==1.4.4)\n",
      "  Downloading CoLT5_attention-0.10.20-py3-none-any.whl.metadata (738 bytes)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (4.10.0)\n",
      "Requirement already satisfied: sympy in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (1.12)\n",
      "Requirement already satisfied: networkx in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch->reformer-pytorch==1.4.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /ext3/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->reformer-pytorch==1.4.4) (12.3.101)\n",
      "Requirement already satisfied: packaging in /ext3/miniconda3/lib/python3.12/site-packages (from colt5-attention>=0.10.14->product-key-memory->reformer-pytorch==1.4.4) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ext3/miniconda3/lib/python3.12/site-packages (from jinja2->torch->reformer-pytorch==1.4.4) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /ext3/miniconda3/lib/python3.12/site-packages (from sympy->torch->reformer-pytorch==1.4.4) (1.3.0)\n",
      "Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
      "Downloading local_attention-1.9.0-py3-none-any.whl (8.2 kB)\n",
      "Downloading product_key_memory-0.2.10-py3-none-any.whl (6.4 kB)\n",
      "Downloading CoLT5_attention-0.10.20-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: axial-positional-embedding\n",
      "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2881 sha256=aca1f7ac67386af8c24d0534b0314840ff784ff6c2019c1edd7fd70fb2ae7574\n",
      "  Stored in directory: /home/ak10514/.cache/pip/wheels/91/d0/51/088e48e1073c0efff046183c48761e6e3829c82e95cabaa392\n",
      "Successfully built axial-positional-embedding\n",
      "Installing collected packages: local-attention, axial-positional-embedding, colt5-attention, product-key-memory, reformer-pytorch\n",
      "Successfully installed axial-positional-embedding-0.2.1 colt5-attention-0.10.20 local-attention-1.9.0 product-key-memory-0.2.10 reformer-pytorch-1.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install reformer-pytorch==1.4.4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a787495a-9804-4cd1-8223-e65b5d627248",
   "metadata": {},
   "source": [
    "Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37abd84d-6648-4b23-b58c-5ba3e86b2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model.iTransformer import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb24e21f-9112-40fc-8178-b18e9b94d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'seq_len': 96, 'pred_len': 96}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961c6e6a-fe42-4e43-b6e1-47dfd2ad6e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(configs)  \n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m/scratch/ak10514/super-resolution/src/model/iTransformer.py:17\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, configs):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len \u001b[38;5;241m=\u001b[39m configs\u001b[38;5;241m.\u001b[39mseq_len\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len \u001b[38;5;241m=\u001b[39m configs\u001b[38;5;241m.\u001b[39mpred_len\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_attention \u001b[38;5;241m=\u001b[39m configs\u001b[38;5;241m.\u001b[39moutput_attention\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'seq_len'"
     ]
    }
   ],
   "source": [
    "model = Model(configs)  \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe20bfa-b161-4a1f-b9c1-94a4568cafdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: iTransformer in /home/ak10514/.local/lib/python3.12/site-packages (0.5.5)\n",
      "Requirement already satisfied: beartype in /home/ak10514/.local/lib/python3.12/site-packages (from iTransformer) (0.17.2)\n",
      "Requirement already satisfied: einops>=0.7.0 in /home/ak10514/.local/lib/python3.12/site-packages (from iTransformer) (0.7.0)\n",
      "Requirement already satisfied: gateloop-transformer>=0.2.3 in /home/ak10514/.local/lib/python3.12/site-packages (from iTransformer) (0.2.4)\n",
      "Requirement already satisfied: rotary-embedding-torch in /home/ak10514/.local/lib/python3.12/site-packages (from iTransformer) (0.5.3)\n",
      "Requirement already satisfied: torch>=2.1 in /ext3/miniconda3/lib/python3.12/site-packages (from iTransformer) (2.2.1)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (4.10.0)\n",
      "Requirement already satisfied: sympy in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (1.12)\n",
      "Requirement already satisfied: networkx in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /ext3/miniconda3/lib/python3.12/site-packages (from torch>=2.1->iTransformer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /ext3/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->iTransformer) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ext3/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.1->iTransformer) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /ext3/miniconda3/lib/python3.12/site-packages (from sympy->torch>=2.1->iTransformer) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e639042-30c0-4a1a-a271-880156b9bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from iTransformer import iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facfe8f7-5863-40c1-9bb1-8f74e61924d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "# using solar energy settings\n",
    "\n",
    "model = iTransformer(\n",
    "    num_variates = 137,\n",
    "    lookback_len = 96,                  # or the lookback length in the paper\n",
    "    dim = 256,                          # model dimensions\n",
    "    depth = 6,                          # depth\n",
    "    heads = 8,                          # attention heads\n",
    "    dim_head = 64,                      # head dimension\n",
    "    pred_length = (12, 24, 36, 48),     # can be one prediction, or many\n",
    "    num_tokens_per_variate = 1,         # experimental setting that projects each variate to more than one token. the idea is that the network can learn to divide up into time tokens for more granular attention across time. thanks to flash attention, you should be able to accommodate long sequence lengths just fine\n",
    "    use_reversible_instance_norm = True # use reversible instance normalization, proposed here https://openreview.net/forum?id=cGDAkQo1C0p . may be redundant given the layernorms within iTransformer (and whatever else attention learns emergently on the first layer, prior to the first layernorm). if i come across some time, i'll gather up all the statistics across variates, project them, and condition the transformer a bit further. that makes more sense\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6910f97-78a1-43d4-befe-9b36468e7bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iTransformer.forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#time_series = torch.randn(2, 96, 137)  # (batch, lookback len, variates)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m preds \u001b[38;5;241m=\u001b[39m model()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<@beartype(iTransformer.iTransformer.iTransformer.forward) at 0x14666ec19300>:67\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(__beartype_object_97443648, __beartype_get_violation, __beartype_conf, __beartype_object_22430177431680, __beartype_getrandbits, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iTransformer.forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "#time_series = torch.randn(2, 96, 137)  # (batch, lookback len, variates)\n",
    "preds = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c0abf-31a1-4ec4-9fdb-0f95dcfe5af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super-resolution",
   "language": "python",
   "name": "super-resolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
