{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feed0634-9ddf-43f3-af9f-8f927022921d",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f588fa-ac74-40a0-a719-81f0e30e1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import mne\n",
    "# from mne import events_from_annotations, concatenate_raws\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.integrate import simps\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from processing.Processing_EEG import process_eeg_raw, process_eeg_epochs\n",
    "from processing.Processing_NIRS import process_nirs_raw, process_nirs_epochs\n",
    "\n",
    "from utilities.Read_Data import read_subject_raw_nirs, read_subject_raw_eeg\n",
    "from utilities.utilities import translate_channel_name_to_ch_id, find_sections, spatial_zscore\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from iTransformer.iTransformer.iTransformerTranscoding import iTransformer\n",
    "\n",
    "import gc\n",
    "\n",
    "from config.Constants import *\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directorys if they dont exist\n",
    "if not os.path.exists(MODEL_WEIGHTS):\n",
    "    os.makedirs(MODEL_WEIGHTS)\n",
    "if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "    os.makedirs(OUTPUT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a6026",
   "metadata": {},
   "source": [
    "## Parameters - Raw ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2db1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subject/Trial Parameters ##\n",
    "subject_ids = np.arange(1,27) # 1-27\n",
    "subjects = []\n",
    "for i in subject_ids:\n",
    "    subjects.append(f'VP{i:03d}')\n",
    "\n",
    "tasks = ['nback']\n",
    "\n",
    "# NIRS Sampling rate\n",
    "fnirs_sample_rate = 10\n",
    "# EEG Downsampling rate\n",
    "eeg_sample_rate = 10\n",
    "\n",
    "# Do processing or not\n",
    "do_processing = True\n",
    "\n",
    "# Redo preprocessing pickle files, TAKES A LONG TIME \n",
    "redo_preprocessing = False\n",
    "\n",
    "# Redo data formating pickle files, TAKES A LONG TIME\n",
    "redo_data_formatting = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514427a",
   "metadata": {},
   "source": [
    "## Signal Prediction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09cead",
   "metadata": {},
   "source": [
    "### Parameters - Signal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window (seconds)\n",
    "eeg_t_min = 0\n",
    "eeg_t_max = 1\n",
    "nirs_t_min = -10\n",
    "nirs_t_max = 10\n",
    "\n",
    "offset_t = 0\n",
    "\n",
    "# Train/Test Size\n",
    "train_size = 4000\n",
    "test_size = 500\n",
    "\n",
    "# training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "do_load = False\n",
    "do_train = False\n",
    "\n",
    "# data projection\n",
    "nirs_token_size = 10\n",
    "eeg_token_size = 5\n",
    "fnirs_lookback = 4000\n",
    "eeg_lookback = 200\n",
    "\n",
    "use_hbr = False\n",
    "\n",
    "test_size_in_subject = 0.2 # percent of test data\n",
    "redo_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c13a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_features, hidden_dim, output_steps):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_features, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_steps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred\n",
    "\n",
    "def create_rnn(n_input, n_output):\n",
    "    '''\n",
    "       n_input (360)\n",
    "       n_output (150)\n",
    "    '''\n",
    "    print(f'Creating RNN with input size: {n_input} and output size: {n_output}')\n",
    "    model = LSTMModel(n_input, 20, n_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_mlp(n_input, n_output):\n",
    "    '''\n",
    "       n_input (360)\n",
    "       n_output (150)\n",
    "    '''\n",
    "    print(f'Creating MLP with input size: {n_input} and output size: {n_output}')\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(n_input, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, n_output)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_transformer(nirs_channels_to_use_base, eeg_channels_to_use):\n",
    "    model = iTransformer(\n",
    "            num_variates = len(nirs_channels_to_use_base),\n",
    "            lookback_len = fnirs_lookback,      # or the lookback length in the paper\n",
    "            target_num_variates=len(eeg_channels_to_use),\n",
    "            target_lookback_len=eeg_lookback,\n",
    "            dim = 256,                          # model dimensions\n",
    "            depth = 6,                          # depth\n",
    "            heads = 8,                          # attention heads\n",
    "            dim_head = 64,                      # head dimension\n",
    "            attn_dropout=0.1,\n",
    "            ff_mult=4,\n",
    "            ff_dropout=0.1,\n",
    "            num_mem_tokens=10,\n",
    "            num_tokens_per_variate = 1,         # experimental setting that projects each variate to more than one token. the idea is that the network can learn to divide up into time tokens for more granular attention across time. thanks to flash attention, you should be able to accommodate long sequence lengths just fine\n",
    "            use_reversible_instance_norm = True # use reversible instance normalization, proposed here https://openreview.net/forum?id=cGDAkQo1C0p . may be redundant given the layernorms within iTransformer (and whatever else attention learns emergently on the first layer, prior to the first layernorm). if i come across some time, i'll gather up all the statistics across variates, project them, and condition the transformer a bit further. that makes more sense\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_eeg(model, data_loader, n_samples, n_channels, n_lookback, eeg_token_size, eeg_fpcas):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Perform inference on test data\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(data_loader):\n",
    "        X_batch = X_batch.to(DEVICE).float()\n",
    "        y_batch = y_batch.to(DEVICE).float()\n",
    "        predictions.append(model(X_batch).detach().cpu().numpy())\n",
    "        targets.append(y_batch.detach().cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    targets = targets.reshape((n_samples, n_lookback, n_channels))\n",
    "\n",
    "    # inverse CA on predictions\n",
    "    predictions = predictions.reshape(n_samples, eeg_token_size, n_channels)\n",
    "    predictions = predictions.transpose(0,2,1)\n",
    "    # predictions = predictions.reshape(eeg_windowed_test.shape[0], eeg_token_size, eeg_windowed_test.shape[2])\n",
    "    predictions = inverse_transform_fpca_over_channels(predictions, eeg_fpcas, eeg_lookback)\n",
    "    predictions = predictions.transpose(0,2,1)\n",
    "\n",
    "    return targets, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0431b",
   "metadata": {},
   "source": [
    "### CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98304bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def inverse_transform_cca_over_channels(data, cca_dict, window):\n",
    "    '''Perform CCA over channels of the data\n",
    "    input:\n",
    "        data (samples x channels x tokens)\n",
    "        cca_dict (dictionary over channels of fit cca object)\n",
    "    output:\n",
    "        data (samples x channels x window)\n",
    "    '''\n",
    "    n_samples, n_channels, _ = data.shape\n",
    "\n",
    "    tokenized_data = np.zeros((n_samples, n_channels, window))\n",
    "    for i in range(n_channels):\n",
    "        cca = cca_dict[i]\n",
    "        tokenized_data[:, i, :] = cca.inverse_transform(data[:, i, :])\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "def perform_cca_over_channels(data, cca_dict, n_components):\n",
    "    '''Perform CCA over channels of the data\n",
    "    input:\n",
    "        data (samples x channels x window)\n",
    "        cca_dict (dictionary over channels of fit cca object)\n",
    "    output:\n",
    "        data (samples x channels x tokens)\n",
    "    '''\n",
    "    n_samples, n_channels, _ = data.shape\n",
    "    tokenized_data = np.zeros((n_samples, n_channels, n_components))\n",
    "    for i in range(n_channels):\n",
    "        cca = cca_dict[i]\n",
    "        tokenized_data[:, i, :] = cca.transform(data[:, i, :])\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "def fit_cca_single_channel(data_reshaped_a, data_reshaped_b, n_components, channel_idx):\n",
    "    cca = CCA(n_components=n_components)\n",
    "    cca.fit(data_reshaped_b, data_reshaped_a)\n",
    "    return channel_idx, cca\n",
    "\n",
    "def fit_cca_model(time_series_a, time_series_b, n_components):\n",
    "    '''Fit CCA model to the data\n",
    "    input:\n",
    "        time_series_a (samples x channels x window)\n",
    "        time_series_b (samples x channels x window)\n",
    "        n_components (int)\n",
    "    output:\n",
    "        cca_dict (dictionary over channels of fit cca object)\n",
    "    '''\n",
    "    n_samples, n_channels, _ = time_series_b.shape\n",
    "    cca_dict = {}\n",
    "\n",
    "    data_reshaped_a = time_series_a.reshape(n_samples, -1)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(fit_cca_single_channel, data_reshaped_a, time_series_b[:, i, :], n_components, i)\n",
    "            for i in range(n_channels)\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            channel_idx, cca = future.result()\n",
    "            cca_dict[channel_idx] = cca\n",
    "            if channel_idx % 10 == 0:\n",
    "                print(f'Finished fitting CCA for channel {channel_idx + 1}')\n",
    "\n",
    "    return cca_dict\n",
    "\n",
    "def get_cca_dict(subject_id, train_nirs_data, train_eeg_data, token_size):\n",
    "    cca_dict_path = os.path.join(MODEL_WEIGHTS, f'cca_dict_{subject_id}.pkl')\n",
    "    if not os.path.exists(cca_dict_path):\n",
    "        print(f'Building CCA Dict')\n",
    "        eeg_windowed_train, nirs_windowed_train, meta_data = grab_ordered_windows(\n",
    "                    nirs_data=train_nirs_data, \n",
    "                    eeg_data=train_eeg_data,\n",
    "                    sampling_rate=200,\n",
    "                    nirs_t_min=nirs_t_min, \n",
    "                    nirs_t_max=nirs_t_max,\n",
    "                    eeg_t_min=0, \n",
    "                    eeg_t_max=1)\n",
    "        print(f'EEG CCA Shape: {eeg_windowed_train.shape}')\n",
    "        print(f'NIRS CCA Shape: {nirs_windowed_train.shape}')\n",
    "        cca_dict = fit_cca_model(eeg_windowed_train, nirs_windowed_train, token_size)\n",
    "        joblib.dump(cca_dict, cca_dict_path)\n",
    "    else:\n",
    "        cca_dict = joblib.load(cca_dict_path)\n",
    "\n",
    "    return cca_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c551d6",
   "metadata": {},
   "source": [
    "### FPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5127c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.datasets import fetch_growth\n",
    "from skfda.exploratory.visualization import FPCAPlot\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "from skfda.representation.basis import (\n",
    "    BSplineBasis,\n",
    "    FourierBasis,\n",
    "    MonomialBasis,\n",
    ")\n",
    "\n",
    "def fdarray_to_numpy(data):\n",
    "    '''Convert fdarray to numpy array\n",
    "    input:\n",
    "        fdarray (samples x window)\n",
    "    output:\n",
    "        data (samples x channels x window)\n",
    "    '''\n",
    "    n_samples, window, _ = data.data_matrix.shape\n",
    "    data_array = data.data_matrix\n",
    "    data_array = data_array.reshape(n_samples, window)\n",
    "    return data_array\n",
    "\n",
    "def numpy_to_fdarray(data):\n",
    "    '''Convert numpy array to fdarray\n",
    "    input:\n",
    "        data (samples x channels x window)\n",
    "    output:\n",
    "        fdarray (samples x window)\n",
    "    '''\n",
    "    n_samples, window = data.shape\n",
    "    fdarray = skfda.FDataGrid(data, grid_points=np.arange(window))\n",
    "    return fdarray\n",
    "\n",
    "def inverse_transform_fpca_over_channels(data, fpca_dict, window):\n",
    "    '''Perform FPCA over channels of the data\n",
    "    input:\n",
    "        data (samples x channels x tokens)\n",
    "        fpca_dict (dictionary over channels of fit fpca object)\n",
    "    output:\n",
    "        data (samples x channels x window)\n",
    "    '''\n",
    "    n_samples, n_channels, _ = data.shape\n",
    "\n",
    "    tokenized_data = np.zeros((n_samples, n_channels, window))\n",
    "    for i in range(n_channels):\n",
    "        fpca = fpca_dict[i]\n",
    "        inverse_transform_fdarray = fpca.inverse_transform(data[:, i, :])\n",
    "        tokenized_data[:, i, :] = fdarray_to_numpy(inverse_transform_fdarray)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "def perform_fpca_over_channels(data, fpca_dict, n_components):\n",
    "    '''Perform FPCA over channels of the data\n",
    "    input:\n",
    "        data (samples x channels x window)\n",
    "        fpca_dict (dictionary over channels of fit fpca object)\n",
    "    output:\n",
    "        data (samples x channels x tokens)\n",
    "    '''\n",
    "    n_samples, n_channels, _ = data.shape\n",
    "    tokenized_data = np.zeros((n_samples, n_channels, n_components))\n",
    "    for i in range(n_channels):\n",
    "        fpca = fpca_dict[i]\n",
    "        fdarray = numpy_to_fdarray(data[:, i, :])\n",
    "        tokenized_data[:, i, :] = fpca.transform(fdarray)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "def fit_fpca_single_channel(data, n_components, channel_idx):\n",
    "    '''Fit PCA model to the data\n",
    "    input:\n",
    "        data (samples x window)\n",
    "        n_components (int)\n",
    "    output:\n",
    "        pca (fit pca object)\n",
    "    '''\n",
    "    fdarray = numpy_to_fdarray(data)\n",
    "    fpca = FPCA(n_components=n_components)\n",
    "    fpca.fit(fdarray)\n",
    "    return channel_idx, fpca\n",
    "\n",
    "def fit_fpca_model(data, n_components):\n",
    "    '''Fit PCA model to the data\n",
    "    input:\n",
    "        data (samples x channels x window)\n",
    "        n_components (int)\n",
    "    output:\n",
    "        pca_dict (dictionary over channels of fit pca object)\n",
    "    '''\n",
    "    n_samples, n_channels, _ = data.shape\n",
    "    pca_dict = {}\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(fit_fpca_single_channel, data[:, i, :], n_components, i)\n",
    "            for i in range(n_channels)\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            channel_idx, pca = future.result()\n",
    "            pca_dict[channel_idx] = pca\n",
    "            if channel_idx % 10 == 0:\n",
    "                print(f'Finished fitting PCA for channel {channel_idx + 1}')\n",
    "\n",
    "    return pca_dict\n",
    "\n",
    "def plot_explained_variance_over_dict(fpca_dict, channel_names, path=''):\n",
    "    '''Plot explained variance over channels\n",
    "    input:\n",
    "        fpca_dict (dictionary over channels of fit fpca object)\n",
    "    '''\n",
    "    explained_variance = []\n",
    "\n",
    "    fig, axs = plt.subplots(len(fpca_dict), 1, figsize=(10, 50))\n",
    "    for i in range(len(fpca_dict)):\n",
    "        channel_name = channel_names[i]\n",
    "        variance_list = fpca_dict[i].explained_variance_ratio_\n",
    "        # plot bar of percentages\n",
    "        axs[i].bar(np.arange(len(variance_list)), variance_list)\n",
    "        axs[i].text(0.5, 0.9, f'{channel_name}', horizontalalignment='center', verticalalignment='center', transform=axs[i].transAxes)\n",
    "    if len(path) > 0:\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def get_fpca_dict(subject_id, train_nirs_data, train_eeg_data, nirs_token_size, eeg_token_size):\n",
    "    fpca_dict_path = os.path.join(MODEL_WEIGHTS, f'fpca_dict_{subject_id}.pkl')\n",
    "    if not os.path.exists(fpca_dict_path):\n",
    "        print(f'Building FPCA Dict')\n",
    "        eeg_windowed_train, nirs_windowed_train, meta_data = grab_ordered_windows(\n",
    "                    nirs_data=train_nirs_data, \n",
    "                    eeg_data=train_eeg_data,\n",
    "                    sampling_rate=200,\n",
    "                    nirs_t_min=nirs_t_min, \n",
    "                    nirs_t_max=nirs_t_max,\n",
    "                    eeg_t_min=0, \n",
    "                    eeg_t_max=1)\n",
    "        print(f'EEG FPCA Shape: {eeg_windowed_train.shape}')\n",
    "        print(f'NIRS FPCA Shape: {nirs_windowed_train.shape}')\n",
    "        eeg_fpca_dict = fit_fpca_model(eeg_windowed_train, eeg_token_size)\n",
    "        nirs_fpca_dict = fit_fpca_model(nirs_windowed_train, nirs_token_size)\n",
    "        fpca_dict = {'eeg': eeg_fpca_dict, 'nirs': nirs_fpca_dict}\n",
    "        joblib.dump(fpca_dict, fpca_dict_path)\n",
    "    else:\n",
    "        fpca_dict = joblib.load(fpca_dict_path)\n",
    "    return fpca_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7f5c6",
   "metadata": {},
   "source": [
    "### Extract Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matlab_file(subject_id):\n",
    "    '''Read matlab file and return data'''\n",
    "    subject_data = loadmat(os.path.join(BASE_PATH, 'matfiles', f'data_vp0{subject_id}.mat'))['subject_data_struct'][0]\n",
    "    # eeg subject_data[1][0]\n",
    "    eeg_data = []\n",
    "    for session_eeg_data in subject_data[1][0]:\n",
    "        eeg_data.append(session_eeg_data.T)\n",
    "    eeg_data = np.hstack(eeg_data)\n",
    "    # fnirs subject_data[3][0]\n",
    "    nirs_data = []\n",
    "    for session_nirs_data in subject_data[3][0]:\n",
    "        nirs_data.append(session_nirs_data.T)\n",
    "    nirs_data = np.hstack(nirs_data)\n",
    "    # mrk subject_data[5][0]\n",
    "    mrk_data = []\n",
    "    for session_mrk_data in subject_data[5][0]:\n",
    "        mrk_data.append(session_mrk_data.T)\n",
    "    mrk_data = np.hstack(mrk_data)\n",
    "\n",
    "    assert eeg_data.shape[1] == nirs_data.shape[1]\n",
    "\n",
    "    return eeg_data, nirs_data, mrk_data\n",
    "\n",
    "def get_single_window(center_point, \n",
    "                      nirs_data, \n",
    "                      eeg_data, \n",
    "                      eeg_i_min, \n",
    "                      eeg_i_max, \n",
    "                      nirs_i_min, \n",
    "                      nirs_i_max):\n",
    "    eeg_low_index = center_point + eeg_i_min\n",
    "    eeg_high_index = center_point + eeg_i_max\n",
    "    single_eeg_window = eeg_data[:,eeg_low_index:eeg_high_index]\n",
    "\n",
    "    nirs_low_index = center_point + nirs_i_min\n",
    "    nirs_high_index = center_point + nirs_i_max\n",
    "    single_nirs_window = nirs_data[:,nirs_low_index:nirs_high_index]\n",
    "    \n",
    "    return single_eeg_window, single_nirs_window\n",
    "\n",
    "def grab_ordered_windows(nirs_data,\n",
    "                        eeg_data,\n",
    "                        sampling_rate,\n",
    "                        nirs_t_min, \n",
    "                        nirs_t_max,\n",
    "                        eeg_t_min, \n",
    "                        eeg_t_max):\n",
    "    nirs_i_min = int(nirs_t_min*sampling_rate)\n",
    "    nirs_i_max = int(nirs_t_max*sampling_rate)\n",
    "    eeg_i_min = int(eeg_t_min*sampling_rate)\n",
    "    eeg_i_max = int(eeg_t_max*sampling_rate)\n",
    "\n",
    "    eeg_window_size = eeg_i_max - eeg_i_min\n",
    "\n",
    "    max_center_eeg = eeg_data.shape[1] - eeg_i_max\n",
    "    max_center_nirs = nirs_data.shape[1] - nirs_i_max\n",
    "    max_center = np.min([max_center_eeg, max_center_nirs])\n",
    "\n",
    "    min_center_eeg = np.abs(eeg_i_min)\n",
    "    min_center_nirs = np.abs(nirs_i_min)\n",
    "    min_center = np.max([min_center_eeg, min_center_nirs])\n",
    "\n",
    "    nirs_full_windows = []\n",
    "    eeg_full_windows = []\n",
    "    meta_data = []\n",
    "\n",
    "    for i in range(min_center, max_center, eeg_window_size):\n",
    "        center_point = i\n",
    "        meta_data.append(center_point)\n",
    "        single_eeg_window, single_nirs_window = get_single_window(center_point, \n",
    "                                                                  nirs_data, \n",
    "                                                                  eeg_data, \n",
    "                                                                  eeg_i_min, \n",
    "                                                                  eeg_i_max, \n",
    "                                                                  nirs_i_min, \n",
    "                                                                  nirs_i_max)\n",
    "        \n",
    "        eeg_full_windows.append(single_eeg_window)\n",
    "        nirs_full_windows.append(single_nirs_window)\n",
    "\n",
    "    nirs_full_windows = np.array(nirs_full_windows)\n",
    "    eeg_full_windows = np.array(eeg_full_windows)\n",
    "\n",
    "    return eeg_full_windows, nirs_full_windows, meta_data\n",
    "    \n",
    "def grab_random_windows(nirs_data, \n",
    "                        eeg_data,\n",
    "                        sampling_rate,\n",
    "                        nirs_t_min, \n",
    "                        nirs_t_max,\n",
    "                        eeg_t_min, \n",
    "                        eeg_t_max,\n",
    "                        number_of_windows=1000):\n",
    "    '''make number_of_windows of size t_min to t_max for each offset 0 to offset_t for eeg and nirs'''\n",
    "\n",
    "    nirs_i_min = int(nirs_t_min*sampling_rate)\n",
    "    nirs_i_max = int(nirs_t_max*sampling_rate)\n",
    "    eeg_i_min = int(eeg_t_min*sampling_rate)\n",
    "    eeg_i_max = int(eeg_t_max*sampling_rate)\n",
    "\n",
    "    max_center_eeg = eeg_data.shape[1] - eeg_i_max\n",
    "    max_center_nirs = nirs_data.shape[1] - nirs_i_max\n",
    "    max_center = np.min([max_center_eeg, max_center_nirs])\n",
    "\n",
    "    min_center_eeg = np.abs(eeg_i_min)\n",
    "    min_center_nirs = np.abs(nirs_i_min)\n",
    "    min_center = np.max([min_center_eeg, min_center_nirs])\n",
    "\n",
    "    nirs_full_windows = []\n",
    "    eeg_full_windows = []\n",
    "    meta_data = []\n",
    "    for i in range(number_of_windows):\n",
    "        center_point = np.random.randint(min_center, max_center)\n",
    "        meta_data.append(center_point)\n",
    "        single_eeg_window, single_nirs_window = get_single_window(center_point, \n",
    "                                                                  nirs_data, \n",
    "                                                                  eeg_data, \n",
    "                                                                  eeg_i_min, \n",
    "                                                                  eeg_i_max, \n",
    "                                                                  nirs_i_min, \n",
    "                                                                  nirs_i_max)\n",
    "        \n",
    "        eeg_full_windows.append(single_eeg_window)\n",
    "        nirs_full_windows.append(single_nirs_window)\n",
    "    \n",
    "    nirs_full_windows = np.array(nirs_full_windows)\n",
    "    eeg_full_windows = np.array(eeg_full_windows)\n",
    "\n",
    "    return eeg_full_windows, nirs_full_windows, meta_data\n",
    "\n",
    "class EEGfNIRSData(Dataset):\n",
    "    def __init__(self, fnirs_data, eeg_data):\n",
    "        self.fnirs_data = fnirs_data\n",
    "        self.eeg_data = eeg_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.eeg_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.fnirs_data[idx], self.eeg_data[idx]\n",
    "\n",
    "def plot_series(target, output, epoch):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(target, label='Target')\n",
    "    plt.plot(output, label='Output', linestyle='--')\n",
    "    plt.title(f'Epoch {epoch + 1}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def find_indices(x, y):\n",
    "    indices = []\n",
    "    for item in y:\n",
    "        if item in x:\n",
    "            indices.append(x.index(item))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746c215",
   "metadata": {},
   "source": [
    "### Windowed correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5dfda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def continuous_correlation2_tensor_looped(X, Y, L, numBins, multipliermethod='multiprod'):\n",
    "    N, T1 = X.shape\n",
    "    M, T2 = Y.shape\n",
    "    if T1 != T2:\n",
    "        raise ValueError('columns have to match')\n",
    "    else:\n",
    "        T = T1\n",
    "    \n",
    "    # Gaussian process matrix with weights to nearby time points\n",
    "    gpMat = np.exp(-cdist(np.arange(1, T+1).reshape(-1, 1), np.linspace(1, T, numBins).reshape(-1, 1))**2 / (2 * L**2))\n",
    "    \n",
    "    covMat = np.zeros((N, M, numBins))\n",
    "    corMat = np.zeros((N, M, numBins))\n",
    "    covMatX = np.zeros((N, N, numBins))\n",
    "    covMatY = np.zeros((M, M, numBins))\n",
    "    \n",
    "    for t in range(numBins):\n",
    "        Xgp = X * gpMat[:, t]\n",
    "        Ygp = Y * gpMat[:, t]\n",
    "        Xmovingavg = np.sum(X * gpMat[:, t], axis=1) / np.sum(gpMat[:, t])\n",
    "        Ymovingavg = np.sum(Y * gpMat[:, t], axis=1) / np.sum(gpMat[:, t])\n",
    "        \n",
    "        Xgp = Xgp - np.outer(Xmovingavg, gpMat[:, t])\n",
    "        Ygp = Ygp - np.outer(Ymovingavg, gpMat[:, t])\n",
    "        \n",
    "        covMat[:, :, t] = np.dot(Xgp, Ygp.T)\n",
    "        covMatX[:, :, t] = np.dot(Xgp, Xgp.T)\n",
    "        covMatY[:, :, t] = np.dot(Ygp, Ygp.T)\n",
    "        \n",
    "        DX = np.sqrt(np.diag(1.0 / np.diag(covMatX[:, :, t])))\n",
    "        DY = np.sqrt(np.diag(1.0 / np.diag(covMatY[:, :, t])))\n",
    "        \n",
    "        corMat[:, :, t] = np.dot(np.dot(np.diag(DX), covMat[:, :, t]), np.diag(DY))\n",
    "    \n",
    "    return corMat\n",
    "\n",
    "# Function to perform rolling correlation and plot the results\n",
    "def rolling_correlation_two_sided(X, X_pred, chan_labels, offset, sampling_frequency, timeSigma, num_bins, zoom_start=None, zoom_end=None, ax=None):\n",
    "    corMat = continuous_correlation2_tensor_looped(X, X_pred, timeSigma, num_bins)\n",
    "    cor = np.zeros(X.shape)\n",
    "    \n",
    "    # Correcting zoom to handle one-dimensional array scaling\n",
    "    for i in range(X.shape[0]):\n",
    "        cor[i, :] = scipy.ndimage.zoom(corMat[i, i, :], X.shape[1] / corMat.shape[2], order=0)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    x_axis = np.tile(np.arange(X.shape[1]), (X.shape[0], 1))\n",
    "    y_axis_target = X + offset * np.arange(1, X.shape[0] + 1).reshape(-1, 1)\n",
    "    y_axis_predicted = X_pred + offset * np.arange(1, X_pred.shape[0] + 1).reshape(-1, 1)\n",
    "    \n",
    "    for idx, (data, y_axis) in enumerate(zip([X, X_pred], [y_axis_target, y_axis_predicted])):\n",
    "        scatter = ax[idx].scatter(x_axis.ravel(), y_axis.ravel(), c=cor.ravel(), cmap='viridis', s=1, vmin=-1, vmax=1)\n",
    "        ax[idx].set_title('Target' if idx == 0 else 'Predicted')\n",
    "        ax[idx].set_xlabel('time(s)')\n",
    "        ax[idx].set_ylabel('Channels')\n",
    "        ax[idx].set_yticks(offset * np.arange(data.shape[0]))\n",
    "        ax[idx].set_yticklabels(chan_labels)\n",
    "        ax[idx].set_xticklabels(np.round(ax[idx].get_xticks() / sampling_frequency, 2))\n",
    "        plt.colorbar(scatter, ax=ax[idx], label='Correlation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Function to perform rolling correlation and plot the results\n",
    "def rolling_correlation(X, X_pred, chan_labels, offset, sampling_frequency, timeSigma, num_bins, zoom_start=None, zoom_end=None, do_legend=True, do_colorbar=True, ax=None, title=None):\n",
    "    corMat = continuous_correlation2_tensor_looped(X, X_pred, timeSigma, num_bins)\n",
    "    cor = np.zeros(X.shape)\n",
    "    \n",
    "    # Correcting zoom to handle one-dimensional array scaling\n",
    "    for i in range(X.shape[0]):\n",
    "        cor[i, :] = scipy.ndimage.zoom(corMat[i, i, :], X.shape[1] / corMat.shape[2], order=0)\n",
    "\n",
    "    # plot single plot of X signal ontop X_pred signal colored by correlation\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(18, 6))\n",
    "    x_axis = np.arange(X.shape[1])\n",
    "    y_axis_target = X + offset * np.arange(1, X.shape[0] + 1).reshape(-1, 1)\n",
    "    y_axis_predicted = X_pred + offset * np.arange(1, X_pred.shape[0] + 1).reshape(-1, 1)\n",
    "    # scatter = ax.scatter(x_axis.ravel(), y_axis_predicted.ravel(), c=cor.ravel(), cmap='viridis', s=1, vmin=-1, vmax=1)\n",
    "    # scatter = ax.scatter(x_axis.ravel(), y_axis_target.ravel(), c=cor.ravel(), cmap='viridis', s=1, vmin=-1, vmax=1)\n",
    "    # scatter = ax.scatter(x_axis.ravel(), y_axis_predicted.ravel(), s=1, vmin=-1, vmax=1, c='pink', alpha=0.8)\n",
    "\n",
    "    for (data, y_axis, color) in zip([X, X_pred], [y_axis_target, y_axis_predicted], [ {'cmap':'viridis'}, {'color':'pink'},]):\n",
    "        time = (np.arange(data.shape[1]) / sampling_frequency).reshape(1,-1)  # Convert index to time\n",
    "        points = np.array([time, y_axis]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "        \n",
    "        norm = plt.Normalize(-1, 1)\n",
    "        if 'cmap' in color:\n",
    "            lc = LineCollection(segments, cmap=color['cmap'], norm=norm, linewidth=2, label='Target')\n",
    "            lc.set_array(cor.ravel())  # Color the lines by the correlation\n",
    "        elif 'color' in color:\n",
    "            # opacity = 0.5\n",
    "            lc = LineCollection(segments, color=color['color'], norm=norm, linewidth=1, label='Predicted', alpha=0.8)\n",
    "\n",
    "        ax.add_collection(lc, autolim=True)\n",
    "    ax.set_yticks(offset * np.arange(X.shape[0]))\n",
    "    ax.set_yticklabels(chan_labels)\n",
    "\n",
    "    ax.set_ylim(min(y_axis_target.min(), y_axis_predicted.min()), max(y_axis_target.max(), y_axis_predicted.max()))\n",
    "    if zoom_start is not None and zoom_end is not None:\n",
    "        ax.set_xlim(zoom_start, zoom_end)\n",
    "    else:\n",
    "        ax.set_xlim(time.min(), time.max())\n",
    "\n",
    "    ax.set_xlabel('time(s)')\n",
    "    ax.set_ylabel('Channels')\n",
    "    if do_legend:\n",
    "        ax.legend()\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    if do_colorbar:\n",
    "        plt.colorbar(lc, ax=ax, label='Correlation')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return ax.get_figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88d303",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def eeg_baseline_correlation(eeg_data, offset=0, timeSigma=100, num_bins=50):\n",
    "    '''\n",
    "    Randomly shuffle a single EEG channel and calculate the windowed correlation between the shuffled data and the original data.\n",
    "    Plot over each single eeg channel.\n",
    "    Inputs:\n",
    "        eeg_data (numpy array): The EEG data with dimensions (samples x windows x n_channels)\n",
    "        offset (int): The offset to apply to the y-axis of the plot\n",
    "        timeSigma (int): The timeSigma parameter for the rolling correlation\n",
    "        num_bins (int): The number of bins for the rolling correlation\n",
    "    '''\n",
    "    n_samples, n_windows, n_channels = eeg_data.shape\n",
    "    highest_r2 = 0\n",
    "    \n",
    "    fig, axs = plt.subplots(n_channels, 1, figsize=(18, 100))\n",
    "    for i in range(n_channels):\n",
    "        type_label = 'Baseline'\n",
    "        do_legend = False\n",
    "        do_colorbar = True\n",
    "\n",
    "        target = eeg_data[:,:,i]\n",
    "        prediction = np.copy(target)\n",
    "        np.random.shuffle(prediction)\n",
    "\n",
    "        target_average = np.mean(target, axis=0)\n",
    "        prediction_average = np.mean(prediction, axis=0)\n",
    "        r2 = r2_score(target_average, prediction_average)\n",
    "        if r2 > highest_r2:\n",
    "            highest_r2 = r2\n",
    "        spearman_corr, spearman_p = stats.spearmanr(target_average, prediction_average)\n",
    "\n",
    "        channel_label = list(EEG_COORDS.keys())[i]\n",
    "\n",
    "        # reshape to 1xn\n",
    "        target = target.reshape(1, -1)\n",
    "        prediction = prediction.reshape(1, -1)\n",
    "\n",
    "        chan_labels = [channel_label]  # Assuming label as provided in the command\n",
    "        test_fig = rolling_correlation(target, \n",
    "                                    prediction, \n",
    "                                    chan_labels, \n",
    "                                    offset=offset,\n",
    "                                    sampling_frequency=eeg_lookback,\n",
    "                                    timeSigma=timeSigma, \n",
    "                                    num_bins=num_bins, \n",
    "                                    zoom_start=0, \n",
    "                                    #    zoom_end=500, \n",
    "                                    do_legend=do_legend,\n",
    "                                    do_colorbar=do_colorbar,\n",
    "                                    ax=axs[i, j], \n",
    "                                    title='')\n",
    "                \n",
    "        axs[i, j].text(0.5, 0.9, f'{channel_label} {type_label} R-squared: {r2:.4f} Spearman: {spearman_corr}', horizontalalignment='center', verticalalignment='center', transform=axs[i, j].transAxes)\n",
    "    \n",
    "    fig.savefig(os.path.join(OUTPUT_DIRECTORY, f'Baseline_{highest_r2:.4f}.jpeg'))\n",
    "    plt.close()\n",
    "    print(f'Finished plotting baseline correlation with highest R-squared: {highest_r2:.4f}')\n",
    "    \n",
    "\n",
    "def evaluate_raw_correlation(eeg_data, fnirs_data):\n",
    "    '''\n",
    "    Plot a matrix of n_eeg_channels x n_nirs_channels. For each plot, take one EEG channel and one fNIRS channel.\n",
    "    Normalize the two signals to be between 0 and 1, then plot each time point of a single EEG channel against\n",
    "    each time point in the fNIRS channel. Place text for r2 score and Spearman correlation on the plots.\n",
    "    Inputs:\n",
    "        eeg_data (numpy array): The EEG data with dimensions (samples x n_eeg_channels)\n",
    "        fnirs_data (numpy array): The fNIRS data with dimensions (samples x n_nirs_channels)\n",
    "\n",
    "    Example usage:\n",
    "        eeg_data = np.random.rand(1000, 5)  # 1000 samples, 5 EEG channels\n",
    "        fnirs_data = np.random.rand(1000, 4)  # 1000 samples, 4 fNIRS channels\n",
    "        evaluate_raw_correlation(eeg_data, fnirs_data)\n",
    "    '''\n",
    "\n",
    "    # Normalize EEG and fNIRS data\n",
    "    eeg_normalized = (eeg_data - np.min(eeg_data, axis=0)) / (np.max(eeg_data, axis=0) - np.min(eeg_data, axis=0))\n",
    "    fnirs_normalized = (fnirs_data - np.min(fnirs_data, axis=0)) / (np.max(fnirs_data, axis=0) - np.min(fnirs_data, axis=0))\n",
    "    \n",
    "    n_eeg_channels = eeg_normalized.shape[1]\n",
    "    n_fnirs_channels = fnirs_normalized.shape[1]\n",
    "\n",
    "    fig, axs = plt.subplots(n_eeg_channels, n_fnirs_channels, figsize=(n_fnirs_channels * 4, n_eeg_channels * 4))\n",
    "    for i in range(n_eeg_channels):\n",
    "        for j in range(n_fnirs_channels):\n",
    "            ax = axs[i, j] if n_eeg_channels > 1 and n_fnirs_channels > 1 else axs[i][j] if n_fnirs_channels == 1 else axs[j]\n",
    "            \n",
    "            # Scatter plot of EEG vs. fNIRS normalized data\n",
    "            ax.scatter(eeg_normalized[:, i], fnirs_normalized[:, j], alpha=0.5)\n",
    "            \n",
    "            # Calculate R² score and Spearman correlation\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(eeg_normalized[:, i], fnirs_normalized[:, j])\n",
    "            spearman_corr, spearman_p = stats.spearmanr(eeg_normalized[:, i], fnirs_normalized[:, j])\n",
    "            \n",
    "            # Annotate the plot with R² score and Spearman correlation\n",
    "            ax.text(0.05, 0.95, f'R² = {r_value**2:.2f}\\nSpearman = {spearman_corr:.2f}', transform=ax.transAxes, \n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "            ax.set_xlabel(f'EEG Channel {i+1}') if j == 0 else None\n",
    "            ax.set_ylabel(f'fNIRS Channel {j+1}') if i == 0 else None\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(os.path.join(OUTPUT_DIRECTORY, f'Baseline_correlation_matrix.jpeg'))\n",
    "    plt.close()\n",
    "\n",
    "    print('Finished plotting correlation matrix')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60187436",
   "metadata": {},
   "source": [
    "## RUN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_functions = {\n",
    "    'rnn': create_rnn,\n",
    "    'mlp': create_mlp,\n",
    "    'transformer': create_transformer\n",
    "}\n",
    "loss_amounts = {\n",
    "    'rnn': 0.01,\n",
    "    'mlp': 0.001,\n",
    "    'transformer': 0.01\n",
    "}\n",
    "\n",
    "def run_model(subject_id_int,\n",
    "              model_name_base, \n",
    "              nirs_channels_to_use_base, \n",
    "              eeg_channels_to_use, \n",
    "              eeg_channel_index, \n",
    "              nirs_channel_index, \n",
    "              num_epochs, \n",
    "              redo_train=False):\n",
    "    model_function = model_functions[model_name_base]\n",
    "    loss_amount = loss_amounts[model_name_base]\n",
    "    subject_id = f'{subject_id_int:02d}'\n",
    "    model_name = f'baseline_{model_name_base}_{subject_id}'\n",
    "    final_model_path = os.path.join(MODEL_WEIGHTS, f'{model_name}_{num_epochs}.pth')\n",
    "    if os.path.exists(final_model_path) and not redo_train:\n",
    "        print(f'Model name exists, skipping {model_name}')\n",
    "    else:\n",
    "        print(f'Starting {model_name}')\n",
    "        # model = model_function(len(nirs_channels_to_use_base)*nirs_token_size, len(eeg_channels_to_use)*eeg_token_size)\n",
    "            \n",
    "        # Pre-allocate memory for training and testing data\n",
    "        eeg_data, nirs_data, mrk_data = read_matlab_file(subject_id)\n",
    "\n",
    "        print(f'EEG Shape: {eeg_data.shape}')\n",
    "        print(f'NIRS Shape: {nirs_data.shape}')\n",
    "        print(f'MRK Shape: {mrk_data.shape}')\n",
    "\n",
    "        # Baselines\n",
    "\n",
    "        # plot baseline with nirs\n",
    "        evaluate_raw_correlation(eeg_data, nirs_data)\n",
    "\n",
    "        eeg_windowed_ordered,_ ,_ = grab_ordered_windows(\n",
    "            nirs_data=eeg_data, \n",
    "            eeg_data=nirs_data,\n",
    "            sampling_rate=200,\n",
    "            nirs_t_min=nirs_t_min, \n",
    "            nirs_t_max=nirs_t_max,\n",
    "            eeg_t_min=0, \n",
    "            eeg_t_max=1)\n",
    "    \n",
    "        eeg_windowed_ordered = eeg_windowed_ordered[:, :, :eeg_lookback]\n",
    "        eeg_windowed_ordered = eeg_windowed_ordered.transpose(0,2,1)\n",
    "        eeg_windowed_ordered = eeg_windowed_ordered[:,:, eeg_channel_index]\n",
    "\n",
    "        # plot baseline\n",
    "        eeg_baseline_correlation(eeg_windowed_ordered, offset=0, timeSigma=100, num_bins=50)\n",
    "\n",
    "        asdasd=asdasd\n",
    "        return\n",
    "\n",
    "        # split train and test on eeg_data, nirs_data, and mrk_data\n",
    "        test_size = int(eeg_data.shape[1]*test_size_in_subject)\n",
    "        train_size = eeg_data.shape[1] - test_size\n",
    "\n",
    "        train_eeg_data = eeg_data[:, :train_size]\n",
    "        train_nirs_data = nirs_data[:, :train_size]\n",
    "        # train_mrk_data = mrk_data[:, :train_size]\n",
    "\n",
    "        test_eeg_data = eeg_data[:, train_size:]\n",
    "        test_nirs_data = nirs_data[:, train_size:]\n",
    "        # test_mrk_data = mrk_data[:, train_size:]\n",
    "\n",
    "        # get CA dict\n",
    "        # cca_dict = get_cca_dict(subject_id, train_nirs_data, train_eeg_data, token_size)\n",
    "        fpca_dict = get_fpca_dict(subject_id, train_nirs_data, train_eeg_data, nirs_token_size, eeg_token_size)\n",
    "        eeg_fpcas = fpca_dict['eeg']\n",
    "        nirs_fpcas = fpca_dict['nirs']\n",
    "\n",
    "        # plot variance explained\n",
    "        plot_explained_variance_over_dict(nirs_fpcas, path=os.path.join(OUTPUT_DIRECTORY, f'variance_{model_name}_nirs_fpca.jpeg'), channel_names=nirs_channels_to_use_base)\n",
    "        plot_explained_variance_over_dict(eeg_fpcas, path=os.path.join(OUTPUT_DIRECTORY, f'variance_{model_name}_eeg_fpca.jpeg'), channel_names=eeg_channels_to_use)\n",
    "\n",
    "        # Train data\n",
    "        eeg_windowed_train, nirs_windowed_train, meta_data = grab_random_windows(\n",
    "                        nirs_data=train_nirs_data, \n",
    "                        eeg_data=train_eeg_data,\n",
    "                        sampling_rate=200,\n",
    "                        nirs_t_min=nirs_t_min, \n",
    "                        nirs_t_max=nirs_t_max,\n",
    "                        eeg_t_min=0, \n",
    "                        eeg_t_max=1,\n",
    "                        number_of_windows=10000)\n",
    "        \n",
    "        eeg_windowed_train = eeg_windowed_train[:, :, :eeg_lookback]\n",
    "        nirs_windowed_train = nirs_windowed_train[:, :, :fnirs_lookback]\n",
    "\n",
    "        # nirs_windowed_train = perform_cca_over_channels(nirs_windowed_train, cca_dict, token_size)\n",
    "        # eeg_windowed_train = perform_cca_over_channels(eeg_windowed_train, cca_dict, token_size)\n",
    "        nirs_windowed_train = perform_fpca_over_channels(nirs_windowed_train, nirs_fpcas, nirs_token_size)\n",
    "        eeg_windowed_train = perform_fpca_over_channels(eeg_windowed_train, eeg_fpcas, eeg_token_size)\n",
    "\n",
    "        n_channels = nirs_windowed_train.shape[1]\n",
    "        # # plot channels\n",
    "        # fig, axs = plt.subplots(n_channels, 1, figsize=(10, 10))\n",
    "        # for i in range(n_channels):\n",
    "        #     axs[i].plot(nirs_windowed_train[0, i, :])\n",
    "        # plt.show()\n",
    "\n",
    "        # Append to the preallocated arrays\n",
    "        eeg_windowed_train = eeg_windowed_train.transpose(0,2,1)\n",
    "        nirs_windowed_train = nirs_windowed_train.transpose(0,2,1)\n",
    "    \n",
    "        eeg_windowed_train = eeg_windowed_train[:,:, eeg_channel_index]\n",
    "        nirs_windowed_train = nirs_windowed_train[:,:, nirs_channel_index]\n",
    "\n",
    "        print(f'EEG Train Shape: {eeg_windowed_train.shape}')\n",
    "        print(f'NIRS Train Shape: {nirs_windowed_train.shape}')\n",
    "\n",
    "        # Train data in order for visualization\n",
    "        eeg_windowed_train_ordered, nirs_windowed_train_ordered, meta_data = grab_ordered_windows(\n",
    "            nirs_data=train_nirs_data, \n",
    "            eeg_data=train_eeg_data,\n",
    "            sampling_rate=200,\n",
    "            nirs_t_min=nirs_t_min, \n",
    "            nirs_t_max=nirs_t_max,\n",
    "            eeg_t_min=0, \n",
    "            eeg_t_max=1)\n",
    "    \n",
    "        eeg_windowed_train = eeg_windowed_train[:, :, :eeg_lookback]\n",
    "        nirs_windowed_train = nirs_windowed_train[:, :, :fnirs_lookback]\n",
    "                \n",
    "        # nirs_windowed_test = perform_cca_over_channels(nirs_windowed_test, cca_dict, token_size)\n",
    "        nirs_windowed_train_ordered = perform_fpca_over_channels(nirs_windowed_train_ordered, nirs_fpcas, nirs_token_size)\n",
    "        \n",
    "        eeg_windowed_train_ordered = eeg_windowed_train_ordered.transpose(0,2,1)\n",
    "        nirs_windowed_train_ordered = nirs_windowed_train_ordered.transpose(0,2,1)\n",
    "    \n",
    "        eeg_windowed_train_ordered = eeg_windowed_train_ordered[:,:, eeg_channel_index]\n",
    "        nirs_windowed_train_ordered = nirs_windowed_train_ordered[:,:, nirs_channel_index]\n",
    "\n",
    "        print(f'EEG Train Ordered Shape: {eeg_windowed_train_ordered.shape}')\n",
    "        print(f'NIRS Train Ordered Shape: {nirs_windowed_train_ordered.shape}')\n",
    "\n",
    "        # Test data\n",
    "        eeg_windowed_test, nirs_windowed_test, meta_data = grab_ordered_windows(\n",
    "                    nirs_data=test_nirs_data, \n",
    "                    eeg_data=test_eeg_data,\n",
    "                    sampling_rate=200,\n",
    "                    nirs_t_min=nirs_t_min,\n",
    "                    nirs_t_max=nirs_t_max,\n",
    "                    eeg_t_min=0, \n",
    "                    eeg_t_max=1)\n",
    "        \n",
    "        eeg_windowed_train = eeg_windowed_train[:, :, :eeg_lookback]\n",
    "        nirs_windowed_train = nirs_windowed_train[:, :, :fnirs_lookback]\n",
    "\n",
    "        # nirs_windowed_test = perform_cca_over_channels(nirs_windowed_test, cca_dict, token_size)\n",
    "        nirs_windowed_test = perform_fpca_over_channels(nirs_windowed_test, nirs_fpcas, nirs_token_size)\n",
    "        \n",
    "        eeg_windowed_test = eeg_windowed_test.transpose(0,2,1)\n",
    "        nirs_windowed_test = nirs_windowed_test.transpose(0,2,1)\n",
    "    \n",
    "        eeg_windowed_test = eeg_windowed_test[:,:, eeg_channel_index]\n",
    "        nirs_windowed_test = nirs_windowed_test[:,:, nirs_channel_index]\n",
    "\n",
    "        print(f'EEG Test Shape: {eeg_windowed_test.shape}')\n",
    "        print(f'NIRS Test Shape: {nirs_windowed_test.shape}')\n",
    "\n",
    "        # Perform inference on test\n",
    "        nirs_test_tensor = nirs_windowed_test.reshape(-1, len(nirs_channel_index)*nirs_token_size)\n",
    "\n",
    "        nirs_test_tensor = torch.from_numpy(nirs_test_tensor).float()\n",
    "        eeg_test_tensor = torch.from_numpy(eeg_windowed_test).float()\n",
    "\n",
    "        test_dataset = EEGfNIRSData(nirs_test_tensor, eeg_test_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        if do_train:\n",
    "            # flatten channels and tokens\n",
    "            nirs_train_tensor = nirs_windowed_train.reshape(-1, len(nirs_channel_index)*nirs_token_size)\n",
    "            eeg_train_tensor = eeg_windowed_train.reshape(-1, len(eeg_channel_index)*eeg_token_size)\n",
    "            \n",
    "            nirs_train_tensor = torch.from_numpy(nirs_train_tensor).float()\n",
    "            eeg_train_tensor = torch.from_numpy(eeg_train_tensor).float()\n",
    "            meta_data_tensor = torch.from_numpy(np.array(meta_data)).float()\n",
    "            \n",
    "            print(nirs_train_tensor.shape)\n",
    "            print(eeg_train_tensor.shape)\n",
    "            \n",
    "            dataset = EEGfNIRSData(nirs_train_tensor, eeg_train_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=500, shuffle=True)\n",
    "        \n",
    "            latest_epoch = 0\n",
    "            loss_list = []\n",
    "            if do_load:\n",
    "                model_path = f'{model_name}_epoch_1.pth'\n",
    "        \n",
    "                # find the latest model\n",
    "                for file in os.listdir(MODEL_WEIGHTS):\n",
    "                    if file.startswith(f'{model_name}_epoch_'):\n",
    "                        epoch = int(file.split('_')[-1].split('.')[0])\n",
    "                        if epoch > latest_epoch:\n",
    "                            latest_epoch = epoch\n",
    "                            model_path = file\n",
    "                print(f'Using Model Weights: {model_path}')\n",
    "                model.load_state_dict(torch.load(os.path.join(MODEL_WEIGHTS, model_path)))\n",
    "            \n",
    "                # load loss list\n",
    "                with open(os.path.join(MODEL_WEIGHTS, f'loss_{model_name}_{latest_epoch}.csv'), 'r') as file_ptr:\n",
    "                    reader = csv.reader(file_ptr)\n",
    "                    loss_list = list(reader)[0]\n",
    "                print(f'Last loss: {float(loss_list[-1])/len(dataloader):.4f}')\n",
    "        \n",
    "            model.to(DEVICE)\n",
    "        \n",
    "            # Optimizer and loss function\n",
    "            optimizer = Adam(model.parameters(), lr=loss_amount)\n",
    "            loss_function = torch.nn.MSELoss()\n",
    "            train_test_loss_dicct = {'train':[], 'test':[]}\n",
    "            for epoch in range(latest_epoch, num_epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "        \n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "                    X_batch = X_batch.to(DEVICE).float()\n",
    "                    y_batch = y_batch.to(DEVICE).float()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    predictions = model(X_batch)\n",
    "        \n",
    "                    # Loss calculation\n",
    "                    loss = loss_function(predictions, y_batch)\n",
    "        \n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                    total_loss += loss.item()\n",
    "                    # if (batch_idx+1) % 20 == 0 or batch_idx == 0:\n",
    "                    #     print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {loss.item():.4f}')\n",
    "                \n",
    "                loss_list.append(total_loss)\n",
    "        \n",
    "                if (epoch+1) % 50 == 0:\n",
    "                    # Save model weights\n",
    "                    torch.save(model.state_dict(), os.path.join(MODEL_WEIGHTS, f'{model_name}_{epoch+1}.pth'))\n",
    "                    with open(os.path.join(MODEL_WEIGHTS,f'loss_{model_name}_{epoch+1}.csv'), 'w', newline='') as file_ptr:\n",
    "                        wr = csv.writer(file_ptr, quoting=csv.QUOTE_ALL)\n",
    "                        wr.writerow(loss_list)\n",
    "                    \n",
    "                    targets, predictions = predict_eeg(model, \n",
    "                                                    test_loader, \n",
    "                                                    n_samples=eeg_windowed_test.shape[0], \n",
    "                                                    n_channels=eeg_windowed_test.shape[2], \n",
    "                                                    n_lookback=eeg_windowed_test.shape[1],\n",
    "                                                    eeg_token_size=eeg_token_size,\n",
    "                                                    eeg_fpcas=eeg_fpcas)\n",
    "                    highest_r2 = 0\n",
    "                    for channel_id in range(len(eeg_channels_to_use)):\n",
    "                        targets_single = targets[:,:,channel_id]\n",
    "                        predictions_single = predictions[:,:,channel_id]\n",
    "\n",
    "                        target_average = np.mean(targets_single, axis=0)\n",
    "                        prediction_average = np.mean(predictions_single, axis=0)\n",
    "                        r2 = r2_score(target_average, prediction_average)\n",
    "                        if r2 > highest_r2:\n",
    "                            highest_r2 = r2\n",
    "                \n",
    "                    train_test_loss_dicct['train'].append(total_loss / len(dataloader))\n",
    "                    train_test_loss_dicct['test'].append(highest_r2)\n",
    "                    print(f'Epoch: {epoch+1}, Train Loss: {total_loss / len(dataloader):.4f}, Test Loss: {highest_r2:.4f}')\n",
    "                elif (epoch+1) % 10 == 0:\n",
    "                    print(f'Epoch: {epoch+1}, Average Loss: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "            # Plotting train vs test loss\n",
    "            fig, axs = plt.subplots(2, 1, figsize=(18, 6))\n",
    "            axs[0].plot(train_test_loss_dicct['train'], label='Train Loss')\n",
    "            axs[1].plot(train_test_loss_dicct['test'], label='Test R2')\n",
    "\n",
    "            axs[0].set_xticklabels(np.arange(0, num_epochs+1, 50))\n",
    "            axs[1].set_xticklabels(np.arange(0, num_epochs+1, 50))\n",
    "            \n",
    "            axs[1].set_xlabel('Epoch')\n",
    "            axs[0].set_ylabel('Loss')\n",
    "            axs[0].set_title(f'Loss for {model_name}')\n",
    "            axs[0].legend()\n",
    "            axs[1].legend()\n",
    "            axs[0].grid(True)\n",
    "            axs[1].grid(True)\n",
    "            fig.savefig(os.path.join(OUTPUT_DIRECTORY, f'loss_{model_name}.jpeg'))\n",
    "            plt.close()\n",
    "\n",
    "        # Perform inference on ordered train\n",
    "        nirs_train_tensor = nirs_windowed_train_ordered.reshape(-1, len(nirs_channel_index)*nirs_token_size)\n",
    "        nirs_train_tensor = torch.from_numpy(nirs_train_tensor).float()\n",
    "        eeg_train_tensor = torch.from_numpy(eeg_windowed_train_ordered).float()\n",
    "\n",
    "        # Assuming fnirs_test and eeg_test are your test datasets\n",
    "        train_dataset = EEGfNIRSData(nirs_train_tensor, eeg_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Get weights for specific epoch\n",
    "        weight_epochs = ['baseline'] #[100, 250, 500, 1000]\n",
    "        for weight_epoch in weight_epochs:\n",
    "            model_path = f'{model_name}_{weight_epoch}.pth'\n",
    "            model.load_state_dict(torch.load(os.path.join(MODEL_WEIGHTS, model_path)))\n",
    "            model.to(DEVICE)\n",
    "            model.eval()\n",
    "\n",
    "            target_train, predictions_train = predict_eeg(model, \n",
    "                                            train_loader, \n",
    "                                            n_samples=eeg_windowed_train_ordered.shape[0], \n",
    "                                            n_channels=eeg_windowed_train_ordered.shape[2], \n",
    "                                            n_lookback=eeg_windowed_train_ordered.shape[1],\n",
    "                                            eeg_token_size=eeg_token_size,\n",
    "                                            eeg_fpcas=eeg_fpcas)\n",
    "            targets, predictions = predict_eeg(model, \n",
    "                                            test_loader, \n",
    "                                            n_samples=eeg_windowed_test.shape[0], \n",
    "                                            n_channels=eeg_windowed_test.shape[2], \n",
    "                                            n_lookback=eeg_windowed_test.shape[1],\n",
    "                                            eeg_token_size=eeg_token_size,\n",
    "                                            eeg_fpcas=eeg_fpcas)\n",
    "            \n",
    "            print(f'Weight Epoch: {weight_epoch}')\n",
    "            print(f'Predictions Shape: {predictions.shape}')\n",
    "            print(f'Targets Shape: {targets.shape}')\n",
    "        \n",
    "            # scipy.io.savemat(os.path.join(OUTPUT_DIRECTORY, f'test_{model_name}_{weight_epoch}.mat'), {'X': targets, \n",
    "            #                                                         'XPred':predictions,\n",
    "            #                                                     'bins':10,\n",
    "            #                                                     'scale':10,\n",
    "            #                                       F              'srate':200})\n",
    "            \n",
    "            offset = 0  # Assuming no offset\n",
    "            timeSigma = 100  # Assuming a given timeSigma\n",
    "            num_bins = 50  # Assuming a given number of bins\n",
    "\n",
    "            # Plotting target vs. output on concatenated data subplots for each channel\n",
    "            highest_r2 = 0\n",
    "            counter = 0\n",
    "            fig, axs = plt.subplots(len(eeg_channels_to_use), 2, figsize=(18, 100))\n",
    "            for i in range(len(eeg_channels_to_use)):\n",
    "                for j in range(2):\n",
    "                    if j == 0:\n",
    "                        type_label = 'Train'\n",
    "                        do_legend = True\n",
    "                        do_colorbar = False\n",
    "                        targets_single = target_train[:,:,counter]\n",
    "                        predictions_single = predictions_train[:,:,counter]\n",
    "\n",
    "                        target_average = np.mean(targets_single, axis=0)\n",
    "                        prediction_average = np.mean(predictions_single, axis=0)\n",
    "                        r2 = r2_score(target_average, prediction_average)\n",
    "                        \n",
    "                        channel_label = list(EEG_COORDS.keys())[counter]\n",
    "                    else:\n",
    "                        type_label = 'Test'\n",
    "                        do_legend = False\n",
    "                        do_colorbar = True\n",
    "                        targets_single = targets[:,:,counter]\n",
    "                        predictions_single = predictions[:,:,counter]\n",
    "\n",
    "                        target_average = np.mean(targets_single, axis=0)\n",
    "                        prediction_average = np.mean(predictions_single, axis=0)\n",
    "                        r2 = r2_score(target_average, prediction_average)\n",
    "                        if r2 > highest_r2:\n",
    "                            highest_r2 = r2\n",
    "\n",
    "                        channel_label = list(EEG_COORDS.keys())[counter]\n",
    "                        counter += 1\n",
    "\n",
    "                    # reshape to 1xn\n",
    "                    targets_single = targets_single.reshape(1, -1)\n",
    "                    predictions_single = predictions_single.reshape(1, -1)\n",
    "\n",
    "                    chan_labels = [channel_label]  # Assuming label as provided in the command\n",
    "                    test_fig = rolling_correlation(targets_single, \n",
    "                                                predictions_single, \n",
    "                                                chan_labels, \n",
    "                                                offset=offset,\n",
    "                                                sampling_frequency=eeg_lookback,\n",
    "                                                timeSigma=timeSigma, \n",
    "                                                num_bins=num_bins, \n",
    "                                                zoom_start=0, \n",
    "                                                #    zoom_end=500, \n",
    "                                                do_legend=do_legend,\n",
    "                                                do_colorbar=do_colorbar,\n",
    "                                                ax=axs[i, j], \n",
    "                                                title='')\n",
    "                            \n",
    "                    axs[i, j].text(0.5, 0.9, f'{eeg_channels_to_use[i]} {type_label} R-squared: {r2:.4f}', horizontalalignment='center', verticalalignment='center', transform=axs[i, j].transAxes)\n",
    "            fig.savefig(os.path.join(OUTPUT_DIRECTORY, f'test_{highest_r2:.4f}_{model_name}_{weight_epoch}.jpeg'))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92787d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels to use\n",
    "nirs_channels_to_use_base = list(NIRS_COORDS.keys())\n",
    "nirs_channel_index = find_indices(list(NIRS_COORDS.keys()),nirs_channels_to_use_base)\n",
    "\n",
    "eeg_channels_to_use = EEG_CHANNEL_NAMES\n",
    "eeg_channel_index = find_indices(EEG_CHANNEL_NAMES,eeg_channels_to_use)\n",
    "\n",
    "for subject_id_int in subject_ids[8:]:\n",
    "    for model_name_base in ['rnn', 'mlp']:\n",
    "        run_model(subject_id_int, \n",
    "                  model_name_base, \n",
    "                  nirs_channels_to_use_base, \n",
    "                  eeg_channels_to_use, \n",
    "                  eeg_channel_index, \n",
    "                  nirs_channel_index, \n",
    "                  num_epochs, \n",
    "                  redo_train=False)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1dc54",
   "metadata": {},
   "source": [
    "## Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71ea0f-4185-41e8-a2ee-53c86288f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdas=adasd\n",
    "test = [4]\n",
    "test_subject_id = ''\n",
    "for i in test:\n",
    "    test_subject_id += f'{i:02d}'\n",
    "    \n",
    "model_name = f'transformer_{test_subject_id}'\n",
    "\n",
    "# Define channels to use\n",
    "nirs_channels_to_use_base = list(NIRS_COORDS.keys())[:16]\n",
    "nirs_channel_index = find_indices(list(NIRS_COORDS.keys()),nirs_channels_to_use_base)\n",
    "\n",
    "eeg_channels_to_use = EEG_CHANNEL_NAMES\n",
    "eeg_channel_index = find_indices(EEG_CHANNEL_NAMES,eeg_channels_to_use)\n",
    "\n",
    "final_model_path = os.path.join(MODEL_WEIGHTS, f'{model_name}_{num_epochs}.pth')\n",
    "if os.path.exists(final_model_path):\n",
    "    print(f'Model name exists, skipping {model_name}')\n",
    "else:\n",
    "    print(f'Starting {model_name}')\n",
    "    model = create_mlp(nirs_channels_to_use_base, eeg_channels_to_use, token_size)\n",
    "        \n",
    "    # Pre-allocate memory for training and testing data\n",
    "    eeg_windowed_train = np.empty((0, eeg_lookback, len(eeg_channels_to_use)))\n",
    "    nirs_windowed_train = np.empty((0, fnirs_lookback, len(nirs_channels_to_use_base)))\n",
    "    eeg_windowed_test = np.empty((0, eeg_lookback, len(eeg_channels_to_use)))\n",
    "    nirs_windowed_test = np.empty((0, fnirs_lookback, len(nirs_channels_to_use_base)))\n",
    "    \n",
    "    for i in subject_ids:\n",
    "        subject_id = f'{i:02d}'\n",
    "    \n",
    "        subject_data = loadmat(os.path.join(BASE_PATH, 'matfiles', f'data_vp0{subject_id}.mat'))['subject_data_struct'][0]\n",
    "        # eeg subject_data[1][0]\n",
    "        eeg_data = []\n",
    "        for session_eeg_data in subject_data[1][0]:\n",
    "            eeg_data.append(session_eeg_data.T)\n",
    "        eeg_data = np.hstack(eeg_data)\n",
    "        # fnirs subject_data[3][0]\n",
    "        nirs_data = []\n",
    "        for session_nirs_data in subject_data[3][0]:\n",
    "            nirs_data.append(session_nirs_data.T)\n",
    "        nirs_data = np.hstack(nirs_data)\n",
    "        # mrk subject_data[5][0]\n",
    "    \n",
    "        assert eeg_data.shape[1] == nirs_data.shape[1]\n",
    "        \n",
    "        if i not in test and do_train:\n",
    "            single_eeg_windowed_train, single_nirs_windowed_train, meta_data = grab_random_windows(\n",
    "                            nirs_data=nirs_data, \n",
    "                            eeg_data=eeg_data,\n",
    "                            sampling_rate=200,\n",
    "                            nirs_t_min=nirs_t_min, \n",
    "                            nirs_t_max=nirs_t_max,\n",
    "                            eeg_t_min=0, \n",
    "                            eeg_t_max=1,\n",
    "                            number_of_windows=10000)\n",
    "            \n",
    "            print(single_eeg_windowed_train.shape) # (1000, 30, 200)\n",
    "            print(single_nirs_windowed_train.shape) # (1000, 30, 200)\n",
    "                \n",
    "            # reduce data\n",
    "            print(f'Start reducing data')\n",
    "            if not os.path.exists(CCA_DICT_PATH):\n",
    "                cca_dict = fit_cca_model(single_eeg_windowed_train, single_nirs_windowed_train, token_size)\n",
    "                joblib.dump(cca_dict, CCA_DICT_PATH)\n",
    "            else:\n",
    "                cca_dict = joblib.load(CCA_DICT_PATH)\n",
    "\n",
    "            single_nirs_windowed_train = perform_cca_over_channels(single_nirs_windowed_train, cca_dict, token_size)\n",
    "\n",
    "            print(single_eeg_windowed_train.shape) # (1000, 30, 200)\n",
    "            print(single_nirs_windowed_train.shape) # (1000, 36, 10)\n",
    "\n",
    "            n_channels = single_nirs_windowed_train.shape[1]\n",
    "            # plot channels\n",
    "            fig, axs = plt.subplots(n_channels, 1, figsize=(10, 10))\n",
    "            for i in range(n_channels):\n",
    "                axs[i].plot(single_nirs_windowed_train[0, i, :])\n",
    "            plt.show()\n",
    "    \n",
    "            # Append to the preallocated arrays\n",
    "            single_eeg_transposed = single_eeg_windowed_train.transpose(0,2,1)\n",
    "            single_nirs_transposed = single_nirs_windowed_train.transpose(0,2,1)\n",
    "    \n",
    "            single_eeg_transposed = single_eeg_transposed[:,:eeg_lookback, eeg_channel_index]\n",
    "            single_nirs_transposed = single_nirs_transposed[:,:, nirs_channel_index]\n",
    "    \n",
    "            # Stack new data into the existing array, avoiding list append\n",
    "            eeg_windowed_train = np.vstack((eeg_windowed_train, single_eeg_transposed))\n",
    "            nirs_windowed_train = np.vstack((nirs_windowed_train, single_nirs_transposed))   \n",
    "        elif i not in test and not do_train:\n",
    "            \n",
    "            assert os.path.exists(CCA_DICT_PATH)\n",
    "            cca_dict = joblib.load(CCA_DICT_PATH)\n",
    "\n",
    "            single_eeg_windowed_train, single_nirs_windowed_train, meta_data = grab_ordered_windows(\n",
    "                    nirs_data=nirs_data, \n",
    "                    eeg_data=eeg_data,\n",
    "                    sampling_rate=200,\n",
    "                    nirs_t_min=nirs_t_min, \n",
    "                    nirs_t_max=nirs_t_max,\n",
    "                    eeg_t_min=0, \n",
    "                    eeg_t_max=1)\n",
    "                \n",
    "            single_nirs_windowed_train = perform_cca_over_channels(single_nirs_windowed_train, cca_dict, token_size)\n",
    "            \n",
    "            single_eeg_windowed_train = single_eeg_windowed_train.transpose(0,2,1)\n",
    "            single_nirs_windowed_train = single_nirs_windowed_train.transpose(0,2,1)\n",
    "        \n",
    "            single_eeg_windowed_train = single_eeg_windowed_train[:,:eeg_lookback, eeg_channel_index]\n",
    "            single_nirs_windowed_train = single_nirs_windowed_train[:,:, nirs_channel_index]\n",
    "            \n",
    "            # For test data, direct stacking since no windowing\n",
    "            eeg_windowed_train = np.vstack((eeg_windowed_train, single_eeg_windowed_train))\n",
    "            nirs_windowed_train = np.vstack((nirs_windowed_train, single_nirs_windowed_train))\n",
    "        else:\n",
    "            assert os.path.exists(CCA_DICT_PATH)\n",
    "            cca_dict = joblib.load(CCA_DICT_PATH)\n",
    "\n",
    "            single_eeg_windowed_test, single_nirs_windowed_test, meta_data = grab_ordered_windows(\n",
    "                    nirs_data=nirs_data, \n",
    "                    eeg_data=eeg_data,\n",
    "                    sampling_rate=200,\n",
    "                    nirs_t_min=nirs_t_min, \n",
    "                    nirs_t_max=nirs_t_max,\n",
    "                    eeg_t_min=0, \n",
    "                    eeg_t_max=1)\n",
    "            \n",
    "            single_eeg_windowed_test = single_eeg_windowed_test.transpose(0,2,1)\n",
    "            single_nirs_windowed_test = single_nirs_windowed_test.transpose(0,2,1)\n",
    "            \n",
    "            single_eeg_windowed_test = single_eeg_windowed_test[:,:eeg_lookback, eeg_channel_index]\n",
    "            single_nirs_windowed_test = single_nirs_windowed_test[:,:, nirs_channel_index]\n",
    "            \n",
    "            # For test data, direct stacking since no windowing\n",
    "            eeg_windowed_test = np.vstack((eeg_windowed_test, single_eeg_windowed_test))\n",
    "            nirs_windowed_test = np.vstack((nirs_windowed_test, single_nirs_windowed_test))\n",
    "            \n",
    "            print(f'Skipping {subject_id}')\n",
    "\n",
    "    print(f'EEG Shape: {eeg_windowed_train.shape}')\n",
    "    print(f'NIRS Shape: {nirs_windowed_train.shape}')\n",
    "    \n",
    "    if do_train:\n",
    "        nirs_train_tensor = torch.from_numpy(nirs_windowed_train).float()\n",
    "        eeg_train_tensor = torch.from_numpy(eeg_windowed_train).float()\n",
    "        meta_data_tensor = torch.from_numpy(np.array(meta_data)).float()\n",
    "        \n",
    "        print(nirs_train_tensor.shape)\n",
    "        print(eeg_train_tensor.shape)\n",
    "        \n",
    "        sequence_length = eeg_train_tensor.shape[1]\n",
    "        eeg_number_of_features = eeg_train_tensor.shape[2]\n",
    "        nirs_number_of_features = nirs_train_tensor.shape[2]\n",
    "        \n",
    "        dataset = EEGfNIRSData(nirs_train_tensor, eeg_train_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=500, shuffle=True)\n",
    "    \n",
    "        latest_epoch = 0\n",
    "        loss_list = []\n",
    "        if do_load:\n",
    "            model_path = f'{model_name}_epoch_1.pth'\n",
    "    \n",
    "            # find the latest model\n",
    "            for file in os.listdir(MODEL_WEIGHTS):\n",
    "                if file.startswith(f'{model_name}_epoch_'):\n",
    "                    epoch = int(file.split('_')[-1].split('.')[0])\n",
    "                    if epoch > latest_epoch:\n",
    "                        latest_epoch = epoch\n",
    "                        model_path = file\n",
    "            print(f'Using Model Weights: {model_path}')\n",
    "            model.load_state_dict(torch.load(os.path.join(MODEL_WEIGHTS, model_path)))\n",
    "            \n",
    "            # load loss list\n",
    "            with open(os.path.join(MODEL_WEIGHTS, f'loss_{model_name}_{latest_epoch}.csv'), 'r') as file_ptr:\n",
    "                reader = csv.reader(file_ptr)\n",
    "                loss_list = list(reader)[0]\n",
    "            print(f'Last loss: {float(loss_list[-1])/len(dataloader):.4f}')\n",
    "    \n",
    "        model.to(DEVICE)\n",
    "    \n",
    "        # Optimizer and loss function\n",
    "        optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "        for epoch in range(latest_epoch, num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "    \n",
    "            for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "                X_batch = X_batch.to(DEVICE).float()\n",
    "                y_batch = y_batch.to(DEVICE).float()\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(X_batch)\n",
    "    \n",
    "                # Loss calculation\n",
    "                loss = loss_function(predictions, y_batch)\n",
    "    \n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item()\n",
    "                # if (batch_idx+1) % 20 == 0 or batch_idx == 0:\n",
    "                #     print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "            loss_list.append(total_loss)\n",
    "        \n",
    "            if (epoch+1) % 50 == 0:\n",
    "                # Save model weights\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_WEIGHTS, f'{model_name}_{epoch+1}.pth'))\n",
    "                with open(os.path.join(MODEL_WEIGHTS,f'loss_{model_name}_{epoch+1}.csv'), 'w', newline='') as file_ptr:\n",
    "                    wr = csv.writer(file_ptr, quoting=csv.QUOTE_ALL)\n",
    "                    wr.writerow(loss_list)\n",
    "                \n",
    "            # Plotting target vs. output for the first example in the last batch\n",
    "            # single_actual = y_batch[0, :, 0].detach().cpu().numpy()\n",
    "            # single_predicted = predictions[0,:,0].detach().cpu().numpy()\n",
    "            # r2 = r2_score(single_actual, single_predicted)\n",
    "            # print(f'R-squared: {r2}')\n",
    "            # if (epoch+1) % 10 == 0:\n",
    "            #     plot_series(single_actual, single_predicted, epoch)\n",
    "    \n",
    "            print(f'Epoch: {epoch+1}, Average Loss: {total_loss / len(dataloader):.4f}')\n",
    "    \n",
    "    # Perform inference on test\n",
    "    \n",
    "    nirs_test_tensor = torch.from_numpy(nirs_windowed_test).float()\n",
    "    eeg_test_tensor = torch.from_numpy(eeg_windowed_test).float()\n",
    "    \n",
    "    # Assuming fnirs_test and eeg_test are your test datasets\n",
    "    test_dataset = EEGfNIRSData(nirs_test_tensor, eeg_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Get weights for specific epoch\n",
    "    weight_epochs = [50,100]\n",
    "    for weight_epoch in weight_epochs:\n",
    "        model_path = f'{model_name}_{weight_epoch}.pth'\n",
    "        model.load_state_dict(torch.load(os.path.join(MODEL_WEIGHTS, model_path)))\n",
    "        model.to(DEVICE)\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Perform inference on test data\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(test_loader):\n",
    "            X_batch = X_batch.to(DEVICE).float()\n",
    "            y_batch = y_batch.to(DEVICE).float()\n",
    "            predictions.append(model(X_batch).detach().cpu().numpy())\n",
    "            targets.append(y_batch.detach().cpu().numpy())\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        # concatenate and plot\n",
    "        predictions = predictions.reshape(-1, len(eeg_channels_to_use))\n",
    "        targets = targets.reshape(-1, len(eeg_channels_to_use))\n",
    "    \n",
    "        scipy.io.savemat(os.path.join(OUTPUT_DIRECTORY, f'test_{model_name}_{weight_epoch}.mat'), {'X': targets, \n",
    "                                                                'XPred':predictions,\n",
    "                                                            'bins':10,\n",
    "                                                            'scale':10,\n",
    "                                                            'srate':200})\n",
    "        \n",
    "        # R2 score\n",
    "        r2 = r2_score(targets, predictions)\n",
    "        print(f'Epoch-{weight_epoch}: {r2}')\n",
    "        \n",
    "        # Plotting target vs. output on concatenated data\n",
    "        for i in range(len(eeg_channels_to_use)):\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(targets[:,i], label='Target')\n",
    "            plt.plot(predictions[:,i], label='Output', linestyle='--')\n",
    "            plt.title(f'Epoch-{weight_epoch} Channel {eeg_channels_to_use[i]} : {r2}')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(OUTPUT_DIRECTORY, f'test_{model_name}_{weight_epoch}.jpeg'))\n",
    "\n",
    "    gc.collect()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048f118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
